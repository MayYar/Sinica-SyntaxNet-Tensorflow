Context Sensitive Stemming for Web Search 
content:
1 ABSTRACT :
1-1:Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms .
1-2:Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation .
1-3:In this paper, we propose a context sensitive stemming method that addresses these two issues .
1-4:Two unique properties make our approach feasible for Web Search .
1-5:First, based on statistical language modeling, we perform context sensitive analysis on the query side .
1-6:We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine .
1-7:This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time .
1-8:Second, our approach performs a context sensitive document matching for those expanded variants .
1-9:This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision .
1-10:Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queries and 1.8% over all query traffic .
1-11:H.3.3 [Information Systems]: Information Storage and .
2 INTRODUCTION :
2-1:Web search has now become a major tool in our daily lives for information seeking .
2-2:One of the important issues in Web search is that user queries are often not best formulated to get optimal results .
2-3:For example, running shoe is a query that occurs frequently in query logs .
2-4:However, the query running shoes is much more likely to give better search results than the original query because documents matching the intent of this query usually contain the words running shoes .
2-5:Correctly formulating a query requires the user to accurately predict which word form is used in the documents that best satisfy his or her information needs .
2-6:This is difficult even for experienced users, and especially difficult for non native speakers .
2-7:One traditional solution is to use stemming [16, 18], the process of transforming inflected or derived words to their root form so that a search term will match and retrieve documents containing all forms of the term .
2-8:Thus, the word run will match running, ran, runs, and shoe will match shoes and shoeing .
2-9:Stemming can be done either on the terms in a document during indexing (and applying the same transformation to the query terms during query processing) or by expanding the query with the variants during query processing .
2-10:Stemming during indexing allows very little flexibility during query processing, while stemming by query expansion allows handling each query differently, and hence is preferred .
2-11:Although traditional stemming increases recall by matching word variants [13], it can reduce precision by retrieving too many documents that have been incorrectly matched .
2-12:When examining the results of applying stemming to a large number of queries, one usually finds that nearly equal numbers of queries are helped and hurt by the technique [6] .
2-13:In addition, it reduces system performance because the search engine has to match all the word variants .
2-14:As we will show in the experiments, this is true even if we simplify stemming to pluralization handling, which is the process of converting a word from its plural to singular form, or vice versa .
2-15:Thus, one needs to be very cautious when using stemming in Web search engines .
2-16:One problem of traditional stemming is its blind transformation of all query terms, that is, it always performs the same transformation for the same query word without considering the context of the word .
2-17:For example, the word book has four forms book, books, booking, booked, and store has four forms store, stores, storing, stored .
2-18:For the query book store, expanding both words to all of their variants significantly increases computation cost and hurts precision, since not all of the variants are useful for this query .
2-19:Transforming book store to match book stores is fine, but matching book storing or booking store is not .
2-20:A weighting method that gives variant words smaller weights alleviates the problems to a certain extent if the weights accurately reflect the importance of the variant in this particular query .
2-21:However uniform weighting is not going to work and a query dependent weighting is still a challenging unsolved problem [20] .
2-22:A second problem of traditional stemming is its blind matching of all occurrences in documents .
2-23:For the query book store, a transformation that allows the variant stores to be matched will cause every occurrence of stores in the document to be treated equivalent to the query term store .
2-24:Thus, a document containing the fragment reading a book in coffee stores will be matched, causing many wrong documents to be selected .
2-25:Although we hope the ranking function can correctly handle these, with many more candidates to rank, the risk of making mistakes increases .
2-26:To alleviate these two problems, we propose a context sensitive stemming approach for Web search .
2-27:Our solution consists of two context sensitive analysis, one on the query side and the other on the document side .
2-28:On the query side, we propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms .
2-29:On the document side, we propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query .
2-30:Our model is simple yet effective and efficient, making it feasible to be used in real commercial Web search engines .
2-31:We use pluralization handling as a running example for our stemming approach .
2-32:The motivation for using pluralization handling as an example is to show that even such simple stemming, if handled correctly, can give significant benefits to search relevance .
2-33:As far as we know, no previous research has systematically investigated the usage of pluralization in Web search .
2-34:As we have to point out, the method we propose is not limited to pluralization handling, it is a general stemming technique, and can also be applied to general query expansion .
2-35:Experiments on general stemming yield additional significant improvements over pluralization handling for long queries, although details will not be reported in this paper .
2-36:In the rest of the paper, we first present the related work and distinguish our method from previous work in Section 2 .
2-37:We describe the details of the context sensitive stemming approach in Section 3 .
2-38:We then perform extensive experiments on a major Web search engine to support our claims in Section 4, followed by discussions in Section 5 .
2-39:Finally, we conclude the paper in Section 6. .
3 RELATED WORK :
3-1:Stemming is a long studied technology .
3-2:Many stemmers have been developed, such as the Lovins stemmer [16] and the Porter stemmer [18] .
3-3:The Porter stemmer is widely used due to its simplicity and effectiveness in many applications .
3-4:However, the Porter stemming makes many mistakes because its simple rules cannot fully describe English morphology .
3-5:Corpus analysis is used to improve Porter stemmer [26] by creating equivalence classes for words that are morphologically similar and occur in similar context as measured by expected mutual information [23] .
3-6:We use a similar corpus based approach for stemming by computing the similarity between two words based on their distributional context features which can be more than just adjacent words [15], and then only keep the morphologically similar words as candidates .
3-7:Using stemming in information retrieval is also a well known technique [8, 10] .
3-8:However, the effectiveness of stemming for English query systems was previously reported to be rather limited .
3-9:Lennon et al .
3-10:[17] compared the Lovins and Porter algorithms and found little improvement in retrieval performance .
3-11:Later, Harman [9] compares three general stemming techniques in text retrieval experiments including pluralization handing (called S stemmer in the paper) .
3-12:They also proposed selective stemming based on query length and term importance, but no positive results were reported .
3-13:On the other hand, Krovetz [14] performed comparisons over small numbers of documents (from 400 to 12k) and showed dramatic precision improvement (up to 45%) .
3-14:However, due to the limited number of tested queries (less than 100) and the small size of the collection, the results are hard to generalize to Web search .
3-15:These mixed results, mostly failures, led early IR researchers to deem stemming irrelevant in general for English [4], although recent research has shown stemming has greater benefits for retrieval in other languages [2] .
3-16:We suspect the previous failures were mainly due to the two problems we mentioned in the introduction .
3-17:Blind stemming, or a simple query length based selective stemming as used in [9] is not enough .
3-18:Stemming has to be decided on case by case basis, not only at the query level but also at the document level .
3-19:As we will show, if handled correctly, significant improvement can be achieved .
3-20:A more general problem related to stemming is query reformulation [3, 12] and query expansion which expands words not only with word variants [7, 22, 24, 25] .
3-21:To decide which expanded words to use, people often use pseudorelevance feedback techniquesthat send the original query to a search engine and retrieve the top documents, extract relevant words from these top documents as additional query words, and resubmit the expanded query again [21] .
3-22:This normally requires sending a query multiple times to search engine and it is not cost effective for processing the huge amount of queries involved in Web search .
3-23:In addition, query expansion, including query reformulation [3, 12], has a high risk of changing the user intent (called query drift) .
3-24:Since the expanded words may have different meanings, adding them to the query could potentially change the intent of the original query .
3-25:Thus query expansion based on pseudorelevance and query reformulation can provide suggestions to users for interactive refinement but can hardly be directly used for Web search .
3-26:On the other hand, stemming is much more conservative since most of the time, stemming preserves the original search intent .
3-27:While most work on query expansion focuses on recall enhancement, our work focuses on increasing both recall and precision .
3-28:The increase on recall is obvious .
3-29:With quality stemming, good documents which were not selected before stemming will be pushed up and those low quality documents will be degraded .
3-30:On selective query expansion, Cronen Townsend et al .
3-31:[6] proposed a method for selective query expansion based on comparing the Kullback Leibler divergence of the results from the unexpanded query and the results from the expanded query .
3-32:This is similar to the relevance feedback in the sense that it requires multiple passes retrieval .
3-33:If a word can be expanded into several words, it requires running this process multiple times to decide which expanded word is useful .
3-34:It is expensive to deploy this in production Web search engines .
3-35:Our method predicts the quality of expansion based on oﬄine information without sending the query to a search engine .
3-36:In summary, we propose a novel approach to attack an old, yet still important and challenging problem for Web search stemming .
3-37:Our approach is unique in that it performs predictive stemming on a per query basis without relevance feedback from the Web, using the context of the variants in documents to preserve precision .
3-38:It"s simple, yet very efficient and effective, making real time stemming feasible for Web search .
3-39:Our results will affirm researchers that stemming is indeed very important to large scale information retrieval. .
4 CONTEXT SENSITIVE STEMMING :
4-1:3.1 Overview Our system has four components as illustrated in Figure 1: candidate generation, query segmentation and head word detection, context sensitive query stemming and context sensitive document matching .
4-2:Candidate generation (component 1) is performed oﬄine and generated candidates are stored in a dictionary .
4-3:For an input query, we first segment query into concepts and detect the head word for each concept (component 2) .
4-4:We then use statistical language modeling to decide whether a particular variant is useful (component 3), and finally for the expanded variants, we perform context sensitive document matching (component 4) .
4-5:Below we discuss each of the components in more detail .
4-6:Component 4: context sensitive document matching Input Query: and head word detection Component 2: segment Component 1: candidate generation comparisons −> comparison Component 3: selective word expansion decision: comparisons −> comparison example: hotel price comparisons output: "hotel" "comparisons" hotel −> hotels Figure 1: System Components 3.2 Expansion candidate generation One of the ways to generate candidates is using the Porter stemmer [18] .
4-7:The Porter stemmer simply uses morphological rules to convert a word to its base form .
4-8:It has no knowledge of the semantic meaning of the words and sometimes makes serious mistakes, such as executive to execution, news to new, and paste to past .
4-9:A more conservative way is based on using corpus analysis to improve the Porter stemmer results [26] .
4-10:The corpus analysis we do is based on word distributional similarity [15] .
4-11:The rationale of using distributional word similarity is that true variants tend to be used in similar contexts .
4-12:In the distributional word similarity calculation, each word is represented with a vector of features derived from the context of the word .
4-13:We use the bigrams to the left and right of the word as its context features, by mining a huge Web corpus .
4-14:The similarity between two words is the cosine similarity between the two corresponding feature vectors .
4-15:The top 20 similar words to develop is shown in the following table .
4-16:rank candidate score rank candidate score 0 develop 1 10 berts 0.119 1 developing 0.339 11 wads 0.116 2 developed 0.176 12 developer 0.107 3 incubator 0.160 13 promoting 0.100 4 develops 0.150 14 developmental 0.091 5 development 0.148 15 reengineering 0.090 6 tutoring 0.138 16 build 0.083 7 analyzing 0.128 17 construct 0.081 8 developement 0.128 18 educational 0.081 9 automation 0.126 19 institute 0.077 Table 1: Top 20 most similar candidates to word develop .
4-17:Column score is the similarity score .
4-18:To determine the stemming candidates, we apply a few Porter stemmer [18] morphological rules to the similarity list .
4-19:After applying these rules, for the word develop, the stemming candidates are developing, developed, develops, development, developement, developer, developmental .
4-20:For the pluralization handling purpose, only the candidate develops is retained .
4-21:One thing we note from observing the distributionally similar words is that they are closely related semantically .
4-22:These words might serve as candidates for general query expansion, a topic we will investigate in the future .
4-23:3.3 Segmentation and headword identification For long queries, it is quite important to detect the concepts in the query and the most important words for those concepts .
4-24:We first break a query into segments, each segment representing a concept which normally is a noun phrase .
4-25:For each of the noun phrases, we then detect the most important word which we call the head word .
4-26:Segmentation is also used in document sensitive matching (section 3.5) to enforce proximity .
4-27:To break a query into segments, we have to define a criteria to measure the strength of the relation between words .
4-28:One effective method is to use mutual information as an indicator on whether or not to split two words [19] .
4-29:We use a log of 25M queries and collect the bigram and unigram frequencies from it .
4-30:For every incoming query, we compute the mutual information of two adjacent words; if it passes a predefined threshold, we do not split the query between those two words and move on to next word .
4-31:We continue this process until the mutual information between two words is below the threshold, then create a concept boundary here .
4-32:Table 2 shows some examples of query segmentation .
4-33:The ideal way of finding the head word of a concept is to do syntactic parsing to determine the dependency structure of the query .
4-34:Query parsing is more difficult than sentence [running shoe] [best] [new york] [medical schools] [pictures] [of] [white house] [cookies] [in] [san francisco] [hotel] [price comparison] Table 2: Query segmentation: a segment is bracketed .
4-35:parsing since many queries are not grammatical and are very short .
4-36:Applying a parser trained on sentences from documents to queries will have poor performance .
4-37:In our solution, we just use simple heuristics rules, and it works very well in practice for English .
4-38:For an English noun phrase, the head word is typically the last nonstop word, unless the phrase is of a particular pattern, like XYZ of in at from UVW .
4-39:In such cases, the head word is typically the last nonstop word of XYZ .
4-40:3.4 Context sensitive word expansion After detecting which words are the most important words to expand, we have to decide whether the expansions will be useful .
4-41:Our statistics show that about half of the queries can be transformed by pluralization via naive stemming .
4-42:Among this half, about 25% of the queries improve relevance when transformed, the majority (about 50%) do not change their top 5 results, and the remaining 25% perform worse .
4-43:Thus, it is extremely important to identify which queries should not be stemmed for the purpose of maximizing relevance improvement and minimizing stemming cost .
4-44:In addition, for a query with multiple words that can be transformed, or a word with multiple variants, not all of the expansions are useful .
4-45:Taking query hotel price comparison as an example, we decide that hotel and price comparison are two concepts .
4-46:Head words hotel and comparison can be expanded to hotels and comparisons .
4-47:Are both transformations useful? To test whether an expansion is useful, we have to know whether the expanded query is likely to get more relevant documents from the Web, which can be quantified by the probability of the query occurring as a string on the Web .
4-48:The more likely a query to occur on the Web, the more relevant documents this query is able to return .
4-49:Now the whole problem becomes how to calculate the probability of query to occur on the Web .
4-50:Calculating the probability of string occurring in a corpus is a well known language modeling problem .
4-51:The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2...wN ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur) .
4-52:The simplest and most successful approach to language modeling is still based on the n gram model .
4-53:By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n gram model approximates this probability by assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e .
4-54:Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n gram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.) denotes the number of occurrences of a specified gram in the training corpus .
4-55:Although one could attempt to use simple n gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary .
4-56:This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6) .
4-57:Also, because of the heavy tailed nature of language (i.e .
4-58:Zipf"s law) one is likely to encounter novel n grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non zero probability to novel n grams is a central and unavoidable issue in statistical language modeling .
4-59:One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n grams) is to use some sort of back off estimator .
4-60:Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), if #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), otherwise (3) where ˆPr(wi|wi−n+1...wi−1) = discount #(wi−n+1...wi) #(wi−n+1...wi−1) (4) is the discounted probability and β(wi−n+1...wi−1) is a normalization constant β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) The discounted probability (4) can be computed with different smoothing techniques, including absolute smoothing, Good Turing smoothing, linear smoothing, and Witten Bell smoothing [5] .
4-61:We used absolute smoothing in our experiments .
4-62:Since the likelihood of a string, Pr(w1w2...wN ), is a very small number and hard to interpret, we use entropy as defined below to score the string .
4-63:Entropy = − 1 N log2 Pr(w1w2...wN ) (6) Now getting back to the example of the query hotel price comparisons, there are four variants of this query, and the entropy of these four candidates are shown in Table 3 .
4-64:We can see that all alternatives are less likely than the input query .
4-65:It is therefore not useful to make an expansion for this query .
4-66:On the other hand, if the input query is hotel price comparisons which is the second alternative in the table, then there is a better alternative than the input query, and it should therefore be expanded .
4-67:To tolerate the variations in probability estimation, we relax the selection criterion to those query alternatives if their scores are within a certain distance (10% in our experiments) to the best score .
4-68:Query variations Entropy hotel price comparison 6.177 hotel price comparisons 6.597 hotels price comparison 6.937 hotels price comparisons 7.360 Table 3: Variations of query hotel price comparison ranked by entropy score, with the original query in bold face .
4-69:3.5 Context sensitive document matching Even after we know which word variants are likely to be useful, we have to be conservative in document matching for the expanded variants .
4-70:For the query hotel price comparisons, we decided that word comparisons is expanded to include comparison .
4-71:However, not every occurrence of comparison in the document is of interest .
4-72:A page which is about comparing customer service can contain all of the words hotel price comparisons comparison .
4-73:This page is not a good page for the query .
4-74:If we accept matches of every occurrence of comparison, it will hurt retrieval precision and this is one of the main reasons why most stemming approaches do not work well for information retrieval .
4-75:To address this problem, we have a proximity constraint that considers the context around the expanded variant in the document .
4-76:A variant match is considered valid only if the variant occurs in the same context as the original word does .
4-77:The context is the left or the right non stop segments 1 of the original word .
4-78:Taking the same query as an example, the context of comparisons is price .
4-79:The expanded word comparison is only valid if it is in the same context of comparisons, which is after the word price .
4-80:Thus, we should only match those occurrences of comparison in the document if they occur after the word price .
4-81:Considering the fact that queries and documents may not represent the intent in exactly the same way, we relax this proximity constraint to allow variant occurrences within a window of some fixed size .
4-82:If the expanded word comparison occurs within the context of price within a window, it is considered valid .
4-83:The smaller the window size is, the more restrictive the matching .
4-84:We use a window size of 4, which typically captures contexts that include the containing and adjacent noun phrases. .
5 EXPERIMENTAL EVALUATION :
5-1:4.1 Evaluation metrics We will measure both relevance improvement and the stemming cost required to achieve the relevance .
5-2:1 a context segment can not be a single stop word .
5-3:4.1.1 Relevance measurement We use a variant of the average Discounted Cumulative Gain (DCG), a recently popularized scheme to measure search engine relevance [1, 11] .
5-4:Given a query and a ranked list of K documents (K is set to 5 in our experiments), the DCG(K) score for this query is calculated as follows: DCG(K) = KX k=1 gk log2(1 + k) .
5-5:(7) where gk is the weight for the document at rank k .
5-6:Higher degree of relevance corresponds to a higher weight .
5-7:A page is graded into one of the five scales: Perfect, Excellent, Good, Fair, Bad, with corresponding weights .
5-8:We use dcg to represent the average DCG(5) over a set of test queries .
5-9:4.1.2 Stemming cost Another metric is to measure the additional cost incurred by stemming .
5-10:Given the same level of relevance improvement, we prefer a stemming method that has less additional cost .
5-11:We measure this by the percentage of queries that are actually stemmed, over all the queries that could possibly be stemmed .
5-12:4.2 Data preparation We randomly sample 870 queries from a three month query log, with 290 from each month .
5-13:Among all these 870 queries, we remove all misspelled queries since misspelled queries are not of interest to stemming .
5-14:We also remove all one word queries since stemming one word queries without context has a high risk of changing query intent, especially for short words .
5-15:In the end, we have 529 correctly spelled queries with at least 2 words .
5-16:4.3 Naive stemming for Web search Before explaining the experiments and results in detail, we"d like to describe the traditional way of using stemming for Web search, referred as the naive model .
5-17:This is to treat every word variant equivalent for all possible words in the query .
5-18:The query book store will be transformed into (book OR books)(store OR stores) when limiting stemming to pluralization handling only, where OR is an operator that denotes the equivalence of the left and right arguments .
5-19:4.4 Experimental setup The baseline model is the model without stemming .
5-20:We first run the naive model to see how well it performs over the baseline .
5-21:Then we improve the naive stemming model by document sensitive matching, referred as document sensitive matching model .
5-22:This model makes the same stemming as the naive model on the query side, but performs conservative matching on the document side using the strategy described in section 3.5 .
5-23:The naive model and document sensitive matching model stem the most queries .
5-24:Out of the 529 queries, there are 408 queries that they stem, corresponding to 46.7% query traffic (out of a total of 870) .
5-25:We then further improve the document sensitive matching model from the query side with selective word stemming based on statistical language modeling (section 3.4), referred as selective stemming model .
5-26:Based on language modeling prediction, this model stems only a subset of the 408 queries stemmed by the document sensitive matching model .
5-27:We experiment with unigram language model and bigram language model .
5-28:Since we only care how much we can improve the naive model, we will only use these 408 queries (all the queries that are affected by the naive stemming model) in the experiments .
5-29:To get a sense of how these models perform, we also have an oracle model that gives the upper bound performance a stemmer can achieve on this data .
5-30:The oracle model only expands a word if the stemming will give better results .
5-31:To analyze the pluralization handling influence on different query categories, we divide queries into short queries and long queries .
5-32:Among the 408 queries stemmed by the naive model, there are 272 short queries with 2 or 3 words, and 136 long queries with at least 4 words .
5-33:4.5 Results We summarize the overall results in Table 4, and present the results on short queries and long queries separately in Table 5 .
5-34:Each row in Table 4 is a stemming strategy described in section 4.4 .
5-35:The first column is the name of the strategy .
5-36:The second column is the number of queries affected by this strategy; this column measures the stemming cost, and the numbers should be low for the same level of dcg .
5-37:The third column is the average dcg score over all tested queries in this category (including the ones that were not stemmed by the strategy) .
5-38:The fourth column is the relative improvement over the baseline, and the last column is the p value of Wilcoxon significance test .
5-39:There are several observations about the results .
5-40:We can see the naively stemming only obtains a statistically insignificant improvement of 1.5% .
5-41:Looking at Table 5, it gives an improvement of 2.7% on short queries .
5-42:However, it also hurts long queries by 2.4% .
5-43:Overall, the improvement is canceled out .
5-44:The reason that it improves short queries is that most short queries only have one word that can be stemmed .
5-45:Thus, blindly pluralizing short queries is relatively safe .
5-46:However for long queries, most queries can have multiple words that can be pluralized .
5-47:Expanding all of them without selection will significantly hurt precision .
5-48:Document context sensitive stemming gives a significant lift to the performance, from 2.7% to 4.2% for short queries and from 2.4% to 1.6% for long queries, with an overall lift from 1.5% to 2.8% .
5-49:The improvement comes from the conservative context sensitive document matching .
5-50:An expanded word is valid only if it occurs within the context of original query in the document .
5-51:This reduces many spurious matches .
5-52:However, we still notice that for long queries, context sensitive stemming is not able to improve performance because it still selects too many documents and gives the ranking function a hard problem .
5-53:While the chosen window size of 4 works the best amongst all the choices, it still allows spurious matches .
5-54:It is possible that the window size needs to be chosen on a per query basis to ensure tighter proximity constraints for different types of noun phrases .
5-55:Selective word pluralization further helps resolving the problem faced by document context sensitive stemming .
5-56:It does not stem every word that places all the burden on the ranking algorithm, but tries to eliminate unnecessary stemming in the first place .
5-57:By predicting which word variants are going to be useful, we can dramatically reduce the number of stemmed words, thus improving both the recall and the precision .
5-58:With the unigram language model, we can reduce the stemming cost by 26.7% (from 408 408 to 300 408) and lift the overall dcg improvement from 2.8% to 3.4% .
5-59:In particular, it gives significant improvements on long queries .
5-60:The dcg gain is turned from negative to positive, from −1.6% to 1.1% .
5-61:This confirms our hypothesis that reducing unnecessary word expansion leads to precision improvement .
5-62:For short queries too, we observe both dcg improvement and stemming cost reduction with the unigram language model .
5-63:The advantages of predictive word expansion with a language model is further boosted with a better bigram language model .
5-64:The overall dcg gain is lifted from 3.4% to 3.9%, and stemming cost is dramatically reduced from 408 408 to 250 408, corresponding to only 29% of query traffic (250 out of 870) and an overall 1.8% dcg improvement overall all query traffic .
5-65:For short queries, bigram language model improves the dcg gain from 4.4% to 4.7%, and reduces stemming cost from 272 272 to 150 272 .
5-66:For long queries, bigram language model improves dcg gain from 1.1% to 2.5%, and reduces stemming cost from 136 136 to 100 136 .
5-67:We observe that the bigram language model gives a larger lift for long queries .
5-68:This is because the uncertainty in long queries is larger and a more powerful language model is needed .
5-69:We hypothesize that a trigram language model would give a further lift for long queries and leave this for future investigation .
5-70:Considering the tight upper bound 2 on the improvement to be gained from pluralization handling (via the oracle model), the current performance on short queries is very satisfying .
5-71:For short queries, the dcg gain upper bound is 6.3% for perfect pluralization handling, our current gain is 4.7% with a bigram language model .
5-72:For long queries, the dcg gain upper bound is 4.6% for perfect pluralization handling, our current gain is 2.5% with a bigram language model .
5-73:We may gain additional benefit with a more powerful language model for long queries .
5-74:However, the difficulties of long queries come from many other aspects including the proximity and the segmentation problem .
5-75:These problems have to be addressed separately .
5-76:Looking at the the upper bound of overhead reduction for oracle stemming, 75% (308 408) of the naive stemmings are wasteful .
5-77:We currently capture about half of them .
5-78:Further reduction of the overhead requires sacrificing the dcg gain .
5-79:Now we can compare the stemming strategies from a different aspect .
5-80:Instead of looking at the influence over all queries as we described above, Table 6 summarizes the dcg improvements over the affected queries only .
5-81:We can see that the number of affected queries decreases as the stemming strategy becomes more accurate (dcg improvement) .
5-82:For the bigram language model, over the 250 408 stemmed queries, the dcg improvement is 6.1% .
5-83:An interesting observation is the average dcg decreases with a better model, which indicates a better stemming strategy stems more difficult queries (low dcg queries). .
6 DISCUSSIONS :
6-1:5.1 Language models from query vs .
6-2:from Web As we mentioned in Section 1, we are trying to predict the probability of a string occurring on the Web .
6-3:The language model should describe the occurrence of the string on the Web .
6-4:However, the query log is also a good resource .
6-5:2 Note that this upperbound is for pluralization handling only, not for general stemming .
6-6:General stemming gives a 8% upperbound, which is quite substantial in terms of our metrics .
6-7:Affected Queries dcg dcg Improvement p value baseline 0 408 7.102 N A N A naive model 408 408 7.206 1.5% 0.22 document context sensitive model 408 408 7.302 2.8% 0.014 selective model: unigram LM 300 408 7.321 3.4% 0.001 selective model: bigram LM 250 408 7.381 3.9% 0.001 oracle model 100 408 7.519 5.9% 0.001 Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming Short Query Results Affected Queries dcg Improvement p value baseline 0 272 N A N A naive model 272 272 2.7% 0.48 document context sensitive model 272 272 4.2% 0.002 selective model: unigram LM 185 272 4.4% 0.001 selective model: bigram LM 150 272 4.7% 0.001 oracle model 71 272 6.3% 0.001 Long Query Results Affected Queries dcg Improvement p value baseline 0 136 N A N A naive model 136 136 2.4% 0.25 document context sensitive model 136 136 1.6% 0.27 selective model: unigram LM 115 136 1.1% 0.001 selective model: bigram LM 100 136 2.5% 0.001 oracle model 29 136 4.6% 0.001 Table 5: Results comparison of different stemming strategies overall short queries and long queries Users reformulate a query using many different variants to get good results .
6-8:To test the hypothesis that we can learn reliable transformation probabilities from the query log, we trained a language model from the same query top 25M queries as used to learn segmentation, and use that for prediction .
6-9:We observed a slight performance decrease compared to the model trained on Web frequencies .
6-10:In particular, the performance for unigram LM was not affected, but the dcg gain for bigram LM changed from 4.7% to 4.5% for short queries .
6-11:Thus, the query log can serve as a good approximation of the Web frequencies .
6-12:5.2 How linguistics helps Some linguistic knowledge is useful in stemming .
6-13:For the pluralization handling case, pluralization and de pluralization is not symmetric .
6-14:A plural word used in a query indicates a special intent .
6-15:For example, the query new york hotels is looking for a list of hotels in new york, not the specific new york hotel which might be a hotel located in California .
6-16:A simple equivalence of hotel to hotels might boost a particular page about new york hotel to top rank .
6-17:To capture this intent, we have to make sure the document is a general page about hotels in new york .
6-18:We do this by requiring that the plural word hotels appears in the document .
6-19:On the other hand, converting a singular word to plural is safer since a general purpose page normally contains specific information .
6-20:We observed a slight overall dcg decrease, although not statistically significant, for document context sensitive stemming if we do not consider this asymmetric property .
6-21:5.3 Error analysis One type of mistakes we noticed, though rare but seriously hurting relevance, is the search intent change after stemming .
6-22:Generally speaking, pluralization or depluralization keeps the original intent .
6-23:However, the intent could change in a few cases .
6-24:For one example of such a query, job at apple, we pluralize job to jobs .
6-25:This stemming makes the original query ambiguous .
6-26:The query job OR jobs at apple has two intents .
6-27:One is the employment opportunities at apple, and another is a person working at Apple, Steve Jobs, who is the CEO and co founder of the company .
6-28:Thus, the results after query stemming returns Steve Jobs as one of the results in top 5 .
6-29:One solution is performing results set based analysis to check if the intent is changed .
6-30:This is similar to relevance feedback and requires second phase ranking .
6-31:A second type of mistakes is the entity concept recognition problem, These include two kinds .
6-32:One is that the stemmed word variant now matches part of an entity or concept .
6-33:For example, query cookies in san francisco is pluralized to cookies OR cookie in san francisco .
6-34:The results will match cookie jar in san francisco .
6-35:Although cookie still means the same thing as cookies, cookie jar is a different concept .
6-36:Another kind is the unstemmed word matches an entity or concept because of the stemming of the other words .
6-37:For example, quote ICE is pluralized to quote OR quotes ICE .
6-38:The original intent for this query is searching for stock quote for ticker ICE .
6-39:However, we noticed that among the top results, one of the results is Food quotes: Ice cream .
6-40:This is matched because of Affected Queries old dcg new dcg dcg Improvement naive model 408 408 7.102 7.206 1.5% document context sensitive model 408 408 7.102 7.302 2.8% selective model: unigram LM 300 408 5.904 6.187 4.8% selective model: bigram LM 250 408 5.551 5.891 6.1% Table 6: Results comparison over the stemmed queries only: column old new dcg is the dcg score over the affected queries before after applying stemming the pluralized word quotes .
6-41:The unchanged word ICE matches part of the noun phrase ice cream here .
6-42:To solve this kind of problem, we have to analyze the documents and recognize cookie jar and ice cream as concepts instead of two independent words .
6-43:A third type of mistakes occurs in long queries .
6-44:For the query bar code reader software, two words are pluralized .
6-45:code to codes and reader to readers .
6-46:In fact, bar code reader in the original query is a strong concept and the internal words should not be changed .
6-47:This is the segmentation and entity and noun phrase detection problem in queries, which we actively are attacking .
6-48:For long queries, we should correctly identify the concepts in the query, and boost the proximity for the words within a concept. .
7-1:We have presented a simple yet elegant way of stemming for Web search
7-2:It improves naive stemming in two aspects: selective word expansion on the query side and conservative word occurrence matching on the document side
7-3:Using pluralization handling as an example, experiments on a major Web search engine data show it significantly improves the Web relevance and reduces the stemming cost
7-4:It also significantly improves Web click through rate (details not reported in the paper)
7-5:For the future work, we are investigating the problems we identified in the error analysis section
7-6:These include: entity and noun phrase matching mistakes, and improved segmentation.
8-1:E
8-2:Agichtein, E
8-3:Brill, and S
8-4:T
8-5:Dumais
8-6:Improving Web Search Ranking by Incorporating User Behavior Information
8-7:In SIGIR, 2006
8-8:E
8-9:Airio
8-10:Word Normalization and Decompounding in Mono and Bilingual IR
8-11:Information Retrieval, 9:249 271, 2006
8-12:P
8-13:Anick
8-14:Using Terminological Feedback for Web Search Refinement: a Log based Study
8-15:In SIGIR, 2003
8-16:R
8-17:Baeza Yates and B
8-18:Ribeiro Neto
8-19:Modern Information Retrieval
8-20:ACM Press Addison Wesley, 1999
8-21:S
8-22:Chen and J
8-23:Goodman
8-24:An Empirical Study of Smoothing Techniques for Language Modeling
8-25:Technical Report TR 10 98, Harvard University, 1998
8-26:S
8-27:Cronen Townsend, Y
8-28:Zhou, and B
8-29:Croft
8-30:A Framework for Selective Query Expansion
8-31:In CIKM, 2004
8-32:H
8-33:Fang and C
8-34:Zhai
8-35:Semantic Term Matching in Axiomatic Approaches to Information Retrieval
8-36:In SIGIR, 2006
8-37:W
8-38:B
8-39:Frakes
8-40:Term Conflation for Information Retrieval
8-41:In C
8-42:J
8-43:Rijsbergen, editor, Research and Development in Information Retrieval, pages 383 389
8-44:Cambridge University Press, 1984
8-45:D
8-46:Harman
8-47:How Effective is Suffixing? JASIS, 42(1):7 15, 1991
8-48:D
8-49:Hull
8-50:Stemming Algorithms  A Case Study for Detailed Evaluation
8-51:JASIS, 47(1):70 84, 1996
8-52:K
8-53:Jarvelin and J
8-54:Kekalainen
8-55:Cumulated Gain Based Evaluation Evaluation of IR Techniques
8-56:ACM TOIS, 20:422 446, 2002
8-57:R
8-58:Jones, B
8-59:Rey, O
8-60:Madani, and W
8-61:Greiner
8-62:Generating Query Substitutions
8-63:In WWW, 2006
8-64:W
8-65:Kraaij and R
8-66:Pohlmann
8-67:Viewing Stemming as Recall Enhancement
8-68:In SIGIR, 1996
8-69:R
8-70:Krovetz
8-71:Viewing Morphology as an Inference Process
8-72:In SIGIR, 1993
8-73:D
8-74:Lin
8-75:Automatic Retrieval and Clustering of Similar Words
8-76:In COLING ACL, 1998
8-77:J
8-78:B
8-79:Lovins
8-80:Development of a Stemming Algorithm
8-81:Mechanical Translation and Computational Linguistics, II:22 31, 1968
8-82:M
8-83:Lennon and D
8-84:Peirce and B
8-85:Tarry and P
8-86:Willett
8-87:An Evaluation of Some Conflation Algorithms for Information Retrieval
8-88:Journal of Information Science, 3:177 188, 1981
8-89:M
8-90:Porter
8-91:An Algorithm for Suffix Stripping
8-92:Program, 14(3):130 137, 1980
8-93:K
8-94:M
8-95:Risvik, T
8-96:Mikolajewski, and P
8-97:Boros
8-98:Query Segmentation for Web Search
8-99:In WWW, 2003
8-100:S
8-101:E
8-102:Robertson
8-103:On Term Selection for Query Expansion
8-104:Journal of Documentation, 46(4):359 364, 1990
8-105:G
8-106:Salton and C
8-107:Buckley
8-108:Improving Retrieval Performance by Relevance Feedback
8-109:JASIS, 41(4):288  297, 1999
8-110:R
8-111:Sun, C. H
8-112:Ong, and T. S
8-113:Chua
8-114:Mining Dependency Relations for Query Expansion in Passage Retrieval
8-115:In SIGIR, 2006
8-116:C
8-117:Van Rijsbergen
8-118:Information Retrieval
8-119:Butterworths, second version, 1979
8-120:B
8-121:V´elez, R
8-122:Weiss, M
8-123:A
8-124:Sheldon, and D
8-125:K
8-126:Gifford
8-127:Fast and Effective Query Refinement
8-128:In SIGIR, 1997
8-129:J
8-130:Xu and B
8-131:Croft
8-132:Query Expansion using Local and Global Document Analysis
8-133:In SIGIR, 1996
8-134:J
8-135:Xu and B
8-136:Croft
8-137:Corpus based Stemming using Cooccurrence of Word Variants
8-138:ACM TOIS, 16 (1):61 81, 1998
picture:
