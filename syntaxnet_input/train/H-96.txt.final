A Study of Factors Affecting the Utility of 
content:
1 ABSTRACT :
1-1:Implicit relevance feedback is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system .
1-2:IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly .
1-3:In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search .
1-4:Our findings suggest that all three of these factors contribute to the utility of IRF .
1-5:H.3.3 [Information Search and Retrieval] .
2 INTRODUCTION :
2-1:Information Retrieval systems are designed to help searchers solve problems .
2-2:In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection .
2-3:However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers .
2-4:As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6] .
2-5:Techniques such as Relevance Feedback [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification .
2-6:However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10] .
2-7:Implicit Relevance Feedback [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact .
2-8:IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse based result interfaces [5] .
2-9:IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7] .
2-10:In this paper we present a study into the use and effectiveness of IRF in an online search environment .
2-11:The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages? This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use .
2-12:The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13] .
2-13:In this paper we use data derived from that experiment to study factors affecting the utility of IRF. .
3 STUDY :
3-1:In this section we describe the user study conducted to address our research questions .
3-2:2.1 Systems Our study used two systems both of which suggested new query terms to the user .
3-3:One system suggested terms based on the user"s interaction (IRF), the other used Explicit RF asking the user to explicitly indicate relevant material .
3-4:Both systems used the same term suggestion algorithm, [15], and used a common interface .
3-5:2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full text and a variety of smaller, query relevant representations, created at retrieval time .
3-6:We used the Web as the test collection in this study and Google1 as the underlying search engine .
3-7:Document representations include the document title and a summary of the document; a list of top ranking sentences extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence) .
3-8:Each summary sentence and top ranking sentence is regarded as a representation of the document .
3-9:The default display contains the list of top ranking sentences and the list of the first ten document titles .
3-10:Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document .
3-11:This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16] .
3-12:In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information .
3-13:Both systems provide an interactive query expansion feature by suggesting new query terms to the user .
3-14:The searcher has the responsibility for choosing which, if any, of these terms to add to the query .
3-15:The searcher can also add or remove terms from the query at will .
3-16:2.1.2 Explicit RF system This version of the system implements explicit RF .
3-17:Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant .
3-18:Only the representations marked relevant by the user are used for suggesting new query terms .
3-19:This system was used as a baseline against which the IRF system could be compared .
3-20:2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact .
3-21:As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document .
3-22:To the searcher this is a way they can find out more information from a potentially interesting source .
3-23:To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant .
3-24:The query modification terms are selected using the same algorithm as in the Explicit RF system .
3-25:Therefore the only difference between the systems is how relevance is communicated to the system .
3-26:The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness .
3-27:2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects .
3-28:The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real life search situations and allow subjects to develop personal assessments of relevance .
3-29:We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects .
3-30:These subjects were not involved in the main experiment .
3-31:For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity .
3-32:As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks .
3-33:By developing tasks of different complexity we can assess how the nature of the task affects the subjects" interactive behaviour and hence the evidence supplied to IRF algorithms .
3-34:Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task .
3-35:In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task .
3-36:Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .
3-37:They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task .
3-38:Figure 1 shows the task statements for three levels of task complexity for one of the six search topics .
3-39:HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source .
3-40:Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide .
3-41:MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends" guests is complaining about the price of petrol and the factors that cause it .
3-42:Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK .
3-43:LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol .
3-44:However, as you have not been driving for long, you are unaware of any major changes in price .
3-45:You decide to find out how the price of petrol has changed in the UK in recent years .
3-46:Figure 1 .
3-47:Varying task complexity (Petrol Prices topic) .
3-48:2.3 Subjects 156 volunteers expressed an interest in participating in our study .
3-49:48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent inexperienced searchers) and experienced (frequent experienced searchers) .
3-50:Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use .
3-51:The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree .
3-52:47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science .
3-53:The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience .
3-54:The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed .
3-55:All were familiar with Web searching, and some with searching in other domains .
3-56:2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity .
3-57:Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface .
3-58:Each subject carried out three tasks, one on each system .
3-59:We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper .
3-60:switched systems after each task and used each system once .
3-61:The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design .
3-62:Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4] .
3-63:System logging was also used to record subject interaction .
3-64:A tutorial carried out prior to the experiment allowed subjects to use a non feedback version of the system to attempt a practice task before using the first experimental system .
3-65:Experiments lasted between oneand a half and two hours, dependent on variables such as the time spent completing questionnaires .
3-66:Subjects were offered a 5 minute break after the first hour .
3-67:In each experiment: the experiments and sign consent forms .
3-68:This set of instructions was written to ensure that each subject received precisely the same information .
3-69:ii .
3-70:the subject was asked to complete an introductory questionnaire .
3-71:This contained questions about the subject"s education, general search experience, computer experience and Web search experience .
3-72:iii .
3-73:the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF .
3-74:iv .
3-75:the subject was given three task sheets and asked to choose one task from the six topics on each sheet .
3-76:No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once .
3-77:Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task .
3-78:minutes to search .
3-79:The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task .
3-80:vi .
3-81:after completion of the search, the subject was asked to complete a post search questionnaire .
3-82:vii .
3-83:the remaining tasks were attempted by the subject, following steps v .
3-84:and vi .
3-85:viii .
3-86:the subject completed a post experiment questionnaire and participated in a post experiment interview .
3-87:Subjects were told that their interaction may be used by the IRF system to help them as they searched .
3-88:They were not told which behaviours would be used or how it would be used .
3-89:We now describe the findings of our analysis. .
4 FINDINGS :
4-1:In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF .
4-2:We present our findings per research question .
4-3:Due to the ordinal nature of much of the data non parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated .
4-4:We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate .
4-5:All Likert scales and semantic differentials were on a 5 point scale where a rating closer to 1 signifies more agreement with the attitude statement .
4-6:The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively .
4-7:The highest, or most positive, values in each table are shown in bold .
4-8:Our analysis uses data from questionnaires, post experiment interviews and background system logging on the ERF and IRF systems .
4-9:3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system .
4-10:In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities .
4-11:We present our findings in terms of the RF provided by subjects and the terms recommended by the systems .
4-12:3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks .
4-13:In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: to the system (i.e .
4-14:ticking boxes or viewing information) was: easy difficult, effective ineffective, useful" not useful .
4-15:to the system made you feel: comfortable uncomfortable, in control not in control .
4-16:The average obtained differential values are shown in Table 1 for IRF and each task category .
4-17:The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement .
4-18:This gives some overall understanding of the subjects" feelings which can be useful as the subjects may not answer individual differentials very precisely .
4-19:The values for ERF are included for reference in this table and all other tables and figures in the Findings section .
4-20:Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback .
4-21:Table 1 .
4-22:Subject perceptions of RF method (lower = better) .
4-23:Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity task on the ERF system, 16 subjects who ran a medium complexity task on the ERF system, etc) .
4-24:Kruskal Wallis Tests were applied to each differential for each type of RF3 .
4-25:Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1 .
4-26:and 2 .
4-27:respectively, i.e., .05 divided by the number of differentials .
4-28:This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true .
4-29:Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair wise comparisons between tasks were significant5 .
4-30:Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .
4-31:To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearman"s Rank Order Correlation Coefficient to participant responses .
4-32:The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .
4-33:On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .
4-34:Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding .
4-35:In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task .
4-36:We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided .
4-37:To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess .
4-38:In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant .
4-39:In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher .
4-40:This proportion measures the searcher"s level of interaction with a document, we take it to measure the user"s interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document .
4-41:There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context .
4-42:Since the interface shows document representations from the top 30 documents, there are 420 representations that a searcher can assess .
4-43:Table 2 shows proportion of representations provided as RF by subjects .
4-44:Table 2 .
4-45:Feedback and documents viewed .
4-46:Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document .
4-47:This suggests a pattern where users are investigating retrieved documents in more depth .
4-48:It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunn"s post hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal Wallis Test); all pair wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunn"s post hoc tests) feedback varies based on the complexity of the search task .
4-49:Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide .
4-50:This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected .
4-51:Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document) .
4-52:This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change .
4-53:In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF .
4-54:3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16] .
4-55:We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks .
4-56:To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant irrelevant and useful not useful .
4-57:Table 3 presents average responses grouped by search task .
4-58:Table 3 .
4-59:Subject perceptions of system terms (lower = better) .
4-60:Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal Wallis Tests were applied within each type of RF .
4-61:The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex .
4-62:10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task .
4-63:For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .
4-64:This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity .
4-65:Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .
4-66:As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested .
4-67:Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query .
4-68:In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearson"s Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF .
4-69:Table 4 .
4-70:Term Acceptance (percentage of top six terms) .
4-71:Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .
4-72:As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .
4-73:Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer .
4-74:This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases .
4-75:For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .
4-76:Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted .
4-77:Differences may reside in the nature of the terms accepted; future work will investigate this issue .
4-78:3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF .
4-79:From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks .
4-80:Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted .
4-81:3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers .
4-82:As such, levels of search experience may affect searchers" use and perceptions of IRF .
4-83:In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed .
4-84:In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects .
4-85:The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task .
4-86:3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1 .
4-87:(i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…) .
4-88:These differentials elicited opinion from experimental subjects about the RF method used .
4-89:In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell .
4-90:13 This was the smallest number of query modification terms that were offered in both systems .
4-91:14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5 .
4-92:Subject perceptions of RF method (lower = better) .
4-93:The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects .
4-94:16 The differences for all other IRF differentials were not statistically significant .
4-95:For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .
4-96:Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .
4-97:As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction .
4-98:Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .
4-99:It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process .
4-100:In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems .
4-101:Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 .
4-102:3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects .
4-103:Table 6 shows the average differential responses obtained from both subject groups .
4-104:Table 6 .
4-105:Subject perceptions of system terms (lower = better) .
4-106:Explicit RF Implicit RF Differential Inexp .
4-107:Exp .
4-108:Inexp .
4-109:Exp .
4-110:Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .
4-111:Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp .
4-112:Exp .
4-113:Inexp .
4-114:Exp .
4-115:Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects .
4-116:This finding was supported by the proportion of query modification terms these subjects accepted .
4-117:In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects .
4-118:Table 7 shows the average number of accepted terms per subject group .
4-119:Table 7 .
4-120:Term Acceptance (percentage of top six terms) .
4-121:Explicit RF Implicit RFProportion of terms Inexp .
4-122:Exp .
4-123:Inexp .
4-124:Exp .
4-125:Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF .
4-126:However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 .
4-127:3.2.3 Summary In this section we have analysed data gathered from two subject groups inexperienced searchers and experienced searchers on how they perceive and use IRF .
4-128:The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful .
4-129:We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries .
4-130:Search experience appears to affect how subjects use the terms recommended as a result of the RF process .
4-131:3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search .
4-132:To test this, our third research question concerned the use and usefulness of IRF during the course of a search .
4-133:In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are .
4-134:For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes .
4-135:We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes .
4-136:In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices .
4-137:This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results .
4-138:Slices 1 3 correspond to the start of the search, 4 6 to the middle of the search and 7 9 to the end .
4-139:In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search .
4-140:The total amount of feedback for a single RF method task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF HC across all nine slices of Figure 2 is 21.50%) .
4-141:To simplify the statistical analysis and comparison we use the grouping of start, middle and end .
4-142:0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Search"precision"(%oftotalrepsprovidedasRF) Explicit RF HC Explicit RF MC Explicit RF LC Implicit RF HC Implicit RF MC Implicit RF LC Figure 2 .
4-143:Distribution of RF provision per search task .
4-144:Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm .
4-145:These are essentially differences in the way users are assessing documents .
4-146:In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .
4-147:When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations .
4-148:At this stage the subjects are perhaps concentrating more on reading the retrieved results .
4-149:Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search .
4-150:This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings .
4-151:Figure 2 also shows the proportion of feedback for tasks of different complexity .
4-152:The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task .
4-153:More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point .
4-154:This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks .
4-155:23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs .
4-156:end Z = 2.58, p = .005 (Dunn"s post hoc tests) .
4-157:24 Although increasing toward the end of the start stage .
4-158:25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items .
4-159:That is, nonstopword, non query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification .
4-160:Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search .
4-161:In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search .
4-162:The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects .
4-163:Table 8 .
4-164:Term Acceptance (proportion of top six terms) .
4-165:Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept .
4-166:Search stage affects term acceptance in IRF but not in ERF26 .
4-167:The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 In this section we discuss the implications of the findings presented in the previous section for each research question .
4-168:4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks .
4-169:From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest) .
4-170:When the search was more complex subjects rarely found results they regarded as completely relevant .
4-171:Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunn"s post hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system .
4-172:In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do .
4-173:The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves .
4-174:It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias .
4-175:Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher .
4-176:For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents .
4-177:Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit .
4-178:4.2 Search Experience We analysed the affect of search experience on the utility of IRF .
4-179:Our analysis revealed a general preference across all subjects for IRF over ERF .
4-180:That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF .
4-181:However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations) .
4-182:All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .
4-183:These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively .
4-184:4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF .
4-185:Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method .
4-186:The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end .
4-187:The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point .
4-188:Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour .
4-189:The findings suggest that searchers interact differently for IRF and ERF .
4-190:Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF .
4-191:The development of such a system represents part of our ongoing work in this area. .
5-1:In this paper we have presented an investigation of Implicit Relevance Feedback (IRF)
5-2:We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF
5-3:These factors were search task complexity, the subjects" search experience and the stage in the search
5-4:Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion
5-5:appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant
5-6:Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity
5-7:Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not
5-8:It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems.
6-1:Bell, D.J
6-2:and Ruthven, I
6-3:(2004)
6-4:Searchers' assessments of task complexity for web searching
6-5:Proceedings of the 26th European Conference on Information Retrieval, 57 71
6-6:Borlund, P
6-7:(2000)
6-8:Experimental components for the evaluation of interactive information retrieval systems
6-9:Journal of Documentation
6-10:56(1): 71 90
6-11:Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F
6-12:(2002)
6-13:Strategic help for user interfaces for information retrieval
6-14:Journal of the American Society for Information Science and Technology
6-15:53(5): 343 358
6-16:Busha, C.H
6-17:and Harter, S.P., (1980)
6-18:Research methods in librarianship: Techniques and interpretation
6-19:Library and information science series
6-20:New York: Academic Press
6-21:Campbell, I
6-22:and Van Rijsbergen, C.J
6-23:(1996)
6-24:The ostensive model of developing information needs
6-25:Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251 268
6-26:Harman, D., (1992)
6-27:Relevance feedback and other query modification techniques
6-28:In Information retrieval: Data structures and algorithms
6-29:New York: Prentice Hall
6-30:Kelly, D
6-31:and Teevan, J
6-32:(2003)
6-33:Implicit feedback for inferring user preference
6-34:SIGIR Forum
6-35:37(2): 18 28
6-36:Koenemann, J
6-37:and Belkin, N.J
6-38:(1996)
6-39:A case for interaction: A study of interactive information retrieval behavior and effectiveness
6-40:Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205 212
6-41:Meddis, R., (1984)
6-42:Statistics using ranks: A unified approach
6-43:Oxford: Basil Blackwell, 303 308
6-44:Morita, M
6-45:and Shinoda, Y
6-46:(1994)
6-47:Information filtering based on user behavior analysis and best match text retrieval
6-48:Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272 281
6-49:Salton, G
6-50:and Buckley, C
6-51:(1990)
6-52:Improving retrieval performance by relevance feedback
6-53:Journal of the American Society for Information Science
6-54:41(4): 288 297
6-55:Siegel, S
6-56:and Castellan, N.J
6-57:(1988)
6-58:Nonparametric statistics for the behavioural sciences
6-59:2nd ed
6-60:Singapore: McGraw Hill
6-61:White, R.W
6-62:(2004)
6-63:Implicit feedback for interactive information retrieval
6-64:Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom
6-65:White, R.W., Jose, J.M
6-66:and Ruthven, I
6-67:(2005)
6-68:An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press
6-69:White, R.W., Jose, J.M., Ruthven, I
6-70:and Van Rijsbergen, C.J
6-71:(2004)
6-72:A simulated study of implicit feedback models
6-73:Proceedings of the 26th European Conference on Information Retrieval, 311 326
6-74:Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B. W
6-75:(2000)
6-76:The impact of fluid documents on reading and browsing: An observational study
6-77:Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249 256
6-78:Appendix B
6-79:Checkboxes to mark relevant document titles in the Explicit RF system
6-80:Appendix A
6-81:Interface to Implicit RF system
6-82:1
6-83:Top Ranking Sentence 2
6-84:Title 3
6-85:Summary 4
6-86:Summary Sentence 5
6-87:Sentence in Context 2 3 4 5 1
picture:
