Scouts, Promoters, and Connectors: The Roles of Ratings 
content:
1 ABSTRACT :
1-1:Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors .
1-2:The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e commerce .
1-3:We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering .
1-4:In particular, we formulate three roles scouts, promoters, and connectors that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.) .
1-5:These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole .
1-6:For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling .
1-7:We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community .
1-8:H.4.2 [Information Systems Applications]: Types of .
2 INTRODUCTION :
2-1:Recommender systems have become integral to e commerce, providing technology that suggests products to a visitor based on previous purchases or rating history .
2-2:Collaborative filtering, a common form of recommendation, predicts a user"s rating for an item by combining (other) ratings of that user with other users" ratings .
2-3:Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8] .
2-4:However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations .
2-5:Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success .
2-6:Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest neighbor collaborative filtering .
2-7:We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds .
2-8:Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations .
2-9:In turn, this capability helps support scenarios such as: ratings may inadvertently connect the user to a neighborhood for which the user"s tastes may not be a perfect match .
2-10:Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood .
2-11:lack of user participation, especially in cold start scenarios [13] involving newly arrived items .
2-12:Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system .
2-13:and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack .
2-14:Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved .
2-15:These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5] .
2-16:As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community .
2-17:The rest of the paper is organized as follows .
2-18:Background on nearest neighbor collaborative filtering and algorithm evaluation is discussed in Section 2 .
2-19:Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles .
2-20:In Section 5, we illustrate the use of these roles to address the goals outlined above. .
3 BACKGROUND :
3-1:2.1 Algorithms Nearest neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction .
3-2:An algorithm of the first kind is called user based, and one of the second kind is called itembased [12] .
3-3:In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user based) or items (for item based) .
3-4:Predictions are then computed by aggregating ratings, which in a user based algorithm involves aggregating the ratings of the target item by the user"s neighbors and, in an item based algorithm, involves aggregating the user"s ratings of items that are neighbors of the target item .
3-5:Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions .
3-6:We consider a user based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al .
3-7:[2], and an item based algorithm similar to that of Sarwar et al .
3-8:[12] .
3-9:The algorithm used by Resnick et al .
3-10:[11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user u"s rating for item i, and ¯ru is the average rating of user u (similarly for v) .
3-11:Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2] .
3-12:These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u .
3-13:A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i .
3-14:The item based algorithm we use is the one defined by Sarwar et al .
3-15:[12] .
3-16:In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i .
3-17:As for the user based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j) .
3-18:(3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item .
3-19:A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i .
3-20:2.2 Evaluation Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3] .
3-21:Studies on recommender algorithms, notably Herlocker et al .
3-22:[2] and Sarwar et al .
3-23:[12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set .
3-24:A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as, MAE = P (u,i)∈T |pu,i − ru,i| |T | .
3-25:(5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3] .
3-26:A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions .
3-27:Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star ratings), and so predictive accuracy may not be reflective of the accuracy of the list .
3-28:So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order .
3-29:Herlocker et al .
3-30:[3] discuss a number of rank accuracy measures, which range from Kendall"s Tau to measures that consider the fact that users tend to only look at a prefix of the list [5] .
3-31:Kendall"s Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender .
3-32:items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3] .
3-33:A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3] .
3-34:For instance, an inversion toward the end of the list is given the same weight as one in the beginning .
3-35:One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list. .
4 ROLES OF A RATING :
4-1:Our basic observation is that each rating plays a different role in each prediction in which it is used .
4-2:Consider a simplified movie recommender system with three users Jim, Jeff, and Tom and their ratings for a few movies, as shown in Fig .
4-3:1 .
4-4:(For this initial discussion we will not consider the rating values involved.) The recommender predicts whether Tom will like The Mask using the other already available ratings .
4-5:How this is done depends on the algorithm: constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jim"s ratings of The Matrix and The Mask; and Jeff"s ratings of Star Wars and The Mask) .
4-6:Tom"s ratings of those movies are then used to make a prediction for The Mask .
4-7:construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Tom"s (e.g., Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix) .
4-8:The prediction of Tom"s rating for The Mask is then based on the ratings of Jeff and Tim .
4-9:Although the nearest neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings .
4-10:So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Tom"s rating of The Mask as walking through a sequence of ratings .
4-11:In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom .
4-12:Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once .
4-13:this example, two paths were used for this prediction as depicted in Fig .
4-14:2: (p1, p2, p3) and (q1, q2, q3) .
4-15:Note that these paths are undirected, and are all of length 3 .
4-16:Only the order in which the ratings are traversed is different between the item based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).) A rating can be part of many paths for a single prediction as shown in Fig .
4-17:3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3) .
4-18:Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value .
4-19:Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors .
4-20:To illustrate these roles, consider the path (p1, p2, p3) in Fig .
4-21:2 used to make a prediction of The Mask for Tom: from Tom to other ratings that can be used to predict Tom"s rating for The Mask .
4-22:This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask .
4-23:recommend The Mask to Tom by connecting the scout to the promoter .
4-24:The Mask, and, therefore, promotes this movie to Tom .
4-25:Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors .
4-26:is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a .
4-27:The scouts, connectors, and promoters for the prediction of Tom"s rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively) .
4-28:Each of these roles has a value in the recommender to the user, the user"s neighborhood, and the system in terms of allowing recommendations to be made .
4-29:3.1 Roles in Detail Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users .
4-30:For example, in Fig .
4-31:4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny .
4-32:Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix .
4-33:However, this does not make The Matrix the best movie to rate for everyone .
4-34:For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him .
4-35:His rating of The Mask is the best scout for Jeff, and Jerry"s only scout is his rating of Star Wars .
4-36:This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system .
4-37:While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items .
4-38:In Fig .
4-39:4, My Cousin Vinny benefits from Jim"s rating, since it allows recommendations to Jeff and Tom .
4-40:The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it .
4-41:On the other hand, Jerry"s rating of Star Wars does not help promote it to any other user .
4-42:We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users .
4-43:Connectors serve a crucial role in a recommender system that is not as obvious .
4-44:The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig .
4-45:4 .
4-46:Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors .
4-47:A connector improves the system"s ability to make recommendations with no explicit gain for the user .
4-48:Note that every rating can be of varied benefit in each of these roles .
4-49:The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter .
4-50:The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter .
4-51:Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter .
4-52:As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not .
4-53:We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system .
4-54:But we can also measure the contribution of each rating to the quality of recommendations or health of the system .
4-55:Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the system"s overall error .
4-56:We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played .
4-57:The next section describes the approach to measuring the values of a rating in each role. .
5 CONTRIBUTIONS OF RATINGS :
5-1:As we"ve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways .
5-2:Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction .
5-3:By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system .
5-4:This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy .
5-5:We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a user"s recommendations .
5-6:Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item based collaborative filtering .
5-7:The definition for user based algorithms is similar and will be presented in an expanded version of this paper .
5-8:4.1 Influence of Ratings Recall that an item based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user .
5-9:As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)) .
5-10:A set of the top K neighbors is maintained for all items for space and computational efficiency .
5-11:A prediction of item i for a user u is computed as the weighted deviation from the item"s mean rating as shown in Equation (4) .
5-12:The list of recommendations for a user is then the list of items sorted in descending order of their predicted values .
5-13:We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j .
5-14:This is the change in the similarity between i and j when a"s rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\{a}(ru,i − ¯ru)2 qP u∈U\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j) .
5-15:If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0 .
5-16:The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1 .
5-17:4.2 Role Values for Predictive Accuracy The value of a rating in each role is computed from the influence depending on the evaluation measure employed .
5-18:Here we illustrate the approach using predictive accuracy as the evaluation metric .
5-19:In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role .
5-20:For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e) .
5-21:Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not .
5-22:We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1] .
5-23:For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences .
5-24:This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter .
5-25:In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount .
5-26:Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles) .
5-27:Every rating is thus summarized by its triple [SF, CF, PF] .
5-28:4.3 Role Values for Rank Accuracy We now define the computation of the contribution of ratings to observed rank accuracy .
5-29:For this computation, we must know the user"s preference order for a set of items for which predictions can be computed .
5-30:We assume that we have a test set of the users" ratings of the items presented in the recommendation list .
5-31:For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference .
5-32:We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ) .
5-33:Similarly, a pair (i, j) is discordant (with error ) if it is not concordant .
5-34:Our experiments described below use an error tolerance of = 0.1 .
5-35:All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited .
5-36:The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the user"s test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances .
5-37:The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences .
5-38:We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating .
5-39:4.4 Aggregating rating roles After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system .
5-40:Here we consider how we can use the role values to characterize the health of a neighborhood .
5-41:Consider the list of top recommendations presented to a user at a specific point in time .
5-42:The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations .
5-43:We call these ratings the recommender neighborhood of the user .
5-44:The user implicitly chooses this neighborhood of ratings through the items he rates .
5-45:Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a user"s satisfaction with the system .
5-46:We can characterize a user"s recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list .
5-47:Different sections of the user"s neighborhood wield varied influence on his recommendation list .
5-48:For instance, ratings reachable through highly rated items have a bigger say in the recommended items .
5-49:Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood .
5-50:A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters .
5-51:He can have a good neighborhood, but his bad scouts may ensure the neighborhood"s potential is rendered useless .
5-52:We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future .
5-53:A user"s neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations .
5-54:Consider a user u and his ordered list of recommendations L .
5-55:An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list .
5-56:In our studies we use K(i) = p position(i) .
5-57:Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i) .
5-58:The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) and PFN(u) are defined similarly .
5-59:This triple estimates the quality of u"s recommendations based on the past track record of the ratings involved in their respective roles. .
6 EXPERIMENTATION :
6-1:As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system .
6-2:In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset .
6-3:In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods .
6-4:5.1 Methodology Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies .
6-5:The ratings are in the range 1 to 5, and are labeled with the time the rating was given .
6-6:As discussed before, we consider only the item based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy .
6-7:Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order .
6-8:The timestamping provided by MovieLens is hence crucial for the analyses presented here .
6-9:We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots) .
6-10:We incrementally update the role values as the time ordered ratings are merged into the model .
6-11:To keep the experiment computationally manageable, we define a test dataset for each user .
6-12:As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data .
6-13:At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions .
6-14:One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles .
6-15:We overcome this concern by repeating the experiment, using different random seeds .
6-16:The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds .
6-17:The results here are based on n = 4 repetitions .
6-18:The item based collaborative filtering algorithm"s performance was ordinary with respect to rank accuracy .
6-19:Fig .
6-20:5 shows a plot of the precision and recall as ratings were merged in time order into the model .
6-21:The recall was always high, but the average precision was just about 53% .
6-22:0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item based collaborative filtering algorithm .
6-23:5.2 Inducing good scouts The ratings of a user that serve as scouts are those that allow the user to receive recommendations .
6-24:We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values .
6-25:Note that the item based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the user"s preference .
6-26:Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists .
6-27:The distribution of the scout values for most users" ratings are Gaussian with mean zero .
6-28:Fig .
6-29:6 shows the frequency distribution of scout values for a sample user at a given snapshot .
6-30:We observe that a large number of ratings never serve as scouts for their users .
6-31:A relatable scenario is when Amazon"s recommender makes suggestions of books or items based on other items that were purchased as gifts .
6-32:With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions .
6-33:Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal .
6-34:An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al .
6-35:[9] .
6-36:They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE .
6-37:Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts .
6-38:We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system .
6-39:Note that the scout values were initially anti correlated with popularity (Fig .
6-40:7), but became moderately correlated as the system evolved .
6-41:Both popularity and popularity*variance performed similarly .
6-42:A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings .
6-43:255 10 0 10 20 30 40 50 60 0.08 0.06 0.04 0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user .
6-44:0.4 0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals) .
6-45:0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals) .
6-46:Table 1: Movies forming the best scouts .
6-47:Best Scouts Conf .
6-48:Pop .
6-49:Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindler"s List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts .
6-50:Worst scouts Conf .
6-51:Pop .
6-52:Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a.. .
6-53:(1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 It"s a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time .
6-54:We claim these movies will make viable scouts for other users .
6-55:We found the aggregated scout values for all movies in intervals of 10,000 ratings each .
6-56:A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list .
6-57:Movies appearing consistently high over time are expected to remain up there in the future .
6-58:The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered .
6-59:Using this measure, the top few movies expected to induce the best scouts are shown in Table 1 .
6-60:Movies that would be bad scout choices are shown in Table 2 with their associated confidences .
6-61:The popularities of the movies are also displayed .
6-62:Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous .
6-63:Interestingly, the best scout ‘Being John Malkovich" is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as ‘makes you feel giddy," ‘seriously weird," ‘comedy with depth," ‘silly," ‘strange," and ‘inventive." Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences .
6-64:On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie .
6-65:Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw .
6-66:Bad scouts ensue when there is deviation in behavior around a common synchronization point .
6-67:5.3 Inducing good promoters Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users .
6-68:So, inducing good promoters is important for cold start recommendation .
6-69:We note that the frequency distribution of promoter values for a sample movie"s ratings is also Gaussian (similar to Fig .
6-70:6) .
6-71:This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users .
6-72:We find a strong correlation between a user"s number of ratings and his aggregated promoter value .
6-73:Fig .
6-74:8 depicts the evolution of the Pearson correlation co efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value .
6-75:We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily .
6-76:Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values .
6-77:A new movie is thus cast into the neighborhood of many other movies improving its visibility .
6-78:Note, though, that a user may have long stopped using the system .
6-79:Tracking promoter values consistently allows only the most active recent users to be considered .
6-80:5.4 Inducing good connectors Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors .
6-81:Similarly, the users that constitute the best promoters are also part of the best connectors .
6-82:Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value .
6-83:In our experiments, we find that a rating"s longest standing role is often as a connector .
6-84:A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout .
6-85:Such ratings can be removed from the prediction process to bring marginal improvements to recommendations .
6-86:In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the system"s overall performance by 1.5% .
6-87:The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall .
6-88:5.5 Monitoring the evolution of rating roles One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time .
6-89:The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used .
6-90:Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system .
6-91:We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system .
6-92:We classify different rating states as good, bad, or negligible .
6-93:Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set .
6-94:If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good .
6-95:Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value .
6-96:For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good, 0 indicates negligible, and − indicates bad .
6-97:The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system .
6-98:Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size .
6-99:For example, even though only 1.7% of all ratings were classified as good scouts, they hold 79% of all positive credit in the system! Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit .
6-100:Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later) .
6-101:Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit .
6-102:The bad connectors (0.8% of the system) hold 36% of all discredit .
6-103:Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit .
6-104:This reiterates that a few ratings influence most of the system"s performance .
6-105:Hence it is important to track transitions between them regardless of their small numbers .
6-106:257 Across different snapshots, a rating can remain in the same state or change .
6-107:A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on .
6-108:It is not practical to expect a recommender system to have no ratings in bad roles .
6-109:However, it suffices to see ratings in bad roles either convert to good or vestigial roles .
6-110:Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system .
6-111:We employ the principle of non overlapping episodes [6] to count such transitions .
6-112:A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1 .
6-113:See [6] for further details about this counting procedure .
6-114:Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions .
6-115:We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role .
6-116:Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states .
6-117:In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted .
6-118:Fig .
6-119:9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset .
6-120:Only transitions with frequency greater than or equal to 3% are shown .
6-121:The percentages for each state indicates the number of ratings that were found to be in those states .
6-122:We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying .
6-123:From Fig .
6-124:9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role .
6-125:The majority of the transitions involve both good and bad ratings becoming negligible .
6-126:Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%) Connector(0.8%) Connector 0 (98%) Promoter + (3%) Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles .
6-127:• The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below) .
6-128:• The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles .
6-129:• Good promoters and good connectors have a much longer survival period than scouts .
6-130:Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts .
6-131:Good connectors are the slowest to decay whereas (good) scouts decay the fastest .
6-132:• Healthy percentages are seen on transitions between promoters and connectors .
6-133:As indicated earlier, there are hardly any transitions from promoters connectors to scouts .
6-134:This indicates that, over the long run, a user"s rating is more useful to others (movies or other users) than to the user himself .
6-135:• The percentages of healthy transitions outweigh the unhealthy ones this hints that the system is healthy, albeit only marginally .
6-136:Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time .
6-137:However a diagram such as Fig .
6-138:9 is clearly useful for monitoring the health of a recommender system .
6-139:For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny .
6-140:Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research .
6-141:5.6 Characterizing neighborhoods Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user .
6-142:In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics .
6-143:To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories: .
7 Inactive user: (SFN(u) = 0) :
7-1:.
8 Good scouts, Good neighborhood: :
8-1:(SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) .
9 Good scouts, Bad neighborhood: :
9-1:(SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) .
10 Bad scouts, Good neighborhood: :
10-1:(SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) .
11 Bad scouts, Bad neighborhood: :
11-1:(SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive .
11-2:Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood .
11-3:Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system .
11-4:As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the system"s recommendations .
11-5:The system can be expected to deliver more if they engineer some good scouts .
11-6:Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector promoter pairs that are causing the damage .
11-7:Handling users with bad scouts and bad neighborhoods is a more difficult challenge .
11-8:Such a classification allows the use of different strategies to better a user"s experience with the system depending on his context .
11-9:In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts. .
12-1:To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings
12-2:A fine grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor
12-3:Although we have presented results on only the item based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user based algorithms and other metrics
12-4:In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions
12-5:We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values
12-6:Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health.
13-1:Cosley, D., Lam, S., Albert, I., Konstan, J., and Riedl, J
13-2:Is Seeing Believing?: How Recommender System Interfaces Affect User"s Opinions
13-3:In Proc
13-4:CHI (2001), pp
13-5:585 592
13-6:Herlocker, J
13-7:L., Konstan, J
13-8:A., Borchers, A., and Riedl, J
13-9:An Algorithmic Framework for Performing Collaborative Filtering
13-10:In Proc
13-11:SIGIR (1999), pp
13-12:230 237
13-13:Herlocker, J
13-14:L., Konstan, J
13-15:A., Terveen, L
13-16:G., and Riedl, J
13-17:T
13-18:Evaluating Collaborative Filtering Recommender Systems
13-19:ACM Transactions on Information Systems Vol
13-20:22, 1 (2004), pp
13-21:5 53
13-22:Konstan, J
13-23:A
13-24:Personal communication
13-25:2003
13-26:Lam, S
13-27:K., and Riedl, J
13-28:Shilling Recommender Systems for Fun and Profit
13-29:In Proceedings of the 13th International World Wide Web Conference (2004), ACM Press, pp
13-30:393 402
13-31:Laxman, S., Sastry, P
13-32:S., and Unnikrishnan, K
13-33:P
13-34:Discovering Frequent Episodes and Learning Hidden Markov Models: A Formal Connection
13-35:IEEE Transactions on Knowledge and Data Engineering Vol
13-36:17, 11 (2005), 1505 1517
13-37:McLaughlin, M
13-38:R., and Herlocker, J
13-39:L
13-40:A Collaborative Filtering Algorithm and Evaluation Metric that Accurately Model the User Experience
13-41:In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (2004), pp
13-42:329  336
13-43:O"Mahony, M., Hurley, N
13-44:J., Kushmerick, N., and Silvestre, G
13-45:Collaborative Recommendation: A Robustness Analysis
13-46:ACM Transactions on Internet Technology Vol
13-47:4, 4 (Nov 2004), pp
13-48:344 377
13-49:Rashid, A
13-50:M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J
13-51:A., and Riedl, J
13-52:Getting to Know You: Learning New User Preferences in Recommender Systems
13-53:In Proceedings of the 2002 Conference on Intelligent User Interfaces (IUI 2002) (2002), pp
13-54:127 134
13-55:Rashid, A
13-56:M., Karypis, G., and Riedl, J
13-57:Influence in Ratings Based Recommender Systems: An Algorithm Independent Approach
13-58:In Proc
13-59:of the SIAM International Conference on Data Mining (2005)
13-60:Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., and Riedl, J
13-61:GroupLens: An Open Architecture for Collaborative Filtering of Netnews
13-62:In Proceedings of the Conference on Computer Supported Collaborative Work (CSCW"94) (1994), ACM Press, pp
13-63:175 186
13-64:Sarwar, B., Karypis, G., Konstan, J., and Reidl,
picture:
