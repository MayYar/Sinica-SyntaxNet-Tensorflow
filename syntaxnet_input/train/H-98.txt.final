Using Asymmetric Distributions 
content:
1 ABSTRACT :
1-1:Text classifiers that give probability estimates are more readily applicable in a variety of scenarios .
1-2:For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run time decision which minimizes a userspecified cost function dynamically chosen at prediction time .
1-3:However, the quality of the probability estimates is crucial .
1-4:We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the extremely irrelevant, hard to discriminate, and obviously relevant items are often significantly different .
1-5:Finally, we analyze the experimental performance of these models over the outputs of two text classifiers .
1-6:The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable .
1-7:H.3.3 [Information Storage and Retrieval]: Information Search .
2 INTRODUCTION :
2-1:Text classifiers that give probability estimates are more flexible in practice than those that give only a simple classification or even a ranking .
2-2:For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model [8] to issue a runtime decision which minimizes the expected cost of a user specified cost function dynamically chosen at prediction time .
2-3:This can be used to minimize a linear utility cost function for filtering tasks where pre specified costs of relevant irrelevant are not available during training but are specified at prediction time .
2-4:Furthermore, the costs can be changed without retraining the model .
2-5:Additionally, probability estimates are often used as the basis of deciding which document"s label to request next during active learning [17, 23] .
2-6:Effective active learning can be key in many information retrieval tasks where obtaining labeled data can be costly severely reducing the amount of labeled data needed to reach the same performance as when new labels are requested randomly [17] .
2-7:Finally, they are also amenable to making other types of cost sensitive decisions [26] and for combining decisions [3] .
2-8:However, in all of these tasks, the quality of the probability estimates is crucial .
2-9:Parametric models generally use assumptions that the data conform to the model to trade off flexibility with the ability to estimate the model parameters accurately with little training data .
2-10:Since many text classification tasks often have very little training data, we focus on parametric methods .
2-11:However, most of the existing parametric methods that have been applied to this task have an assumption we find undesirable .
2-12:While some of these methods allow the distributions of the documents relevant and irrelevant to the topic to have different variances, they typically enforce the unnecessary constraint that the documents are symmetrically distributed around their respective modes .
2-13:We introduce several asymmetric parametric models that allow us to relax this assumption without significantly increasing the number of parameters and demonstrate how we can efficiently fit the models .
2-14:Additionally, these models can be interpreted as assuming the scores produced by the text classifier have three basic types of empirical behavior one corresponding to each of the extremely irrelevant, hard to discriminate, and obviously relevant items .
2-15:We first review related work on improving probability estimates and score modeling in information retrieval .
2-16:Then, we discuss in further detail the need for asymmetric models .
2-17:After this, we describe two specific asymmetric models and, using two standard text classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be efficiently used to recalibrate poor probability estimates or produce high quality probability estimates from raw scores .
2-18:We then review experiments using previously proposed methods and the asymmetric methods over several text classification corpora to demonstrate the strengths and weaknesses of the various methods .
2-19:Finally, we summarize our contributions and discuss future directions. .
3 RELATED WORK :
3-1:Parametric models have been employed to obtain probability estimates in several areas of information retrieval .
3-2:Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes though the quality of the probability estimates are not directly evaluated; it is simply performed as an intermediate step in active learning .
3-3:Manmatha et .
3-4:al [20] introduced models appropriate to produce probability estimates from relevance scores returned from search engines and demonstrated how the resulting probability estimates could be subsequently employed to combine the outputs of several search engines .
3-5:They use a different parametric distribution for the relevant and irrelevant classes, but do not pursue two sided asymmetric distributions for a single class as described here .
3-6:They also survey the long history of modeling the relevance scores of search engines .
3-7:Our work is similar in flavor to these previous attempts to model search engine scores, but we target text classifier outputs which we have found demonstrate a different type of score distribution behavior because of the role of training data .
3-8:Focus on improving probability estimates has been growing lately .
3-9:Zadrozny & Elkan [26] provide a corrective measure for decision trees (termed curtailment) and a non parametric method for recalibrating na¨ıve Bayes .
3-10:In more recent work [27], they investigate using a semi parametric method that uses a monotonic piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a linear SVM .
3-11:While they compared their methods to other parametric methods based on symmetry, they fail to provide significance test results .
3-12:Our work provides asymmetric parametric methods which complement the non parametric and semi parametric methods they propose when data scarcity is an issue .
3-13:In addition, their methods reduce the resolution of the scores output by the classifier (the number of distinct values output), but the methods here do not have such a weakness since they are continuous functions .
3-14:There is a variety of other work that this paper extends .
3-15:Platt [22] uses a logistic regression framework that models noisy class labels to produce probabilities from the raw output of an SVM .
3-16:His work showed that this post processing method not only can produce probability estimates of similar quality to SVMs directly trained to produce probabilities (regularized likelihood kernel methods), but it also tends to produce sparser kernels (which generalize better) .
3-17:Finally, Bennett [1] obtained moderate gains by applying Platt"s method to the recalibration of na¨ıve Bayes but found there were more problematic areas than when it was applied to SVMs .
3-18:Recalibrating poorly calibrated classifiers is not a new problem .
3-19:Lindley et .
3-20:al [19] first proposed the idea of recalibrating classifiers, and DeGroot & Fienberg [5, 6] gave the now accepted standard formalization for the problem of assessing calibration initiated by others [4, 24]. .
4 PROBLEM DEFINITION & APPROACH :
4-1:Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text classifier outputs where a majority of the previous literature targeted the output of search engines .
4-2:3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1 .
4-3:A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic) .
4-4:We assume throughout there are only two classes: the positive and the negative (or irrelevant) class ("+" and " " respectively) .
4-5:There are two general types of parametric approaches .
4-6:The first of these tries to fit the posterior function directly, i.e .
4-7:there is one p(s|+) p(s|−) Bayes" RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box highlighted in grey .
4-8:The internals are for one type of approach .
4-9:function estimator that performs a direct mapping of the score s to the probability P(+|s(d)) .
4-10:The second type of approach breaks the problem down as shown in the grey box of Figure 1 .
4-11:An estimator for each of the class conditional densities (i.e .
4-12:p(s|+) and p(s|−)) is produced, then Bayes" rule and the class priors are used to obtain the estimate for P(+|s(d)) .
4-13:3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem either directly or indirectly (when fitting only the posterior) correspond to fitting Gaussians to the class conditional densities; they differ only in the criterion used to estimate the parameters .
4-14:We can visualize this as depicted in Figure 2 .
4-15:Since increasing s usually indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+) .
4-16:A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen .
4-17:Namely, if we have a raw output score that can be used for discrimination, then the empirical behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2) .
4-18:Intuitively, the area between the modes corresponds to the hard examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished .
4-19:This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A Gaussian in Figure 3) .
4-20:As a result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier .
4-21:Ideally (i.e .
4-22:perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant .
4-23:Furthermore, no examples fall between θ− and θ+ .
4-24:The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity .
4-25:Because text classifiers have training data to use to separate the classes, the final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved .
4-26:This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query .
4-27:Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes .
4-28:Practically, some examples will fall between θ− and θ+, and it is often important to estimate the probabilities of these examples well (since they correspond to the hard examples) .
4-29:Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric .
4-30:A natural first candidate for an asymmetric distribution is to generalize a common symmetric distribution, e.g .
4-31:the Laplace or the Gaussian .
4-32:An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters .
4-33:θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right .
4-34:We will use the notation Λ(X | θ, β, γ) to refer to this distribution .
4-35:0 0.002 0.004 0.006 0.008 0.01 300 200 100 0 100 200 p(s|Class={+, }) Unnormalized Confidence Score s Gaussian A Gaussian Figure 3: Gaussians vs .
4-36:Asymmetric Gaussians .
4-37:A Shortcoming of Symmetric Distributions The vertical lines show the modes as estimated nonparametrically .
4-38:We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters .
4-39:To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr) .
4-40:While these distributions are composed of halves, the resulting function is a single continuous distribution .
4-41:These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters .
4-42:We could instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive) .
4-43:In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way .
4-44:Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4) .
4-45:To our knowledge, neither family of distributions has been previously used in machine learning or information retrieval .
4-46:Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task .
4-47:3.3 Estimating the Parameters of the Asymmetric Distributions This section develops the method for finding maximum likelihood estimates of the parameters for the above asymmetric distributions .
4-48:In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ .
4-49:Because of the simplicity of analysis in the latter alternative, we choose this method .
4-50:3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, .
4-51:.
4-52:.
4-53:, xN } where the xi are i.i.d .
4-54:and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ) .
4-55:Now, we fix θ and compute the maximum likelihood for that choice of θ .
4-56:Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ .
4-57:The complete derivation is omitted because of space but is available in [2] .
4-58:We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ .
4-59:Note that Dl and Dr are the sum of the absolute differences between the x belonging to the left and right halves of the distribution (respectively) and θ .
4-60:Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl .
4-61:(3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ .
4-62:The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e .
4-63:the closer Dl and Dr are to being equal, the closer the resulting inverse scales) .
4-64:By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform .
4-65:Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e .
4-66:almost all mass at θ for that half) .
4-67:Dr = 0 is handled similarly .
4-68:Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily computable .
4-69:Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time .
4-70:In addition, if the scores are sorted, then we can perform the whole process quite efficiently .
4-71:Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately .
4-72:Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left .
4-73:Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is dominated by sorting the scores, O(n log n) (or expected linear time) .
4-74:3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, .
4-75:.
4-76:.
4-77:, xN } where the xi are i.i.d .
4-78:and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ) .
4-79:The MLEs can be worked out similar to the above .
4-80:We assume the same definitions as above (the complete derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr .
4-81:The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2 3 l2 D 1 3 r2 N (4) σr,MLE = Dr2 + D 2 3 r2 D 1 3 l2 N .
4-82:(5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp .
4-83:Dr2 = 0), we assign σl = 0 (resp .
4-84:σr = 0) .
4-85:Again, the same computational complexity analysis applies to estimating these parameters. .
5 EXPERIMENTAL ANALYSIS :
5-1:4.1 Methods For each of the methods that use a class prior, we use a smoothed add one estimate, i.e .
5-2:P(c) = |c|+1 N+2 where N is the number of documents .
5-3:For methods that fit the class conditional densities, p(s|+) and p(s|−), the resulting densities are inverted using Bayes" rule as described above .
5-4:All of the methods below are fit using maximum likelihood estimates .
5-5:For recalibrating a classifier (i.e .
5-6:correcting poor probability estimates output by the classifier), it is usual to use the log odds of the classifier"s estimate as s(d) .
5-7:The log odds are defined to be log P (+|d) P (−|d) .
5-8:The normal decision threshold (minimizing error) in terms of log odds is at zero (i.e .
5-9:P(+|d) = P(−|d) = 0.5) .
5-10:Since it scales the outputs to a space [−∞, ∞], the log odds make normal (and similar distributions) applicable [19] .
5-11:Lewis & Gale [17] give a more motivating viewpoint that fitting the log odds is a dampening effect for the inaccurate independence assumption and a bias correction for inaccurate estimates of the priors .
5-12:In general, fitting the log odds can serve to boost or dampen the signal from the original classifier as the data dictate .
5-13:Gaussians A Gaussian is fit to each of the class conditional densities, using the usual maximum likelihood estimates .
5-14:This method is denoted in the tables below as Gauss .
5-15:Asymmetric Gaussians An asymmetric Gaussian is fit to each of the class conditional densities using the maximum likelihood estimation procedure described above .
5-16:Intervals between adjacent scores are divided by 10 in testing candidate θs, i.e .
5-17:8 points between actual scores occurring in the data set are tested .
5-18:This method is denoted as A .
5-19:Gauss .
5-20:Laplace Distributions Even though Laplace distributions are not typically applied to this task, we also tried this method to isolate why benefit is gained from the asymmetric form .
5-21:The usual MLEs were used for estimating the location and scale of a classical symmetric Laplace distribution as described in [14] .
5-22:We denote this method as Laplace below .
5-23:Asymmetric Laplace Distributions An asymmetric Laplace is fit to each of the class conditional densities using the maximum likelihood estimation procedure described above .
5-24:As with the asymmetric Gaussian, intervals between adjacent scores are divided by 10 in testing candidate θs .
5-25:This method is denoted as A .
5-26:Laplace below .
5-27:Logistic Regression This method is the first of two methods we evaluated that directly fit the posterior, P(+|s(d)) .
5-28:Both methods restrict the set of families to a two parameter sigmoid family; they differ primarily in their model of class labels .
5-29:As opposed to the above methods, one can argue that an additional boon of these methods is they completely preserve the ranking given by the classifier .
5-30:When this is desired, these methods may be more appropriate .
5-31:The previous methods will mostly preserve the rankings, but they can deviate if the data dictate it .
5-32:Thus, they may model the data behavior better at the cost of departing from a monotonicity constraint in the output of the classifier .
5-33:Lewis & Gale [17] use logistic regression to recalibrate na¨ıve Bayes for subsequent use in active learning .
5-34:The model they use is: P(+|s(d)) = exp(a + b s(d)) 1 + exp(a + b s(d)) .
5-35:(6) Instead of using the probabilities directly output by the classifier, they use the loglikelihood ratio of the probabilities, log P (d|+) P (d|−) , as the score s(d) .
5-36:Instead of using this below, we will use the logodds ratio .
5-37:This does not affect the model as it simply shifts all of the scores by a constant determined by the priors .
5-38:We refer to this method as LogReg below .
5-39:Logistic Regression with Noisy Class Labels Platt [22] proposes a framework that extends the logistic regression model above to incorporate noisy class labels and uses it to produce probability estimates from the raw output of an SVM .
5-40:This model differs from the LogReg model only in how the parameters are estimated .
5-41:The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled .
5-42:The noise is modeled by assuming there is a finite probability of mislabeling a positive example and of mislabeling a negative example; these two noise estimates are determined by the number of positive examples and the number of negative examples (using Bayes" rule to infer the probability of incorrect label) .
5-43:Even though the performance of this model would not be expected to deviate much from LogReg, we evaluate it for completeness .
5-44:We refer to this method below as LR+Noise .
5-45:4.2 Data We examined several corpora, including the MSN Web Directory, Reuters, and TREC AP .
5-46:MSN Web Directory The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified .
5-47:We used the same train test split of 50078 10024 documents as that reported in [9] .
5-48:The MSN Web hierarchy is a seven level hierarchy; we used all 13 of the top level categories .
5-49:The class proportions in the training set vary from 1.15% to 22.29% .
5-50:In the testing set, they range from 1.14% to 21.54% .
5-51:The classes are general subjects such as Health & Fitness and Travel & Vacation .
5-52:Human indexers assigned the documents to zero or more categories .
5-53:For the experiments below, we used only the top 1000 words with highest mutual information for each class; approximately 195K words appear in at least three training documents .
5-54:Reuters The Reuters 21578 corpus [16] contains Reuters news articles from 1987 .
5-55:For this data set, we used the ModApte standard train test split of 9603 3299 documents (8676 unused documents) .
5-56:The classes are economic subjects (e.g., acq for acquisitions, earn for earnings, etc.) that human taggers applied to the document; a document may have multiple subjects .
5-57:There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most frequent classes since small numbers of testing examples make interpreting some performance measures difficult due to high variance.1 Limiting to the ten largest classes allows us to compare our results to previously published results [10, 13, 21, 22] .
5-58:The class proportions in the training set vary from 1.88% to 29.96% .
5-59:In the testing set, they range from 1.7% to 32.95% .
5-60:For the experiments below we used only the top 300 words with highest mutual information for each class; approximately 15K words appear in at least three training documents .
5-61:TREC AP The TREC AP corpus is a collection of AP news stories from 1988 to 1990 .
5-62:We used the same train test split of 142791 66992 documents that was used in [18] .
5-63:As described in [17] (see also [15]), the categories are defined by keywords in a keyword field .
5-64:The title and body fields are used in the experiments below .
5-65:There are twenty categories in total .
5-66:The class proportions in the training set vary from 0.06% to 2.03% .
5-67:In the testing set, they range from 0.03% to 4.32% .
5-68:For the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123K words appear in at least 3 training documents .
5-69:4.3 Classifiers We selected two classifiers for evaluation .
5-70:A linear SVM classifier which is a discriminative classifier that does not normally output probability values, and a na¨ıve Bayes classifier whose probability outputs are often poor [1, 7] but can be improved [1, 26, 27] .
5-71:1 A separate comparison of only LogReg, LR+Noise, and After accounting for the variance, that evaluation also supported the claims made here .
5-72:SVM For linear SVMs, we use the Smox toolkit which is based on Platt"s Sequential Minimal Optimization algorithm .
5-73:The features were represented as continuous values .
5-74:We used the raw output score of the SVM as s(d) since it has been shown to be appropriate before [22] .
5-75:The normal decision threshold (assuming we are seeking to minimize errors) for this classifier is at zero .
5-76:Na¨ıve Bayes The na¨ıve Bayes classifier model is a multinomial model [21] .
5-77:We smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m estimate, respectively .
5-78:We use the log odds estimated by the classifier as s(d) .
5-79:The normal decision threshold is at zero .
5-80:4.4 Performance Measures We use log loss [12] and squared error [4, 6] to evaluate the quality of the probability estimates .
5-81:For a document d with class c(d) ∈ {+, −} (i.e .
5-82:the data have known labels and not probabilities), logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d) where δ(a, b) .
5-83:= 1 if a = b and 0 otherwise .
5-84:The squared error is δ(c(d), +)(1 − P(+|d))2 + δ(c(d), −)(1 − P(−|d))2 .
5-85:When the class of a document is correctly predicted with a probability of one, log loss is zero and squared error is zero .
5-86:When the class of a document is incorrectly predicted with a probability of one, log loss is −∞ and squared error is one .
5-87:Thus, both measures assess how close an estimate comes to correctly predicting the item"s class but vary in how harshly incorrect predictions are penalized .
5-88:We report only the sum of these measures and omit the averages for space .
5-89:Their averages, average log loss and mean squared error (MSE), can be computed from these totals by dividing by the number of binary decisions in a corpus .
5-90:In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities .
5-91:This evaluates how the probability estimates have improved with respect to the decision threshold P(+|d) = 0.5 .
5-92:Thus, error only indicates how the methods would perform if a false positive was penalized the same as a false negative and not the general quality of the probability estimates .
5-93:It is presented simply to provide the reader with a more complete understanding of the empirical tendencies of the methods .
5-94:We use a a standard paired micro sign test [25] to determine statistical significance in the difference of all measures .
5-95:Only pairs that the methods disagree on are used in the sign test .
5-96:This test compares pairs of scores from two systems with the null hypothesis that the number of items they disagree on are binomially distributed .
5-97:We use a significance level of p = 0.01 .
5-98:4.5 Experimental Methodology As the categories under consideration in the experiments are not mutually exclusive, the classification was done by training n binary classifiers, where n is the number of classes .
5-99:In order to generate the scores that each method uses to fit its probability estimates, we use five fold cross validation on the training data .
5-100:We note that even though it is computationally efficient to perform leave one out cross validation for the na¨ıve Bayes classifier, this may not be desirable since the distribution of scores can be skewed as a result .
5-101:Of course, as with any application of n fold cross validation, it is also possible to bias the results by holding n too low and underestimating the performance of the final classifier .
5-102:4.6 Results & Discussion The results for recalibrating na¨ıve Bayes are given in Table 1a .
5-103:Table 1b gives results for producing probabilistic outputs for SVMs .
5-104:Log loss Error2 Errors MSN Web Gauss 60656.41 10503.30 10754 A.Gauss 57262.26 8727.47 9675 Laplace 45363.84 8617.59 10927 A.Laplace 36765.88 6407.84† 8350 LogReg 36470.99 6525.47 8540 LR+Noise 36468.18 6534.61 8563 na¨ıve Bayes 1098900.83 17117.50 17834 Reuters Gauss 5523.14 1124.17 1654 A.Gauss 4929.12 652.67 888 Laplace 5677.68 1157.33 1416 A.Laplace 3106.95‡ 554.37‡ 726 LogReg 3375.63 603.20 786 LR+Noise 3374.15 604.80 785 na¨ıve Bayes 52184.52 1969.41 2121 TREC AP Gauss 57872.57 8431.89 9705 A.Gauss 66009.43 7826.99 8865 Laplace 61548.42 9571.29 11442 A.Laplace 48711.55 7251.87‡ 8642 LogReg 48250.81 7540.60 8797 LR+Noise 48251.51 7544.84 8801 na¨ıve Bayes 1903487.10 41770.21 43661 Log loss Error2 Errors MSN Web Gauss 54463.32 9090.57 10555 A promising extension to the work presented here is a hybrid distribution of a Gaussian (on the outside slopes) and exponentials (on the inner slopes) .
5-105:From the empirical evidence presented in [22], the expectation is that such a distribution might allow more emphasis of the probability mass around the modes (as with the exponential) while still providing more accurate estimates toward the tails .
5-106:Just as logistic regression allows the log odds of the posterior distribution to be fit directly with a line, we could directly fit the log odds of the posterior with a three piece line (a spline) instead of indirectly doing the same thing by fitting the asymmetric Laplace .
5-107:This approach may provide more power since it retains the asymmetry assumption but not the assumption that the class conditional densities are from an asymmetric Laplace .
5-108:Finally, extending these methods to the outputs of other discriminative classifiers is an open area .
5-109:We are currently evaluating the appropriateness of these methods for the output of a voted perceptron [11] .
5-110:By analogy to the log odds, the operative score that appears promising is log weight perceptrons voting + weight perceptrons voting − . .
6-1:We have reviewed a wide variety of parametric methods for producing probability estimates from the raw scores of a discriminative classifier and for recalibrating an uncalibrated probabilistic classifier
6-2:In addition, we have introduced two new families that attempt to capitalize on the asymmetric behavior that tends to arise from learning a discrimination function
6-3:We have given an efficient way to estimate the parameters of these distributions
6-4:While these distributions attempt to strike a balance between the generalization power of parametric distributions and the flexibility that the added asymmetric parameters give, the asymmetric Gaussian appears to have too great of an emphasis away from the modes
6-5:In striking contrast, the asymmetric Laplace distribution appears to be preferable over several large text domains and a variety of performance measures to the primary competing parametric methods, though comparable performance is sometimes achieved with one of two varieties of logistic regression
6-6:Given the ease of estimating the parameters of this distribution, it is a good first choice for producing quality probability estimates
6-7:Acknowledgments We are grateful to Francisco Pereira for the sign test code, Anton Likhodedov for logistic regression code, and John Platt for the code support for the linear SVM classifier toolkit Smox
6-8:Also, we sincerely thank Chris Meek and John Platt for the very useful advice provided in the early stages of this work
6-9:Thanks also to Jaime Carbonell and John Lafferty for their useful feedback on the final versions of this paper.
7-1:P
7-2:N
7-3:Bennett
7-4:Assessing the calibration of naive bayes" posterior estimates
7-5:Technical Report CMU CS 00 155, Carnegie Mellon, School of Computer Science, 2000
7-6:P
7-7:N
7-8:Bennett
7-9:Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods
7-10:Technical Report CMU CS 02 126, Carnegie Mellon, School of Computer Science, 2002
7-11:H
7-12:Bourlard and N
7-13:Morgan
7-14:A continuous speech recognition system embedding mlp into hmm
7-15:In NIPS "89, 1989
7-16:G
7-17:Brier
7-18:Verification of forecasts expressed in terms of probability
7-19:Monthly Weather Review, 78:1 3, 1950
7-20:M
7-21:H
7-22:DeGroot and S
7-23:E
7-24:Fienberg
7-25:The comparison and evaluation of forecasters
7-26:Statistician, 32:12 22, 1983
7-27:M
7-28:H
7-29:DeGroot and S
7-30:E
7-31:Fienberg
7-32:Comparing probability forecasters: Basic binary concepts and multivariate extensions
7-33:In P
7-34:Goel and A
7-35:Zellner, editors, Bayesian Inference and Decision Techniques
7-36:Elsevier Science Publishers B.V., 1986
7-37:P
7-38:Domingos and M
7-39:Pazzani
7-40:Beyond independence: Conditions for the optimality of the simple bayesian classifier
7-41:In ICML "96, 1996
7-42:R
7-43:Duda, P
7-44:Hart, and D
7-45:Stork
7-46:Pattern Classification
7-47:John Wiley & Sons, Inc., 2001
7-48:S
7-49:T
7-50:Dumais and H
7-51:Chen
7-52:Hierarchical classification of web content
7-53:In SIGIR "00, 2000
7-54:S
7-55:T
7-56:Dumais, J
7-57:Platt, D
7-58:Heckerman, and M
7-59:Sahami
7-60:Inductive learning algorithms and representations for text categorization
7-61:In CIKM "98, 1998
7-62:Y
7-63:Freund and R
7-64:Schapire
7-65:Large margin classification using the perceptron algorithm
7-66:Machine Learning, 37(3):277 296, 1999
7-67:I
7-68:Good
7-69:Rational decisions
7-70:Journal of the Royal Statistical Society, Series B, 1952
7-71:T
7-72:Joachims
7-73:Text categorization with support vector machines: Learning with many relevant features
7-74:In ECML "98, 1998
7-75:S
7-76:Kotz, T
7-77:J
7-78:Kozubowski, and K
7-79:Podgorski
7-80:The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance
7-81:Birkh¨auser, 2001
7-82:D
7-83:D
7-84:Lewis
7-85:A sequential algorithm for training text classifiers: Corrigendum and additional data
7-86:SIGIR Forum, 29(2):13 19, Fall 1995
7-87:D
7-88:D
7-89:Lewis
7-90:Reuters 21578, distribution 1.0
7-91:http:  www.daviddlewis.com resources  testcollections reuters21578, January 1997
7-92:D
7-93:D
7-94:Lewis and W
7-95:A
7-96:Gale
7-97:A sequential algorithm for training text classifiers
7-98:In SIGIR "94, 1994
7-99:D
7-100:D
7-101:Lewis, R
7-102:E
7-103:Schapire, J
7-104:P
7-105:Callan, and R
7-106:Papka
7-107:Training algorithms for linear text classifiers
7-108:In SIGIR "96, 1996
7-109:D
7-110:Lindley, A
7-111:Tversky, and R
7-112:Brown
7-113:On the reconciliation of probability assessments
7-114:Journal of the Royal Statistical Society, 1979
7-115:R
7-116:Manmatha, T
7-117:Rath, and F
7-118:Feng
7-119:Modeling score distributions for combining the outputs of search engines
7-120:In SIGIR "01, 2001
7-121:A
7-122:McCallum and K
7-123:Nigam
7-124:A comparison of event models for naive bayes text classification
7-125:In AAAI "98, Workshop on Learning for Text Categorization, 1998
7-126:J
7-127:C
7-128:Platt
7-129:Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods
7-130:In A
7-131:J
7-132:Smola, P
7-133:Bartlett, B
7-134:Scholkopf, and D
7-135:Schuurmans, editors, Advances in Large Margin Classifiers
7-136:MIT Press, 1999
7-137:M
7-138:Saar Tsechansky and F
7-139:Provost
7-140:Active learning for class probability estimation and ranking
7-141:In IJCAI "01, 2001
7-142:R
7-143:L
7-144:Winkler
7-145:Scoring rules and the evaluation of probability assessors
7-146:Journal of the American Statistical Association, 1969
7-147:Y
7-148:Yang and X
7-149:Liu
7-150:A re examination of text categorization methods
7-151:In SIGIR "99, 1999
7-152:B
7-153:Zadrozny and C
7-154:Elkan
7-155:Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers
7-156:In ICML "01, 2001
7-157:B
7-158:Zadrozny and C
7-159:Elkan
7-160:Reducing multiclass to binary by coupling probability estimates
7-161:In KDD "02, 2002
picture:
