Performance Prediction Using Spatial Autocorrelation 
content:
1 ABSTRACT :
1-1:Evaluation of information retrieval systems is one of the core tasks in information retrieval .
1-2:Problems include the inability to exhaustively label all documents for a topic, nongeneralizability from a small number of topics, and incorporating the variability of retrieval systems .
1-3:Previous work addresses the evaluation of systems, the ranking of queries by difficulty, and the ranking of individual retrievals by performance .
1-4:Approaches exist for the case of few and even no relevance judgments .
1-5:Our focus is on zero judgment performance prediction of individual retrievals .
1-6:One common shortcoming of previous techniques is the assumption of uncorrelated document scores and judgments .
1-7:If documents are embedded in a high dimensional space (as they often are), we can apply techniques from spatial data analysis to detect correlations between document scores .
1-8:We find that the low correlation between scores of topically close documents often implies a poor retrieval performance .
1-9:When compared to a state of the art baseline, we demonstrate that the spatial analysis of retrieval scores provides significantly better prediction performance .
1-10:These new predictors can also be incorporated with classic predictors to improve performance further .
1-11:We also describe the first large scale experiment to evaluate zero judgment performance prediction for a massive number of retrieval systems over a variety of collections in several languages .
1-12:H.3.3 [Information Search and Retrieval]: Retrieval .
2 INTRODUCTION :
2-1:In information retrieval, a user poses a query to a system .
2-2:The system retrieves n documents each receiving a realvalued score indicating the predicted degree of relevance .
2-3:If we randomly select pairs of documents from this set, we expect some pairs to share the same topic and other pairs to not share the same topic .
2-4:Take two topically related documents from the set and call them a and b .
2-5:If the scores of a and b are very different, we may be concerned about the performance of our system .
2-6:That is, if a and b are both on the topic of the query, we would like them both to receive a high score; if a and b are not on the topic of the query, we would like them both to receive a low score .
2-7:We might become more worried as we find more differences between scores of related documents .
2-8:We would be more comfortable with a retrieval where scores are consistent between related documents .
2-9:Our paper studies the quantification of this inconsistency in a retrieval from a spatial perspective .
2-10:Spatial analysis is appropriate since many retrieval models embed documents in some vector space .
2-11:If documents are embedded in a space, proximity correlates with topical relationships .
2-12:Score consistency can be measured by the spatial version of autocorrelation known as the Moran coefficient or IM [5, 10] .
2-13:In this paper, we demonstrate a strong correlation between IM and retrieval performance .
2-14:The discussion up to this point is reminiscent of the cluster hypothesis .
2-15:The cluster hypothesis states: closely related documents tend to be relevant to the same request [12] .
2-16:As we shall see, a retrieval function"s spatial autocorrelation measures the degree to which closely related documents receive similar scores .
2-17:Because of this, we interpret autocorrelation as measuring the degree to which a retrieval function satisfies the clustering hypothesis .
2-18:If this connection is reasonable, in Section 6, we present evidence that failure to satisfy the cluster hypothesis correlates strongly with poor performance .
2-19:In this work, we provide the following contributions, performance of retrievals with zero relevance judgments (Section 3) .
2-20:motivations behind several state of the art performance prediction techniques (Section 4) .
2-21:single run performance prediction (Sections 5 and 6). .
3 PROBLEM DEFINITION :
3-1:Given a query, an information retrieval system produces a ranking of documents in the collection encoded as a set of scores associated with documents .
3-2:We refer to the set of scores for a particular query system combination as a retrieval .
3-3:We would like to predict the performance of this retrieval with respect to some evaluation measure (eg, mean average precision) .
3-4:In this paper, we present results for ranking retrievals from arbitrary systems .
3-5:We would like this ranking to approximate the ranking of retrievals by the evaluation measure .
3-6:This is different from ranking queries by the average performance on each query .
3-7:It is also different from ranking systems by the average performance on a set of queries .
3-8:Scores are often only computed for the top n documents from the collection .
3-9:We place these scores in the length n vector, y, where yi refers to the score of the ith ranked document .
3-10:We adjust scores to have zero mean and unit variance .
3-11:We use this method because of its simplicity and its success in previous work [15]. .
4 SPATIAL CORRELATION :
4-1:In information retrieval, we often assume that the representations of documents exist in some high dimensional vector space .
4-2:For example, given a vocabulary, V, this vector space may be an arbitrary |V| dimensional space with cosine inner product or a multinomial simplex with a distributionbased distance measure .
4-3:An embedding space is often selected to respect topical proximity; if two documents are near, they are more likely to share a topic .
4-4:Because of the prevalence and success of spatial models of information retrieval, we believe that the application of spatial data analysis techniques are appropriate .
4-5:Whereas in information retrieval, we are concerned with the score at a point in a space, in spatial data analysis, we are concerned with the value of a function at a point or location in a space .
4-6:We use the term function here to mean a mapping from a location to a real value .
4-7:For example, we might be interested in the prevalence of a disease in the neighborhood of some city .
4-8:The function would map the location of a neighborhood to an infection rate .
4-9:If we want to quantify the spatial dependencies of a function, we would employ a measure referred to as the spatial autocorrelation [5, 10] .
4-10:High spatial autocorrelation suggests that knowing the value of a function at location a will tell us a great deal about the value at a neighboring location representing the temperature of a location since knowing the temperature at a location a will tell us a lot about the temperature at a neighboring location b .
4-11:Low spatial autocorrelation suggests that knowing the value of a function at location a tells us little about the value at a neighboring location b .
4-12:There is low spatial autocorrelation in a function measuring the outcome of a coin toss at a and b .
4-13:In this section, we will begin by describing what we mean by spatial proximity for documents and then define a measure of spatial autocorrelation .
4-14:We conclude by extending this model to include information from multiple retrievals from multiple systems for a single query .
4-15:3.1 Spatial Representation of Documents Our work does not focus on improving a specific similarity measure or defining a novel vector space .
4-16:Instead, we choose an inner product known to be effective at detecting interdocument topical relationships .
4-17:Specifically, we adopt tf.idf document vectors, ˜di = di log „ (n + 0.5) − ci 0.5 + ci « (1) where d is a vector of term frequencies, c is the length |V| document frequency vector .
4-18:We use this weighting scheme due to its success for topical link detection in the context of Topic Detection and Tracking evaluations [6] .
4-19:Assuming vectors are scaled by their L2 norm, we use the inner product, ˜di, ˜dj , to define similarity .
4-20:Given documents and some similarity measure, we can construct a matrix which encodes the similarity between pairs of documents .
4-21:Recall that we are given the top n documents retrieved in y .
4-22:We can compute an n × n similarity matrix, W .
4-23:An element of this matrix, Wij represents the similarity between documents ranked i and j .
4-24:In practice, we only include the affinities for a document"s k nearest neighbors .
4-25:In all of our experiments, we have fixed k to 5 .
4-26:We leave exploration of parameter sensitivity to future work .
4-27:We also row normalize the matrix so that Pn j=1 Wij = 1 for all i .
4-28:3.2 Spatial Autocorrelation of a Retrieval Recall that we are interested in measuring the similarity between the scores of spatially close documents .
4-29:One such suitable measure is the Moran coefficient of spatial autocorrelation .
4-30:Assuming the function y over n locations, this is defined as ˜IM = n eTWe P i,j Wijyiyj P i y2 i = n eTWe yT Wy yTy (2) where eT We = P ij Wij .
4-31:We would like to compare autocorrelation values for different retrievals .
4-32:Unfortunately, the bound for Equation 2 is not consistent for different W and y .
4-33:Therefore, we use the Cauchy Schwartz inequality to establish a bound, ˜IM ≤ n eTWe s yTWTWy yTy And we define the normalized spatial autocorrelation as IM = yT Wy p yTy × yTWTWy Notice that if we let ˜y = Wy, then we can write this formula as, IM = yT ˜y y 2 ˜y 2 (3) which can be interpreted as the correlation between the original retrieval scores and a set of retrieval scores diffused in the space .
4-34:We present some examples of autocorrelations of functions on a grid in Figure 1 .
4-35:3.3 Correlation with Other Retrievals Sometimes we are interested in the performance of a single retrieval but have access to scores from multiple systems for (a) IM = 0.006 (b) IM = 0.241 (c) IM = 0.487 Figure 1: The Moran coefficient, IM for a several binary functions on a grid .
4-36:The Moran coefficient is a local measure of function consistency .
4-37:From the perspective of information retrieval, each of these grid spaces would represent a document and documents would be organized so that they lay next to topically related documents .
4-38:Binary retrieval scores would define a pattern on this grid .
4-39:Notice that, as the Moran coefficient increases, neighboring cells tend to have similar values .
4-40:the same query .
4-41:In this situation, we can use combined information from these scores to construct a surrogate for a high quality ranking [17] .
4-42:We can treat the correlation between the retrieval we are interested in and the combined scores as a predictor of performance .
4-43:Assume that we are given m score functions, yi, for the same n documents .
4-44:We will represent the mean of these vectors as yµ = Pm i=1 yi .
4-45:We use the mean vector as an approximation to relevance .
4-46:Since we use zero mean and unit variance normalization, work in metasearch suggests that this assumption is justified [15] .
4-47:Because yµ represents a very good retrieval, we hypothesize that a strong similarity between yµ and y will correlate positively with system performance .
4-48:We use Pearson"s product moment correlation to measure the similarity between these vectors, ρ(y, yµ) = yT yµ y 2 yµ 2 (4) We will comment on the similarity between Equation 3 and 4 in Section 7 .
4-49:Of course, we can combine ρ(y, ˜y) and ρ(y, yµ) if we assume that they capture different factors in the prediction .
4-50:One way to accomplish this is to combine these predictors as independent variables in a linear regression .
4-51:An alternative means of combination is suggested by the mathematical form of our predictors .
4-52:Since ˜y encodes the spatial dependencies in y and yµ encodes the spatial properties of the multiple runs, we can compute a third correlation between these two vectors, ρ(˜y, yµ) = ˜yT yµ ˜y 2 yµ 2 (5) We can interpret Equation 5 as measuring the correlation between a high quality ranking (yµ) and a spatially smoothed version of the retrieval (˜y). .
5 RELATIONSHIP WITH OTHER PREDICTORS :
5-1:PREDICTORS One way to predict the effectiveness of a retrieval is to look at the shared vocabulary of the top n retrieved documents .
5-2:If we computed the most frequent content words in this set, we would hope that they would be consistent with our topic .
5-3:In fact, we might believe that a bad retrieval would include documents on many disparate topics, resulting in an overlap of terminological noise .
5-4:The Clarity of a query attempts to quantify exactly this [7] .
5-5:Specifically, Clarity measures the similarity of the words most frequently used in retrieved documents to those most frequently used in the whole corpus .
5-6:The conjecture is that a good retrieval will use language distinct from general text; the overlapping language in a bad retrieval will tend to be more similar to general text .
5-7:Mathematically, we can compute a representation of the language used in the initial retrieval as a weighted combination of document language models, P(w|θQ) = nX i=1 P(w|θi) P(Q|θi) Z (6) where θi is the language model of the ith ranked document, P(Q|θi) is the query likelihood score of the ith ranked document and Z = Pn i=1 P(Q|θi) is a normalization constant .
5-8:The similarity between the multinomial P(w|θQ) and a model of general text can be computed using the Kullback Leibler divergence, DV KL(θQ θC ) .
5-9:Here, the distribution P(w|θC ) is our model of general text which can be computed using term frequencies in the corpus .
5-10:In Figure 2a, we present Clarity as measuring the distance between the weighted center of mass of the retrieval (labeled y) and the unweighted center of mass of the collection (labeled O) .
5-11:Clarity reaches a minimum when a retrieval assigns every document the same score .
5-12:Let"s again assume we have a set of n documents retrieved for our query .
5-13:Another way to quantify the dispersion of a set of documents is to look at how clustered they are .
5-14:We may hypothesize that a good retrieval will return a single, tight cluster .
5-15:A poorly performing retrieval will return a loosely related set of documents covering many topics .
5-16:One proposed method of quantifying this dispersion is to measure the distance from a random document a to it"s nearest neighbor, b .
5-17:A retrieval which is tightly clustered will, on average, have a low distance between a and b; a retrieval which is less tightly closed will, on average have high distances between a and b .
5-18:This average corresponds to using the Cox Lewis statistic to measure the randomness of the top n documents retrieved from a system [18] .
5-19:In Figure 2a, this is roughly equivalent to measuring the area of the set n .
5-20:Notice that we are throwing away information about the retrieval function y .
5-21:Therefore the Cox Lewis statistic is highly dependent on selecting the top n documents.1 Remember that we have n documents and a set of scores .
5-22:Let"s assume that we have access to the system which provided the original scores and that we can also request scores for new documents .
5-23:This suggests a third method for predicting performance .
5-24:Take some document, a, from the retrieved set and arbitrarily add or remove words at random to create a new document ˜a .
5-25:Now, we can ask our system to score ˜a with respect to our query .
5-26:If, on average over the n documents, the scores of a and ˜a tend to be very different, we might suspect that the system is failing on this query .
5-27:So, an alternative approach is to measure the simi1 The authors have suggested coupling the query with the distance measure [18] .
5-28:The information introduced by the query, though, is retrieval independent so that, if two retrievals return the same set of documents, the approximate Cox Lewis statistic will be the same regardless of the retrieval scores .
5-29:yOy (a) Global Divergence µ(y)˜y y (b) Score Perturbation µ(y) y (c) Multirun Averaging Figure 2: Representation of several performance predictors on a grid .
5-30:In Figure 2a, we depict predictors which measure the divergence between the center of mass of a retrieval and the center of the embedding space .
5-31:In Figure 2b, we depict predictors which compare the original retrieval, y, to a perturbed version of the retrieval, ˜y .
5-32:Our approach uses a particular type of perturbation based on score diffusion .
5-33:Finally, in Figure 2c, we depict prediction when given retrievals from several other systems on the same query .
5-34:Here, we can consider the fusion of these retrieval as a surrogate for relevance .
5-35:larity between the retrieval and a perturbed version of that retrieval [18, 19] .
5-36:This can be accomplished by either perturbing the documents or queries .
5-37:The similarity between the two retrievals can be measured using some correlation measure .
5-38:This is depicted in Figure 2b .
5-39:The upper grid represents the original retrieval, y, while the lower grid represents the function after having been perturbed, ˜y .
5-40:The nature of the perturbation process requires additional scorings or retrievals .
5-41:Our predictor does not require access to the original scoring function or additional retrievals .
5-42:So, although our method is similar to other perturbation methods in spirit, it can be applied in situations when the retrieval system is inaccessible or costly to access .
5-43:Finally, assume that we have, in addition to the retrieval we want to evaluate, m retrievals from a variety of different systems .
5-44:In this case, we might take a document a, compare its rank in the retrieval to its average rank in the m retrievals .
5-45:If we believe that the m retrievals provide a satisfactory approximation to relevance, then a very large difference in rank would suggest that our retrieval is misranking a .
5-46:If this difference is large on average over all n documents, then we might predict that the retrieval is bad .
5-47:If, on the other hand, the retrieval is very consistent with the m retrievals, then we might predict that the retrieval is good .
5-48:The similarity between the retrieval and the combined retrieval may be computed using some correlation measure .
5-49:This is depicted in Figure 2c .
5-50:In previous work, the Kullback Leibler divergence between the normalized scores of the retrieval and the normalized scores of the combined retrieval provides the similarity [1]. .
6 EXPERIMENTS :
6-1:Our experiments focus on testing the predictive power of each of our predictors: ρ(y, ˜y), ρ(y, yµ), and ρ(˜y, yµ) .
6-2:As stated in Section 2, we are interested in predicting the performance of the retrieval generated by an arbitrary system .
6-3:Our methodology is consistent with previous research in that we predict the relative performance of a retrieval by comparing a ranking based on our predictor to a ranking based on average precision .
6-4:We present results for two sets of experiments .
6-5:The first set of experiments presents detailed comparisons of our predictors to previously proposed predictors using identical data sets .
6-6:Our second set of experiments demonstrates the generalizability of our approach to arbitrary retrieval methods, corpus types, and corpus languages .
6-7:5.1 Detailed Experiments In these experiments, we will predict the performance of language modeling scores using our autocorrelation predictor, ρ(y, ˜y); we do not consider ρ(y, yµ) or ρ(˜y, yµ) because, in these detailed experiments, we focus on ranking the retrievals from a single system .
6-8:We use retrievals, values for baseline predictors, and evaluation measures reported in previous work [19] .
6-9:5.1.1 Topics and Collections These performance prediction experiments use language model retrievals performed for queries associated with collections in the TREC corpora .
6-10:Using TREC collections allows us to confidently associate an average precision with a retrieval .
6-11:In these experiments, we use the following topic collections: TREC 4 ad hoc, TREC 5 ad hoc, Robust 2004, Terabyte 2004, and Terabyte 2005 .
6-12:5.1.2 Baselines We provide two baselines .
6-13:Our first baseline is the classic Clarity predictor presented in Equation 6 .
6-14:Clarity is designed to be used with language modeling systems .
6-15:Our second baseline is Zhou and Croft"s ranking robustness predictor .
6-16:This predictor corrupts the top k documents from retrieval and re computes the language model scores for these corrupted documents .
6-17:The value of the predictor is the Spearman rank correlation between the original ranking and the corrupted ranking .
6-18:In our tables, we will label results for Clarity using DV KL and the ranking robustness predictor using P .
6-19:5.2 Generalizability Experiments Our predictors do not require a particular baseline retrieval system; the predictors can be computed for an arbitrary retrieval, regardless of how scores were generated .
6-20:We believe that that is one of the most attractive aspects of our algorithm .
6-21:Therefore, in a second set of experiments, we demonstrate the ability of our techniques to generalize to a variety of collections, topics, and retrieval systems .
6-22:5.2.1 Topics and Collections We gathered a diverse set of collections from all possible TREC corpora .
6-23:We cast a wide net in order to locate collections where our predictors might fail .
6-24:Our hypothesis is that documents with high topical similarity should have correlated scores .
6-25:Therefore, we avoided collections where scores were unlikely to be correlated (eg, question answering) or were likely to be negatively correlated (eg, novelty) .
6-26:Nevertheless, our collections include corpora where correlations are weakly justified (eg, non English corpora) or not justified at all (eg, expert search) .
6-27:We use the ad hoc tracks from TREC3 8, TREC Robust 2003 2005, TREC Terabyte 20042005, TREC4 5 Spanish, TREC5 6 Chinese, and TREC Enterprise Expert Search 2005 .
6-28:In all cases, we use only the automatic runs for ad hoc tracks submitted to NIST .
6-29:For all English and Spanish corpora, we construct the matrix W according to the process described in Section 3.1 .
6-30:For Chinese corpora, we use na¨ıve character based tf.idf vectors .
6-31:For entities, entries in W are proportional to the number of documents in which two entities cooccur .
6-32:5.2.2 Baselines In our detailed experiments, we used the Clarity measure as a baseline .
6-33:Since we are predicting the performance of retrievals which are not based on language modeling, we use a version of Clarity referred to as ranked list Clarity [7] .
6-34:Ranked list clarity converts document ranks to P(Q|θi) values .
6-35:This conversion begins by replacing all of the scores in y with the respective ranks .
6-36:Our estimation of P(Q|θi) from the ranks, then is, P(Q|θi) = ( 2(c+1−yi) c(c+1) if yi ≤ c 0 otherwise (7) where c is a cutoff parameter .
6-37:As suggested by the authors, we fix the algorithm parameters c and λ2 so that c = 60 and λ2 = 0.10 .
6-38:We use Equation 6 to estimate P(w|θQ) and DV KL(θQ θC ) to compute the value of the predictor .
6-39:We will refer to this predictor as DV KL, superscripted by V to indicate that the Kullback Leibler divergence is with respect to the term embedding space .
6-40:When information from multiple runs on the same query is available, we use Aslam and Pavlu"s document space multinomial divergence as a baseline [1] .
6-41:This rank based method first normalizes the scores in a retrieval as an n dimensional multinomial .
6-42:As with ranked list Clarity, we begin by replacing all of the scores in y with their respective ranks .
6-43:Then, we adjust the elements of y in the following way, ˆyi = 1 2n 0 @1 + nX k=yi 1 k 1 A (8) In our multirun experiments, we only use the top 75 documents from each retrieval (n = 75); this is within the range of parameter values suggested by the authors .
6-44:However, we admit not tuning this parameter for either our system or the baseline .
6-45:The predictor is the divergence between the candidate distribution, y, and the mean distribution, yµ .
6-46:With the uniform linear combination of these m retrievals represented as yµ, we can compute the divergence as Dn KL(ˆy ˆyµ) where we use the superscript n to indicate that the summation is over the set of n documents .
6-47:This baseline was developed in the context of predicting query difficulty but we adopt it as a reasonable baseline for predicting retrieval performance .
6-48:5.2.3 Parameter Settings When given multiple retrievals, we use documents in the union of the top k = 75 documents from each of the m retrievals for that query .
6-49:If the size of this union is ˜n, then yµ and each yi is of length ˜n .
6-50:In some cases, a system did not score a document in the union .
6-51:Since we are making a Gaussian assumption about our scores, we can sample scores for these unseen documents from the negative tail of the distribution .
6-52:Specifically, we sample from the part of the distribution lower than the minimum value of in the normalized retrieval .
6-53:This introduces randomness into our algorithm but we believe it is more appropriate than assigning an arbitrary fixed value .
6-54:We optimized the linear regression using the square root of each predictor .
6-55:We found that this substantially improved fits for all predictors, including the baselines .
6-56:We considered linear combinations of pairs of predictors (labeled by the components) and all predictors (labeled as β) .
6-57:5.3 Evaluation Given a set of retrievals, potentially from a combination of queries and systems, we measure the correlation of the rank ordering of this set by the predictor and by the performance metric .
6-58:In order to ensure comparability with previous results, we present Kendall"s τ correlation between the predictor"s ranking and ranking based on average precision of the retrieval .
6-59:Unless explicitly noted, all correlations are significant with p < 0.05 .
6-60:Predictors can sometimes perform better when linearly combined [9, 11] .
6-61:Although previous work has presented the coefficient of determination (R2 ) to measure the quality of the regression, this measure cannot be reliably used when comparing slight improvements from combining predictors .
6-62:Therefore, we adopt the adjusted coefficient of determination which penalizes models with more variables .
6-63:The adjusted R2 allows us to evaluate the improvement in prediction achieved by adding a parameter but loses the statistical interpretation of R2 .
6-64:We will use Kendall"s τ to evaluate the magnitude of the correlation and the adjusted R2 to evaluate the combination of variables. .
7 RESULTS :
7-1:We present results for our detailed experiments comparing the prediction of language model scores in Table 1 .
7-2:Although the Clarity measure is theoretically designed for language model scores, it consistently underperforms our system agnostic predictor .
7-3:Ranking robustness was presented as an improvement to Clarity for web collections (represented in our experiments by the terabyte04 and terabyte05 collections), shifting the τ correlation from 0.139 to 0.150 for terabyte04 and 0.171 to 0.208 for terabyte05 .
7-4:However, these improvements are slight compared to the performance of autocorrelation on these collections .
7-5:Our predictor achieves a τ correlation of 0.454 for terabyte04 and 0.383 for terabyte05 .
7-6:Though not always the strongest, autocorrelation achieves correlations competitive with baseline predictors .
7-7:When examining the performance of linear combinations of predictors, we note that in every case, autocorrelation factors as a necessary component of a strong predictor .
7-8:We also note that the adjusted R2 for individual baselines are always significantly improved by incorporating autocorrelation .
7-9:We present our generalizability results in Table 2 .
7-10:We begin by examining the situation in column (a) where we are presented with a single retrieval and no information from additional retrievals .
7-11:For every collection except one, we achieve significantly better correlations than ranked list Clarity .
7-12:Surprisingly, we achieve relatively strong correlations for Spanish and Chinese collections despite our na¨ıve processing .
7-13:We do not have a ranked list clarity correlation for ent05 because entity modeling is itself an open research question .
7-14:However, our autocorrelation measure does not achieve high correlations perhaps because relevance for entity retrieval does not propagate according to the cooccurrence links we use .
7-15:As noted above, the poor Clarity performance on web data is consistent with our findings in the detailed experiments .
7-16:Clarity also notably underperforms for several news corpora (trec5, trec7, and robust04) .
7-17:On the other hand, autocorrelation seems robust to the changes between different corpora .
7-18:Next, we turn to the introduction of information from multiple retrievals .
7-19:We compare the correlations between those predictors which do not use this information in column (a) and those which do in column (b) .
7-20:For every collection, the predictors in column (b) outperform the predictors in column (a), indicating that the information from additional runs can be critical to making good predictions .
7-21:Inspecting the predictors in column (b), we only draw weak conclusions .
7-22:Our new predictors tend to perform better on news corpora .
7-23:And between our new predictors, the hybrid ρ(˜y, yµ) predictor tends to perform better .
7-24:Recall that our ρ(˜y, yµ) measure incorporates both spatial and multiple retrieval information .
7-25:Therefore, we believe that the improvement in correlation is the result of incorporating information from spatial behavior .
7-26:In column (c), we can investigate the utility of incorporating spatial information with information from multiple retrievals .
7-27:Notice that in the cases where autocorrelation, ρ(y, ˜y), alone performs well (trec3, trec5 spanish, and trec6 chinese), it is substantially improved by incorporating multiple retrieval information from ρ(y, yµ) in the linear regression, β .
7-28:In the cases where ρ(y, yµ) performs well, incorporating autocorrelation rarely results in a significant improvement in performance .
7-29:In fact, in every case where our predictor outperforms the baseline, it includes information from multiple runs. .
8 DISCUSSION :
8-1:The most important result from our experiments involves prediction when no information is available from multiple runs (Tables 1 and 2a) .
8-2:This situation arises often in system design .
8-3:For example, a system may need to, at retrieval time, assess its performance before deciding to conduct more intensive processing such as pseudo relevance feedback or interaction .
8-4:Assuming the presence of multiple retrievals is unrealistic in this case .
8-5:We believe that autocorrelation is, like multiple retrieval algorithms, approximating a good ranking; in this case by diffusing scores .
8-6:Why is ˜y a reasonable surrogate? We know that diffusion of scores on the web graph and language model graphs improves performance [14, 16] .
8-7:Therefore, if score diffusion tends to, in general, improve performance, then diffused scores will, in general, provide a good surrogate for relevance .
8-8:Our results demonstrate that this approximation is not as powerful as information from multiple retrievals .
8-9:Nevertheless, in situations where this information is lacking, autocorrelation provides substantial information .
8-10:The success of autocorrelation as a predictor may also have roots in the clustering hypothesis .
8-11:Recall that we regard autocorrelation as the degree to which a retrieval satisfies the clustering hypothesis .
8-12:Our experiments, then, demonstrate that a failure to respect the clustering hypothesis correlates with poor performance .
8-13:Why might systems fail to conform to the cluster hypothesis? Query based information retrieval systems often score documents independently .
8-14:The score of document a may be computed by examining query term or phrase matches, the document length, and perhaps global collection statistics .
8-15:Once computed, a system rarely compares the score of a to the score of a topically related document b .
8-16:With some exceptions, the correlation of document scores has largely been ignored .
8-17:We should make it clear that we have selected tasks where topical autocorrelation is appropriate .
8-18:There are certainly cases where there is no reason to believe that retrieval scores will have topical autocorrelation .
8-19:For example, ranked lists which incorporate document novelty should not exhibit spatial autocorrelation; if anything autocorrelation should be negative for this task .
8-20:Similarly, answer candidates in a question answering task may or may not exhibit autocorrelation; in this case, the semantics of links is questionable too .
8-21:It is important before applying this measure to confirm that, given the semantics for some link between two retrieved items, we should expect a correlation between scores. .
9 RELATED WORK :
9-1:In this section we draw more general comparisons to other work in performance prediction and spatial data analysis .
9-2:There is a growing body of work which attempts to predict the performance of individual retrievals [7, 3, 11, 9, 19] .
9-3:We have attempted to place our work in the context of much of this work in Section 4 .
9-4:However, a complete comparison is beyond the scope of this paper .
9-5:We note, though, that our experiments cover a larger and more diverse set of retrievals, collections, and topics than previously examined .
9-6:Much previous work particularly in the context of TRECfocuses on predicting the performance of systems .
9-7:Here, each system generates k retrievals .
9-8:The task is, given these retrievals, to predict the ranking of systems according to some performance measure .
9-9:Several papers attempt to address this task under the constraint of few judgments [2, 4] .
9-10:Some work even attempts to use zero judgments by leveraging multiple retrievals for the same query [17] .
9-11:Our task differs because we focus on ranking retrievals independent of the generating system .
9-12:The task here is not to test the hypothesis system A is superior to system B but to test the hypothesis retrieval A is superior to retrieval B .
9-13:Autocorrelation manifests itself in many classification tasks .
9-14:Neville and Jensen define relational autocorrelation for relational learning problems and demonstrate that many classification tasks manifest autocorrelation [13] .
9-15:Temporal autocorrelation of initial retrievals has also been used to predict performance [9] .
9-16:However, temporal autocorrelation is performed by projecting the retrieval function into the temporal embedding space .
9-17:In our work, we focus on the behavior of the function over the relationships between documents .
9-18:τ adjusted R2 DV KL P ρ(y, ˜y) DV KL P ρ(y, ˜y) DV KL, P DV KL, ρ(y, ˜y) Pρ(y, ˜y) β trec4 0.353 0.548 0.513 0.168 0.363 0.422 0.466 0.420 0.557 0.553 trec5 0.311 0.329 0.357 0.116 0.190 0.236 0.238 0.244 0.266 0.269 robust04 0.418 0.398 0.373 0.256 0.304 0.278 0.403 0.373 0.402 0.442 terabyte04 0.139 0.150 0.454 0.059 0.045 0.292 0.076 0.293 0.289 0.284 terabyte05 0.171 0.208 0.383 0.022 0.072 0.193 0.120 0.225 0.218 0.257 Table 1: Comparison to Robustness and Clarity measures for language model scores .
9-19:Evaluation replicates experiments from [19] .
9-20:We present correlations between the classic Clarity measure (DV KL), the ranking robustness measure (P), and autocorrelation (ρ(y, ˜y)) each with mean average precision in terms of Kendall"s Measures in bold represent the strongest correlation for that test collection pair .
9-21:multiple run (a) (b) (c) τ τ adjusted R2 DKL ρ(y, ˜y) Dn KL ρ(y, yµ) ρ(˜y, yµ) Dn KL ρ(y, ˜y) ρ(y, yµ) ρ(˜y, yµ) β trec3 0.201 0.461 0.461 0.439 0.456 0.444 0.395 0.394 0.386 0.498 trec4 0.252 0.396 0.455 0.482 0.489 0.379 0.263 0.429 0.482 0.483 trec5 0.016 0.277 0.433 0.459 0.393 0.280 0.157 0.375 0.323 0.386 trec6 0.230 0.227 0.352 0.428 0.418 0.203 0.089 0.323 0.325 0.325 trec7 0.083 0.326 0.341 0.430 0.483 0.264 0.182 0.363 0.442 0.400 trec8 0.235 0.396 0.454 0.508 0.567 0.402 0.272 0.490 0.580 0.523 robust03 0.302 0.354 0.377 0.385 0.447 0.269 0.206 0.274 0.392 0.303 robust04 0.183 0.308 0.301 0.384 0.453 0.200 0.182 0.301 0.393 0.335 robust05 0.224 0.249 0.371 0.377 0.404 0.341 0.108 0.313 0.328 0.336 terabyte04 0.043 0.245 0.544 0.420 0.392 0.516 0.105 0.357 0.343 0.365 terabyte05 0.068 0.306 0.480 0.434 0.390 0.491 0.168 0.384 0.309 0.403 trec4 spanish 0.307 0.388 0.488 0.398 0.395 0.423 0.299 0.282 0.299 0.388 trec5 spanish 0.220 0.458 0.446 0.484 0.475 0.411 0.398 0.428 0.437 0.529 trec5 chinese 0.092 0.199 0.367 0.379 0.384 0.379 0.199 0.273 0.276 0.310 trec6 chinese 0.144 0.276 0.265 0.353 0.376 0.115 0.128 0.188 0.223 0.199 ent05 0.181 0.324 0.305 0.282 0.211 0.043 0.158 0.155 0.179 Table 2: Large scale prediction experiments .
9-22:We predict the ranking of large sets of retrievals for various collections and retrieval systems .
9-23:Kendall"s τ correlations are computed between the predicted ranking and a ranking based on the retrieval"s average precision .
9-24:In column (a), we have predictors which do not use information from other retrievals for the same query .
9-25:In columns (b) and (c) we present performance for predictors which incorporate information from multiple retrievals .
9-26:The adjusted coefficient of determination is computed to determine effectiveness of combining predictors .
9-27:Measures in bold represent the strongest correlation for that test collection pair .
9-28:Finally, regularization based re ranking processes are also closely related to our work [8] .
9-29:These techniques seek to maximize the agreement between scores of related documents by solving a constrained optimization problem .
9-30:The maximization of consistency is equivalent to maximizing the Moran autocorrelation .
9-31:Therefore, we believe that our work provides explanation for why regularization based re ranking works. .
10-1:We have presented a new method for predicting the performance of a retrieval ranking without any relevance judgments
10-2:We consider two cases
10-3:First, when making predictions in the absence of retrievals from other systems, our predictors demonstrate robust, strong correlations with average precision
10-4:This performance, combined with a simple implementation, makes our predictors, in particular, very attractive
10-5:We have demonstrated this improvement for many, diverse settings
10-6:To our knowledge, this is the first large scale examination of zero judgment, single retrieval performance prediction
10-7:Second, when provided retrievals from other systems, our extended methods demonstrate competitive performance with state of the art baselines
10-8:Our experiments also demonstrate the limits of the usefulness of our predictors when information from multiple runs is provided
10-9:Our results suggest two conclusions
10-10:First, our results could affect retrieval algorithm design
10-11:Retrieval algorithms designed to consider spatial autocorrelation will conform to the cluster hypothesis and improve performance
10-12:Second, our results could affect the design of minimal test collection algorithms
10-13:Much of the recent work in ranking systems sometimes ignores correlations between document labels and scores
10-14:We believe that these two directions could be rewarding given the theoretical and experimental evidence in this paper
10-15:10
10-16:ACKNOWLEDGMENTS This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011 06 C 0023
10-17:Any opinions, findings and conclusions or recommendations expressed in this material are the author"s and do not necessarily reflect those of the sponsor
10-18:We thank Yun Zhou and Desislava Petkova for providing data and Andre Gauthier for technical assistance
10-19:11
10-20:REFERENCES
11-1:J
11-2:Aslam and V
11-3:Pavlu
11-4:Query hardness estimation using jensen shannon divergence among multiple scoring functions
11-5:In ECIR 2007: Proceedings of the 29th European Conference on Information Retrieval, 2007
11-6:J
11-7:A
11-8:Aslam, V
11-9:Pavlu, and E
11-10:Yilmaz
11-11:A statistical method for system evaluation using incomplete judgments
11-12:In S
11-13:Dumais, E
11-14:N
11-15:Efthimiadis, D
11-16:Hawking, and K
11-17:Jarvelin, editors, Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 541 548
11-18:ACM Press, August 2006
11-19:D
11-20:Carmel, E
11-21:Yom Tov, A
11-22:Darlow, and D
11-23:Pelleg
11-24:What makes a query difficult? In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 390 397, New York, NY, USA, 2006
11-25:ACM Press
11-26:B
11-27:Carterette, J
11-28:Allan, and R
11-29:Sitaraman
11-30:Minimal test collections for retrieval evaluation
11-31:In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 268 275, New York, NY, USA, 2006
11-32:ACM Press
11-33:A
11-34:D
11-35:Cliff and J
11-36:K
11-37:Ord
11-38:Spatial Autocorrelation
11-39:Pion Ltd., 1973
11-40:M
11-41:Connell, A
11-42:Feng, G
11-43:Kumaran, H
11-44:Raghavan, C
11-45:Shah, and J
11-46:Allan
11-47:Umass at tdt 2004
11-48:Technical Report CIIR Technical Report IR  357, Department of Computer Science, University of Massachusetts, 2004
11-49:S
11-50:Cronen Townsend, Y
11-51:Zhou, and W
11-52:B
11-53:Croft
11-54:Precision prediction based on ranked list coherence
11-55:Inf
11-56:Retr., 9(6):723 755, 2006
11-57:F
11-58:Diaz
11-59:Regularizing ad hoc retrieval scores
11-60:In CIKM "05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 672 679, New York, NY, USA, 2005
11-61:ACM Press
11-62:F
11-63:Diaz and R
11-64:Jones
11-65:Using temporal profiles of queries for precision prediction
11-66:In SIGIR "04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 18 24, New York, NY, USA, 2004
11-67:ACM Press
11-68:D
11-69:A
11-70:Griffith
11-71:Spatial Autocorrelation and Spatial Filtering
11-72:Springer Verlag, 2003
11-73:B
11-74:He and I
11-75:Ounis
11-76:Inferring Query Performance Using Pre retrieval Predictors
11-77:In The Eleventh Symposium on String Processing and Information Retrieval (SPIRE), 2004
11-78:N
11-79:Jardine and C
11-80:J
11-81:V
11-82:Rijsbergen
11-83:The use of hierarchic clustering in information retrieval
11-84:Information Storage and Retrieval, 7:217 240, 1971
11-85:D
11-86:Jensen and J
11-87:Neville
11-88:Linkage and autocorrelation cause feature selection bias in relational learning
11-89:In ICML "02: Proceedings of the Nineteenth International Conference on Machine Learning, pages 259 266, San Francisco, CA, USA, 2002
11-90:Morgan Kaufmann Publishers Inc
11-91:O
11-92:Kurland and L
11-93:Lee
11-94:Corpus structure, language models, and ad hoc information retrieval
11-95:In SIGIR "04: Proceedings of the 27th annual international conference on Research and development in information retrieval, pages 194 201, New York, NY, USA, 2004
11-96:ACM Press
11-97:M
11-98:Montague and J
11-99:A
11-100:Aslam
11-101:Relevance score normalization for metasearch
11-102:In CIKM "01: Proceedings of the tenth international conference on Information and knowledge management, pages 427 433, New York, NY, USA, 2001
11-103:ACM Press
11-104:T
11-105:Qin, T. Y
11-106:Liu, X. D
11-107:Zhang, Z
11-108:Chen, and W. Y
11-109:Ma
11-110:A study of relevance propagation for web search
11-111:In SIGIR "05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 408 415, New York, NY, USA, 2005
11-112:ACM Press
11-113:I
11-114:Soboroff, C
11-115:Nicholas, and P
11-116:Cahan
11-117:Ranking retrieval systems without relevance judgments
11-118:In SIGIR "01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 66 73, New York, NY, USA, 2001
11-119:ACM Press
11-120:V
11-121:Vinay, I
11-122:J
11-123:Cox, N
11-124:Milic Frayling, and K
11-125:Wood
11-126:On ranking the effectiveness of searches
11-127:In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 398 404, New York, NY, USA, 2006
11-128:ACM Press
11-129:Y
11-130:Zhou and W
11-131:B
11-132:Croft
11-133:Ranking robustness: a novel framework to predict query performance
11-134:In CIKM "06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 567 574, New York, NY, USA, 2006
11-135:ACM Press
picture:
