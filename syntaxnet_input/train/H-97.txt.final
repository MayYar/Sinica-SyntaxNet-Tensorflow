Feature Representation for Effective Action-Item Detection 
content:
1 ABSTRACT :
1-1:E mail users face an ever growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination .
1-2:Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action .
1-3:Unlike standard topic driven text classification, action item detection requires inferring the sender"s intent, and as such responds less well to pure bag of words classification .
1-4:However, using enriched feature sets, such as n grams (up to n=4) with chi squared feature selection, and contextual cues for action item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross validation .
1-5:H.3.3 [Information Storage and Retrieval]: Information Search .
2 INTRODUCTION :
2-1:E mail users are facing an increasingly difficult task of managing their inboxes in the face of mounting challenges that result from rising e mail usage .
2-2:This includes prioritizing e mails over a range of sources from business partners to family members, filtering and reducing junk e mail, and quickly managing requests that demand From: Henry Hutchins <hhutchins@innovative.company.com> To: Sara Smith; Joe Johnson; William Woolings Subject: meeting with prospective customers Sent: Fri 12 10 2005 8:08 AM Hi All, I"d like to remind all of you that the group from GRTY will be visiting us next Friday at 4:30 p.m .
2-3:The current schedule looks like this: + 9:30 a.m .
2-4:Informal Breakfast and Discussion in Cafeteria + 10:30 a.m .
2-5:Company Overview + 11:00 a.m .
2-6:Individual Meetings (Continue Over Lunch) + 2:00 p.m .
2-7:Tour of Facilities + 3:00 p.m .
2-8:Sales Pitch In order to have this go off smoothly, I would like to practice the presentation well in advance .
2-9:As a result, I will need each of your parts by Wednesday .
2-10:Keep up the good work! Henry Figure 1: An E mail with emphasized Action Item, an explicit request that requires the recipient"s attention or action .
2-11:the receiver"s attention or action .
2-12:Automated action item detection targets the third of these problems by attempting to detect which e mails require an action or response with information, and within those e mails, attempting to highlight the sentence (or other passage length) that directly indicates the action request .
2-13:Such a detection system can be used as one part of an e mail agent which would assist a user in processing important e mails quicker than would have been possible without the agent .
2-14:We view action item detection as one necessary component of a successful e mail agent which would perform spam detection, action item detection, topic classification and priority ranking, among other functions .
2-15:The utility of such a detector can manifest as a method of prioritizing e mails according to task oriented criteria other than the standard ones of topic and sender or as a means of ensuring that the email user hasn"t dropped the proverbial ball by forgetting to address an action request .
2-16:Action item detection differs from standard text classification in two important ways .
2-17:First, the user is interested both in detecting whether an email contains action items and in locating exactly where these action item requests are contained within the email body .
2-18:In contrast, standard text categorization merely assigns a topic label to each text, whether that label corresponds to an e mail folder or a controlled indexing vocabulary [12, 15, 22] .
2-19:Second, action item detection attempts to recover the email sender"s intent whether she means to elicit response or action on the part of the receiver; note that for this task, classifiers using only unigrams as features do not perform optimally, as evidenced in our results below .
2-20:Instead we find that we need more information laden features such as higher order n grams .
2-21:Text categorization by topic, on the other hand, works very well using just individual words as features [2, 9, 13, 17] .
2-22:In fact, genre classification, which one would think may require more than a bag of words approach, also works quite well using just unigram features [14] .
2-23:Topic detection and tracking (TDT), also works well with unigram feature sets [1, 20] .
2-24:We believe that action item detection is one of the first clear instances of an IR related task where we must move beyond bag of words to achieve high performance, albeit not too far, as bag of n grams seem to suffice .
2-25:We first review related work for similar text classification problems such as e mail priority ranking and speech act identification .
2-26:Then we more formally define the action item detection problem, discuss the aspects that distinguish it from more common problems like topic classification, and highlight the challenges in constructing systems that can perform well at the sentence and document level .
2-27:From there, we move to a discussion of feature representation and selection techniques appropriate for this problem and how standard text classification approaches can be adapted to smoothly move from the sentence level detection problem to the documentlevel classification problem .
2-28:We then conduct an empirical analysis that helps us determine the effectiveness of our feature extraction procedures as well as establish baselines for a number of classification algorithms on this task .
2-29:Finally, we summarize this paper"s contributions and consider interesting directions for future work. .
3 RELATED WORK :
3-1:Several other researchers have considered very similar text classification tasks .
3-2:Cohen et al .
3-3:[5] describe an ontology of speech acts, such as Propose a Meeting, and attempt to predict when an e mail contains one of these speech acts .
3-4:We consider action items to be an important specific type of speech act that falls within their more general classification .
3-5:While they provide results for several classification methods, their methods only make use of human judgments at the document level .
3-6:In contrast, we consider whether accuracy can be increased by using finer grained human judgments that mark the specific sentences and phrases of interest .
3-7:Corston Oliver et al .
3-8:[6] consider detecting items in e mail to Put on a To Do List .
3-9:This classification task is very similar to ours except they do not consider simple factual questions to belong to this category .
3-10:We include questions, but note that not all questions are action items some are rhetorical or simply social convention, How are you? .
3-11:From a learning perspective, while they make use of judgments at the sentence level, they do not explicitly compare what if any benefits finer grained judgments offer .
3-12:Additionally, they do not study alternative choices or approaches to the classification task .
3-13:Instead, they simply apply a standard SVM at the sentence level and focus primarily on a linguistic analysis of how the sentence can be logically reformulated before adding it to the task list .
3-14:In this study, we examine several alternative classification methods, compare document level and sentence level approaches and analyze the machine learning issues implicit in these problems .
3-15:Interest in a variety of learning tasks related to e mail has been rapidly growing in the recent literature .
3-16:For example, in a forum dedicated to e mail learning tasks, Culotta et al .
3-17:[7] presented methods for learning social networks from e mail .
3-18:In this work, we do not focus on peer relationships; however, such methods could complement those here since peer relationships often influence word choice when requesting an action. .
4 PROBLEM DEFINITION & APPROACH :
4-1:In contrast to previous work, we explicitly focus on the benefits that finer grained, more costly, sentence level human judgments offer over coarse grained document level judgments .
4-2:Additionally, we consider multiple standard text classification approaches and analyze both the quantitative and qualitative differences that arise from taking a document level vs .
4-3:a sentence level approach to classification .
4-4:Finally, we focus on the representation necessary to achieve the most competitive performance .
4-5:3.1 Problem Definition In order to provide the most benefit to the user, a system would not only detect the document, but it would also indicate the specific sentences in the e mail which contain the action items .
4-6:Therefore, there are three basic problems: not it contains an action item .
4-7:documents containing action items occur as high as possible in the ranking .
4-8:to whether or not it is an action item .
4-9:As in most Information Retrieval tasks, the weight the evaluation metric should give to precision and recall depends on the nature of the application .
4-10:In situations where a user will eventually read all received messages, ranking (e.g., via precision at recall of 1) may be most important since this will help encourage shorter delays in communications between users .
4-11:In contrast, high precision detection at low recall will be of increasing importance when the user is under severe time pressure and therefore will likely not read all mail .
4-12:This can be the case for crisis managers during disaster management .
4-13:Finally, sentence detection plays a role in both timepressure situations and simply to alleviate the user"s required time to gist the message .
4-14:3.2 Approach As mentioned above, the labeled data can come in one of two forms: a document labeling provides a yes no label for each document as to whether it contains an action item; a phrase labeling provides only a yes label for the specific items of interest .
4-15:We term the human judgments a phrase labeling since the user"s view of the action item may not correspond with actual sentence boundaries or predicted sentence boundaries .
4-16:Obviously, it is straightforward to generate a document labeling consistent with a phrase labeling by labeling a document yes if and only if it contains at least one phrase labeled yes .
4-17:To train classifiers for this task, we can take several viewpoints related to both the basic problems we have enumerated and the form of the labeled data .
4-18:The document level view treats each e mail as a learning instance with an associated class label .
4-19:Then, the document can be converted to a feature value vector and learning progresses as usual .
4-20:Applying a document level classifier to document detection and ranking is straightforward .
4-21:In order to apply it to sentence detection, one must make additional steps .
4-22:For example, if the classifier predicts a document contains an action item, then areas of the document that contain a high concentration of words which the model weights heavily in favor of action items can be indicated .
4-23:The obvious benefit of the document level approach is that training set collection costs are lower since the user only has to specify whether or not an e mail contains an action item and not the specific sentences .
4-24:In the sentence level view, each e mail is automatically segmented into sentences, and each sentence is treated as a learning instance with an associated class label .
4-25:Since the phrase labeling provided by the user may not coincide with the automatic segmentation, we must determine what label to assign a partially overlapping sentence when converting it to a learning instance .
4-26:Once trained, applying the resulting classifiers to sentence detection is now straightforward, but in order to apply the classifiers to document detection and document ranking, the individual predictions over each sentence must be aggregated in order to make a document level prediction .
4-27:This approach has the potential to benefit from morespecific labels that enable the learner to focus attention on the key sentences instead of having to learn based on data that the majority of the words in the e mail provide no or little information about class membership .
4-28:3.2.1 Features Consider some of the phrases that might constitute part of an action item: would like to know, let me know, as soon as possible, have you .
4-29:Each of these phrases consists of common words that occur in many e mails .
4-30:However, when they occur in the same sentence, they are far more indicative of an action item .
4-31:Additionally, order can be important: consider have you versus you have .
4-32:Because of this, we posit that n grams play a larger role in this problem than is typical of problems like topic classification .
4-33:Therefore, we consider all n grams up to size 4 .
4-34:When using n grams, if we find an n gram of size 4 in a segment of text, we can represent the text as just one occurrence of the ngram or as one occurrence of the n gram and an occurrence of each smaller n gram contained by it .
4-35:We choose the second of these alternatives since this will allow the algorithm itself to smoothly back off in terms of recall .
4-36:Methods such as na¨ıve Bayes may be hurt by such a representation because of double counting .
4-37:Since sentence ending punctuation can provide information, we retain the terminating punctuation token when it is identifiable .
4-38:Additionally, we add a beginning of sentence and end of sentence token in order to capture patterns that are often indicators at the beginning or end of a sentence .
4-39:Assuming proper punctuation, these extra tokens are unnecessary, but often e mail lacks proper punctuation .
4-40:In addition, for the sentence level classifiers that use ngrams, we additionally code for each sentence a binary encoding of the position of the sentence relative to the document .
4-41:This encoding has eight associated features that represent which octile (the first eighth, second eighth, etc.) contains the sentence .
4-42:3.2.2 Implementation Details In order to compare the document level to the sentence level approach, we compare predictions at the document level .
4-43:We do not address how to use a document level classifier to make predictions at the sentence level .
4-44:In order to automatically segment the text of the e mail, we use the RASP statistical parser [4] .
4-45:Since the automatically segmented sentences may not correspond directly with the phrase level boundaries, we treat any sentence that contains at least 30% of a marked action item segment as an action item .
4-46:When evaluating sentencedetection for the sentence level system, we use these class labels as ground truth .
4-47:Since we are not evaluating multiple segmentation approaches, this does not bias any of the methods .
4-48:If multiple segmentation systems were under evaluation, one would need to use a metric that matched predicted positive sentences to phrases labeled positive .
4-49:The metric would need to punish overly long true predictions as well as too short predictions .
4-50:Our criteria for converting to labeled instances implicitly includes both criteria .
4-51:Since the segmentation is fixed, an overly long prediction would be predicting yes for many no instances since presumably the extra length corresponds to additional segmented sentences all of which do not contain 30% of action item .
4-52:Likewise, a too short prediction must correspond to a small sentence included in the action item but not constituting all of the action item .
4-53:Therefore, in order to consider the prediction to be too short, there will be an additional preceding following sentence that is an action item where we incorrectly predicted no .
4-54:Once a sentence level classifier has made a prediction for each sentence, we must combine these predictions to make both a document level prediction and a document level score .
4-55:We use the simple policy of predicting positive when any of the sentences is predicted positive .
4-56:In order to produce a document score for ranking, the confidence that the document contains an action item is: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w .
4-57:where s is a sentence in document d, π is the classifier"s 1 0 prediction, ψ is the score the classifier assigns as its confidence that π(s) = 1, and n(d) is the greater of 1 and the number of (unigram) tokens in the document .
4-58:In other words, when any sentence is predicted positive, the document score is the length normalized sum of the sentence scores above threshold .
4-59:When no sentence is predicted positive, the document score is the maximum sentence score normalized by length .
4-60:As in other text problems, we are more likely to emit false positives for documents with more words or sentences .
4-61:Thus we include a length normalization factor. .
5 EXPERIMENTAL ANALYSIS :
5-1:4.1 The Data Our corpus consists of e mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job candidate interviews, publishing proceedings, and talk announcements .
5-2:The messages were anonymized by replacing the names of each individual and institution with a pseudonym.1 After attempting to identify and eliminate duplicate e mails, the corpus contains 744 e mail messages .
5-3:After identity anonymization, the corpora has three basic versions .
5-4:Quoted material refers to the text of a previous e mail that an author often leaves in an e mail message when responding to the e mail .
5-5:Quoted material can act as noise when learning since it may include action items from previous messages that are no longer relevant .
5-6:To isolate the effects of quoted material, we have three versions of the corpora .
5-7:The raw form contains the basic messages .
5-8:The auto stripped version contains the messages after quoted material has been automatically removed .
5-9:The hand stripped version contains the messages after quoted material has been removed by a human .
5-10:Additionally, the hand stripped version has had any xml content and e mail signatures removed leaving only the essential content of the message .
5-11:The studies reported here are performed with the hand stripped version .
5-12:This allows us to balance the cognitive load in terms of number of tokens that must be read in the user studies we report including quoted material would complicate the user studies since some users might skip the material while others read it .
5-13:Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the corpus that can be made available for some outside experimentation .
5-14:Please contact the authors for more information on obtaining this data .
5-15:prevents tainting the cross validation since otherwise a test item could occur as quoted material in a training document .
5-16:4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action item .
5-17:In addition, they identified each segment of the e mail which contained an action item .
5-18:A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence .
5-19:They were instructed that an action item is an explicit request for information that requires the recipient"s attention or a required action and told to highlight the phrases or sentences that make up the request .
5-20:Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items .
5-21:Annotator Two labeled 327 messages as containing action items .
5-22:The agreement of the human annotators is shown in Tables 1 and both marked the same document as containing no action items or both marked at least one action item regardless of whether the text segments were the same .
5-23:At the document level, the annotators agreed 93% of the time .
5-24:The kappa statistic [3, 5] is often used to evaluate inter annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement .
5-25:R is the empirical estimate of the probability of random agreement given the empirical class priors .
5-26:A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected .
5-27:At the document level, the kappa statistic for inter annotator agreement is 0.85 .
5-28:This value is both strong enough to expect the problem to be learnable and is comparable with results for similar tasks [5, 6] .
5-29:In order to determine the sentence level agreement, we use each judgment to create a sentence corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences .
5-30:This allows us to compare agreement over no judgments .
5-31:We perform this comparison over the hand stripped corpus since that eliminates spurious no judgments that would come from including quoted material, etc .
5-32:Both annotators were free to label the subject as an action item, but since neither did, we omit the subject line of the message as well .
5-33:This only reduces the number of no agreements .
5-34:This leaves 6301 automatically segmented sentences .
5-35:At the sentence level, the annotators agreed 98% of the time, and the kappa statistic for inter annotator agreement is 0.82 .
5-36:In order to produce one single set of judgments, the human annotators went through each annotation where there was disagreement and came to a consensus opinion .
5-37:The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements .
5-38:For example, If you would like to keep your job, come to tomorrow"s meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrow"s meeting does not .
5-39:The first would be an action item in most contexts while the second would not .
5-40:Of course, many conditional statements are not so clearly interpretable .
5-41:After reconciling the judgments there are 416 e mails with no action items and 328 e mails containing actionitems .
5-42:Of the 328 e mails containing action items, 259 messages have one action item segment; 55 messages have two action item segments; 11 messages have three action item segments .
5-43:Two messages have four action item segments, and one message has six action item segments .
5-44:Computing the sentence level agreement using the reconciled gold standard judgments with each of the annotators" individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two .
5-45:In terms of message characteristics, there were on average 132 content tokens in the body after stripping .
5-46:For action item messages, there were 115 .
5-47:However, by examining Figure 2 we see the length distributions are nearly identical .
5-48:As would be expected for e mail, it is a long tailed distribution with about half the messages having more than 60 tokens in the body (this paragraph has 65 tokens) .
5-49:4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms .
5-50:In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs .
5-51:generative and lazy vs .
5-52:eager .
5-53:We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand .
5-54:This is important since it is easy to improve a strawman classifier by introducing a new representation .
5-55:By thoroughly sampling alternative classifier choices we demonstrate that representation improvements over bag of words are not due to using the information in the bag of words poorly .
5-56:4.2.1 kNN We employ a standard variant of the k nearest neighbor algorithm used in text classification, kNN with s cut score thresholding [19] .
5-57:We use a tfidf weighting of the terms with a distanceweighted vote of the neighbors to compute the score before thresholding it .
5-58:In order to choose the value of s for thresholding, we perform leave one out cross validation over the training set .
5-59:The value of k is set to be 2( log2 N + 1) where N is the number of training points .
5-60:This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal classifier as the number of training points increases [8] .
5-61:In practice, we have also found it to be a computational convenience that frequently leads to comparable results with numerically optimizing k via a cross validation procedure .
5-62:4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16] .
5-63:In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m estimate, respectively .
5-64:0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length .
5-65:A bin size of 20 words was used .
5-66:Only tokens in the body after hand stripping were counted .
5-67:After stripping, the majority of words left are usually actual message content .
5-68:Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document Detection Performance during Cross Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics .
5-69:The best performance for each classifier is shown in bold .
5-70:4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2 norm as implemented in the SVMlight package v6.01 [11] .
5-71:All default settings were used .
5-72:4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel based learning method .
5-73:We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf weighting and an L2 norm .
5-74:The voted perceptron is an online learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct .
5-75:With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron .
5-76:The output of the classifier uses the weights on the perceptra to make a final voted classification .
5-77:When used in an offline manner, multiple passes can be made through the training data .
5-78:Both the voted perceptron and the SVM give a solution from the same hypothesis space in this case, a linear classifier .
5-79:Furthermore, it is well known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10] .
5-80:Since Cohen et al .
5-81:[5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin .
5-82:Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a competitive alternative classifier to the SVM for the basic bag of words representation .
5-83:4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy .
5-84:The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives .
5-85:4.4 Experimental Methodology We perform standard 10 fold cross validation on the set of documents .
5-86:For the sentence level approach, all sentences in a document are either entirely in the training set or entirely in the test set for each fold .
5-87:For significance tests, we use a two tailed t test [21] to compare the values obtained during each cross validation fold with a p value of 0.05 .
5-88:Feature selection was performed using the chi squared statistic .
5-89:Different levels of feature selection were considered for each classifier .
5-90:Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000 .
5-91:There are approximately 4700 unigram tokens without feature selection .
5-92:In order to choose the number of features to use for each classifier, we perform nested cross validation and choose the settings that yield the optimal document level F1 for that classifier .
5-93:For this study, only the body of each e mail message was used .
5-94:Feature selection is always applied to all candidate features .
5-95:That is, for the n gram representation, the n grams and position features are also subject to removal by the feature selection method .
5-96:4.5 Results The results for document level classification are given in Table are critical for this task; if this is true, we expect to see a significant gap in performance between the document level classifiers that use n grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram) .
5-97:Examining Table 3, we observe that this is indeed the case for every classifier except na¨ıve Bayes .
5-98:This difference in performance produced by the n gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4) .
5-99:Na¨ıve Bayes poor performance with the n gram representation is not surprising since the bag of n grams causes excessive doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence level because the sparse examples provide few chances for agglomerative effects of double counting .
5-100:In either case, when a language modeling approach is desired, modeling the n grams directly would be preferable to na¨ıve Bayes .
5-101:More importantly for the n gram hypothesis, the n grams lead to the best document level classifier performance as well .
5-102:As would be expected, the difference between the sentence level n gram representation and unigram representation is small .
5-103:This is because the window of text is so small that the unigram representation, when done at the sentence level, implicitly picks up on the power of the n grams .
5-104:Further improvement would signify that the order of the words matter even when only considering a small sentence size window .
5-105:Therefore, the finer grained sentence level judgments allows a unigram representation to succeed but only when performed in a small window behaving as an n gram representation for all practical purposes .
5-106:Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n grams versus unigrams for document detection using document level and sentence level classifiers .
5-107:When the F1 result is statistically significant, it is shown in bold .
5-108:When the accuracy result is significant, it is shown with a † .
5-109:F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence level classifiers vs .
5-110:document level classifiers for the document detection problem .
5-111:When the result is statistically significant, it is shown in bold .
5-112:Further highlighting the improvement from finer grained judgments and n grams, Figure 3 graphically depicts the edge the SVM sentence level classifier has over the standard bag of words approach with a precision recall curve .
5-113:In the high precision area of the graph, the consistent edge of the sentence level classifier is rather impressive continuing at precision 1 out to 0.1 recall .
5-114:This would mean that a tenth of the user"s action items would be placed at the top of their action item sorted inbox .
5-115:Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score .
5-116:Considering the relative unexplored nature of classification at the sentence level, this gives great hope for further increases in performance .
5-117:Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence Level Classifiers at Sentence Detection Although Cohen et al .
5-118:[5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of similar tasks, we see no such behavior here .
5-119:This further strengthens the evidence that an alternate classifier with the bag of words representation could not reach the same level of performance .
5-120:The Voted Perceptron classifier does improve when the number of training iterations are increased, but it is still lower than the SVM classifier .
5-121:Sentence detection results are presented in Table 6 .
5-122:With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem .
5-123:That is, unlike document detection where actionitem documents are fairly common, action item sentences are very rare .
5-124:Thus, as in other text problems, the accuracy numbers are deceptively high sheerly because of the default accuracy attainable by always predicting no .
5-125:Although, the results here are significantly above random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification .
5-126:Figure 4: Users find action items quicker when assisted by a classification system .
5-127:Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end user .
5-128:In order to demonstrate the impact this task can have on e mail users, we conducted a user study using an earlier less accurate version of the sentence classifier where instead of using just a single sentence, a threesentence windowed approach was used .
5-129:There were three distinct sets of e mail in which users had to find action items .
5-130:These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n grams and a small prediction window lead to consistent improvements over the standard approach .
5-131:center sentence in the highest confidence window highlighted (Order+help) .
5-132:In order to perform fair comparisons between conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifier"s reordering .
5-133:Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets .
5-134:This is typically handled by varying the ordering of the sets across users so that the means are comparable .
5-135:While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects .
5-136:Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action items when assisted by the classifier, but were most critically aided in the first five minutes .
5-137:Although, the classifier consistently aids the users, we did not gain an additional end user impact by highlighting .
5-138:As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection .
5-139:For example, highlighting the wrong sentence near an actual action item hurts the user"s trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near miss .
5-140:Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy .
5-141:4.6 Discussion In contrast to problems where n grams have yielded little difference, we believe their power here stems from the fact that many of the meaningful n grams for action items consist of common words, e.g., let me know .
5-142:Therefore, the document level unigram approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co occur in the document but not necessarily in a phrase .
5-143:Additionally, action item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document .
5-144:As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document .
5-145:Even though we discarded the header information, examining the top ranked features at the document level reveals that many of the features are names or parts of e mail addresses that occurred in the body and are highly associated with e mails that tend to contain many or no action items .
5-146:A few examples are terms such as org, bob, and gov .
5-147:We note that these features will be sensitive to the particular distribution (senders receivers) and thus the document level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions .
5-148:This points out that part of the problem of going beyond bag of words may be the methodology, and investigating such properties as learning curves and how well a model transfers may highlight differences in models which appear to have similar performance when tested on the distributions they were trained on .
5-149:We are currently investigating whether the sentence level classifiers do perform better over different test corpora without retraining. .
6 FUTURE WORK :
6-1:While applying text classifiers at the document level is fairly well understood, there exists the potential for significantly increasing the performance of the sentence level classifiers .
6-2:Such methods include alternate ways of combining the predictions over each sentence, weightings other than tfidf, which may not be appropriate since sentences are small, better sentence segmentation, and other types of phrasal analysis .
6-3:Additionally, named entity tagging, time expressions, etc., seem likely candidates for features that can further improve this task .
6-4:We are currently pursuing some of these avenues to see what additional gains these offer .
6-5:Finally, it would be interesting to investigate the best methods for combining the document level and sentence level classifiers .
6-6:Since the simple bag of words representation at the document level leads to a learned model that behaves somewhat like a context specific prior dependent on the sender receiver and general topic, a first choice would be to treat it as such when combining probability estimates with the sentence level classifier .
6-7:Such a model might serve as a general example for other problems where bag of words can establish a baseline model but richer approaches are needed to achieve performance beyond that baseline. .
7-1:The effectiveness of sentence level detection argues that labeling at the sentence level provides significant value
7-2:Further experiments are needed to see how this interacts with the amount of training data available
7-3:Sentence detection that is then agglomerated to document level detection works surprisingly better given low recall than would be expected with sentence level items
7-4:This, in turn, indicates that improved sentence segmentation methods could yield further improvements in classification
7-5:In this work, we examined how action items can be effectively detected in e mails
7-6:Our empirical analysis has demonstrated that n grams are of key importance to making the most of documentlevel judgments
7-7:When finer grained judgments are available, then a standard bag of words approach using a small (sentence) window size and automatic segmentation techniques can produce results almost as good as the n gram based approaches
7-8:Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No
7-9:NBCHD030010
7-10:Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Department of InteriorNational Business Center (DOI NBC)
7-11:We would like to extend our sincerest thanks to Jill Lehman whose efforts in data collection were essential in constructing the corpus, and both Jill and Aaron Steinfeld for their direction of the HCI experiments
7-12:We would also like to thank Django Wexler for constructing and supporting the corpus labeling tools and Curtis Huttenhower"s support of the text preprocessing package
7-13:Finally, we gratefully acknowledge Scott Fahlman for his encouragement and useful discussions on this topic.
8-1:J
8-2:Allan, J
8-3:Carbonell, G
8-4:Doddington, J
8-5:Yamron, and Y
8-6:Yang
8-7:Topic detection and tracking pilot study: Final report
8-8:In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Washington, D.C., 1998
8-9:C
8-10:Apte, F
8-11:Damerau, and S
8-12:M
8-13:Weiss
8-14:Automated learning of decision rules for text categorization
8-15:ACM Transactions on Information Systems, 12(3):233 251, July 1994
8-16:J
8-17:Carletta
8-18:Assessing agreement on classification tasks: The kappa statistic
8-19:Computational Linguistics, 22(2):249 254, 1996
8-20:J
8-21:Carroll
8-22:High precision extraction of grammatical relations
8-23:In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 134 140, 2002
8-24:W
8-25:W
8-26:Cohen, V
8-27:R
8-28:Carvalho, and T
8-29:M
8-30:Mitchell
8-31:Learning to classify email into speech acts
8-32:In EMNLP 2004 (Conference on Empirical Methods in Natural Language Processing), pages 309 316, 2004
8-33:S
8-34:Corston Oliver, E
8-35:Ringger, M
8-36:Gamon, and R
8-37:Campbell
8-38:Task focused summarization of email
8-39:In Text Summarization Branches Out: Proceedings of the ACL 04 Workshop, pages 43 50, 2004
8-40:A
8-41:Culotta, R
8-42:Bekkerman, and A
8-43:McCallum
8-44:Extracting social networks and contact information from email and the web
8-45:In CEAS 2004 (Conference on Email and Anti Spam), Mountain View, CA, July 2004
8-46:L
8-47:Devroye, L
8-48:Gy¨orfi, and G
8-49:Lugosi
8-50:A Probabilistic Theory of Pattern Recognition
8-51:Springer Verlag, New York, NY, 1996
8-52:S
8-53:T
8-54:Dumais, J
8-55:Platt, D
8-56:Heckerman, and M
8-57:Sahami
8-58:Inductive learning algorithms and representations for text categorization
8-59:In CIKM "98, Proceedings of the 7th ACM Conference on Information and Knowledge Management, pages 148 155, 1998
8-60:Y
8-61:Freund and R
8-62:Schapire
8-63:Large margin classification using the perceptron algorithm
8-64:Machine Learning, 37(3):277 296, 1999
8-65:T
8-66:Joachims
8-67:Making large scale svm learning practical
8-68:In B
8-69:Sch¨olkopf, C
8-70:J
8-71:Burges, and A
8-72:J
8-73:Smola, editors, Advances in Kernel Methods  Support Vector Learning, pages 41 56
8-74:MIT Press, 1999
8-75:L
8-76:S
8-77:Larkey
8-78:A patent search and classification system
8-79:In Proceedings of the Fourth ACM Conference on Digital Libraries, pages 179  187, 1999
8-80:D
8-81:D
8-82:Lewis
8-83:An evaluation of phrasal and clustered representations on a text categorization task
8-84:In SIGIR "92, Proceedings of the 15th Annual International ACM Conference on Research and Development in Information Retrieval, pages 37 50, 1992
8-85:Y
8-86:Liu, J
8-87:Carbonell, and R
8-88:Jin
8-89:A pairwise ensemble approach for accurate genre classification
8-90:In Proceedings of the European Conference on Machine Learning (ECML), 2003
8-91:Y
8-92:Liu, R
8-93:Yan, R
8-94:Jin, and J
8-95:Carbonell
8-96:A comparison study of kernels for multi label text classification using category association
8-97:In The Twenty first International Conference on Machine Learning (ICML), 2004
8-98:A
8-99:McCallum and K
8-100:Nigam
8-101:A comparison of event models for naive bayes text classification
8-102:In Working Notes of AAAI "98 (The 15th National Conference on Artificial Intelligence), Workshop on Learning for Text Categorization, pages 41 48, 1998
8-103:TR WS 98 05
8-104:F
8-105:Sebastiani
8-106:Machine learning in automated text categorization
8-107:ACM Computing Surveys, 34(1):1 47, March 2002
8-108:C
8-109:J
8-110:van Rijsbergen
8-111:Information Retrieval
8-112:Butterworths, London, 1979
8-113:Y
8-114:Yang
8-115:An evaluation of statistical approaches to text categorization
8-116:Information Retrieval, 1(1 2):67 88, 1999
8-117:Y
8-118:Yang, J
8-119:Carbonell, R
8-120:Brown, T
8-121:Pierce, B
8-122:T
8-123:Archibald, and X
8-124:Liu
8-125:Learning approaches to topic detection and tracking
8-126:IEEE EXPERT, Special Issue on Applications of Intelligent Information Retrieval, 1999
8-127:Y
8-128:Yang and X
8-129:Liu
8-130:A re examination of text categorization methods
8-131:In SIGIR "99, Proceedings of the 22nd Annual International ACM Conference on Research and Development in Information Retrieval, pages 42 49, 1999
8-132:Y
8-133:Yang, J
8-134:Zhang, J
8-135:Carbonell, and C
8-136:Jin
8-137:Topic conditioned novelty detection
8-138:In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 2002
picture:
