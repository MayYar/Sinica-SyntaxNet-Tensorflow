A Time Machine for Text Search 
content:
1 ABSTRACT :
1-1:Text search over temporally versioned document collections such as web archives has received little attention as a research problem .
1-2:As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t .
1-3:In this work, we address this shortcoming and propose an efficient solution for time travel text search by extending the inverted file index to make it ready for temporal search .
1-4:We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results .
1-5:In order to further improve the performance of time travel queries, we introduce two principled techniques to trade off index size for its performance .
1-6:These techniques can be formulated as optimization problems that can be solved to near optimality .
1-7:Finally, our approach is evaluated in a comprehensive series of experiments on two large scale real world datasets .
1-8:Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections .
1-9:H.3.1 [Content Analysis and Indexing]: Indexing .
2 INTRODUCTION :
2-1:In this work we address time travel text search over temporally versioned document collections .
2-2:Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t .
2-3:An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds .
2-4:Text search on these collections, however, is mostly time ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents .
2-5:Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text search functionality is often completely missing .
2-6:Time travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates .
2-7:For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians .
2-8:Sending an appropriate query to a major web search engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives .
2-9:If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalist"s information need .
2-10:Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered .
2-11:Looking at their evolutionary history, we are faced with even larger data volumes .
2-12:As a consequence, na¨ıve approaches to time travel text search fail, and viable approaches must scale up well to such large data volumes .
2-13:This paper presents an efficient solution to time travel text search by making the following key contributions: transparently extended to enable time travel text search .
2-14:indexsize explosion while keeping results highly accurate .
2-15:improve index performance that allow trading off space vs .
2-16:performance .
2-17:approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large scale real world datasets with versioned documents .
2-18:The remainder of this paper is organized as follows .
2-19:The presented work is put in context with related work in Section 2 .
2-20:We delineate our model of a temporally versioned document collection in Section 3 .
2-21:We present our time travel inverted index in Section 4 .
2-22:Building on it, temporal coalescing is described in Section 5 .
2-23:In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. .
3 RELATED WORK :
3-1:We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document content overlap or by pruning portions of the index .
3-2:We briefly review work under these categories here .
3-3:To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents .
3-4:Anick and Flynn [3], while pioneering this research, describe a help desk system that supports historical queries .
3-5:Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past .
3-6:Burrows and Hisgen [10], in a patent description, delineate a method for indexing range based values and mention its potential use for searching based on dates associated with documents .
3-7:Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text containment queries only and neglect the relevance scoring of results .
3-8:Stack [29] reports practical experiences made when adapting the open source search engine Nutch to search web archives .
3-9:This adaptation, however, does not provide the intended time travel text search functionality .
3-10:In contrast, research in temporal databases has produced several index structures tailored for time evolving databases; a comprehensive overview of the state of art is available in [28] .
3-11:Unlike the inverted file index, their applicability to text search is not well understood .
3-12:Moving on to the second category of related work, Broder et al .
3-13:[8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size .
3-14:Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context .
3-15:More recent approaches by Hersovici et al .
3-16:[17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size .
3-17:None of the approaches, however, considers time explicitly or provides the desired time travel text search functionality .
3-18:Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result .
3-19:They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index size reduction .
3-20:It should be noted that index pruning techniques can be adapted to work along with the temporal text index we propose here. .
4 MODEL :
4-1:In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following .
4-2:Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , .
4-3:.
4-4:.
4-5:.
4-6:Each version dti has an associated timestamp ti reflecting when the version was created .
4-7:Each version is a vector of searchable terms or features .
4-8:Any modification to a document version results in the insertion of a new version with corresponding timestamp .
4-9:We employ a discrete definition of time, so that timestamps are non negative integers .
4-10:The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥ .
4-11:The validity time interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now) .
4-12:Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .
4-13:As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t .
4-14:The enriched time travel query is written as q t for brevity .
4-15:As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf idf [4] or language models [26] as well .
4-16:For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .
4-17:In the above formula, the relevance w(q t , dti ) of a document version dti to the time travel query q t is defined .
4-18:We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered .
4-19:The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .
4-20:It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti .
4-21:The length normalization parameter b and the tf saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively .
4-22:The second factor widf (v, t), which we refer to as the idf score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t .
4-23:While the idf score depends on the whole corpus as of the query time t, the tf score is specific to each version. .
5 TIME TRAVELINVERTEDFILEINDEX :
5-1:The inverted file index is a standard technique for text indexing, deployed in many systems .
5-2:In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time travel text search .
5-3:4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+ Tree, that maps each term to its idfscore and inverted list .
5-4:The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document identifier and p is the so called payload .
5-5:The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document .
5-6:The sort order of index lists depends on which queries are to be supported efficiently .
5-7:For Boolean queries it is favorable to sort index lists in document order .
5-8:Frequencyorder and impact order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31] .
5-9:A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists .
5-10:For an excellent recent survey about inverted file indexes we refer to [35] .
5-11:4.2 Time Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information .
5-12:The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid .
5-13:The postings in our time travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time interval .
5-14:As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .
5-15:Similarly, the extended vocabulary structure maintains for each term a time series of idf scores organized as a B+Tree .
5-16:Unlike the tf score, the idf score of every term could vary with every change in the corpus .
5-17:Therefore, we take a simplified approach to idf score maintenance, by computing idf scores for all terms in the corpus at specific (possibly periodic) times .
5-18:4.3 Query Processing During processing of a time travel query q t , for each query term the corresponding idf score valid at time t is retrieved from the extended vocabulary .
5-19:Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings .
5-20:We transparently extend the sequential reading, which is to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time travel queryprocessing .
5-21:To this end, sequential reading is extended by skipping all postings whose validity time interval does not contain t (i.e., t ∈ [tb, te)) .
5-22:Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I O cost .
5-23:As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I O overhead significantly .
5-24:We note that our proposed extension of the inverted file index makes no assumptions about the sort order of index lists .
5-25:As a consequence, existing query processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. .
6 TEMPORAL COALESCING :
6-1:If we employ the time travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version .
6-2:For frequent terms and large highly dynamic collections, this time score non coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance .
6-3:The approximate temporal coalescing technique that we propose in this section counters this blowup in index list size .
6-4:It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched .
6-5:As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all .
6-6:Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded .
6-7:This idea is illustrated in Figure 1, which plots non coalesced and coalesced scores of postings belonging to a single document .
6-8:Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example .
6-9:The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al .
6-10:[6], where the simpler problem of coalescing only equal information was considered .
6-11:We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions .
6-12:Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv .
6-13:As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), .
6-14:.
6-15:.
6-16:, ( d, pn−1, [tn−1, tn)) ) .
6-17:Each sequence represents a contiguous time period during which the term was present in a single document d .
6-18:If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately .
6-19:We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), .
6-20:.
6-21:.
6-22:, ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time range, i.e., ti = tj and tn = tm .
6-23:Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .
6-24:In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .
6-25:In this paper, as an error function we employ the relative error between payloads (i.e., tf scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| |pi| .
6-26:Finding an optimal output sequence of postings can be cast into finding a piecewise constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee .
6-27:Similar problems occur in time series segmentation [21, 30] and histogram construction [19, 20] .
6-28:Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence .
6-29:In our setting, as a key difference, only a guarantee on the local error is retained in contrast to a guarantee on the global error in the aforementioned settings .
6-30:Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time .
6-31:Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5] .
6-32:The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work .
6-33:As an alternative, we introduce a linear time approximate algorithm that is based on the sliding window algorithm given in [21] .
6-34:This algorithm produces nearly optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution .
6-35:Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), .
6-36:.
6-37:.
6-38:O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I .
6-39:While doing so, it coalesces sequences of postings having maximal length .
6-40:The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details) .
6-41:When reading the next posting, the algorithm tries to add it to the current sequence of postings .
6-42:It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee .
6-43:If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized .
6-44:The time complexity of the algorithm is in O(n) .
6-45:Note that, since we make no assumptions about the sort order of index lists, temporal coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. .
7 SUBLIST MATERIALIZATION :
7-1:Efficiency of processing a query q t on our time travel inverted index is influenced adversely by the wasted I O due to read but skipped postings .
7-2:Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains .
7-3:In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index .
7-4:Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist .
7-5:Note that all those postings whose validity time interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists .
7-6:Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t .
7-7:We illustrate the idea of sublist materialization using an example shown in Figure 2 .
7-8:The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3 .
7-9:For ease of description, we have numbered boundaries of validity time intervals, in increasing time order, as t1, .
7-10:.
7-11:.
7-12:, t10 and numbered the postings themselves as 1, .
7-13:.
7-14:.
7-15:, 10 .
7-16:Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list .
7-17:Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case .
7-18:Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively .
7-19:Then, we can process the above query with optimal cost by reading only those postings that existed at this t .
7-20:At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section .
7-21:However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone .
7-22:The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries .
7-23:Further, the two techniques, can be applied separately and are independent .
7-24:If applied in conjunction, though, there is a synergetic effect sublists that are materialized from a temporally coalesced index are generally smaller .
7-25:We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .
7-26:To aid the presentation in the rest of the paper, we first provide some definitions .
7-27:Let T = t1 .
7-28:.
7-29:.
7-30:tn be the sorted sequence of all unique time interval boundaries of an inverted list Lv .
7-31:Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals .
7-32:We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time travel queries q t for all t ∈ [t1, tn) can be processed .
7-33:We also assume that intervals in M are disjoint .
7-34:We can make this assumption without ruling out any optimal solution with regard to space or performance defined below .
7-35:The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M .
7-36:Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1) .
7-37:The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ) .
7-38:Thus, in order to optimize the performance of processing queries we minimize their processing costs .
7-39:6.1 Performance Space Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E .
7-40:In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved .
7-41:Therefore, we will refer to this approach as Popt in the remainder .
7-42:The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder .
7-43:This approach requires minimal space, since it keeps each posting exactly once .
7-44:Popt and Sopt are extremes: the former provides the best possible performance but is not space efficient, the latter requires minimal space but does not provide good performance .
7-45:The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach .
7-46:6.2 Performance Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly identical sublists .
7-47:In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting .
7-48:If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3) .
7-49:The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained .
7-50:In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1 .
7-51:Formally, this problem can be stated as argmin M S( M ) s.t .
7-52:∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .
7-53:An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .
7-54:Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee .
7-55:Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5] .
7-56:The time complexity of the algorithm is in O(n2 ) for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed .
7-57:The space complexity is in O(n2 ) the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems .
7-58:6.3 Space Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space .
7-59:In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit .
7-60:The technique presented next, which is named SB, tackles this very problem .
7-61:The space restriction is modeled by means of a user specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space optimal solution provided by Sopt .
7-62:The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance) .
7-63:In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time point being in [ti, ti+1) .
7-64:Formally, this space bound sublist materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t .
7-65:X m∈M |Lv : m| ≤ κ |Lv| .
7-66:The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization .
7-67:A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5] .
7-68:Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets .
7-69:We obtain an approximate solution to the problem using simulated annealing [22, 23] .
7-70:Simulated annealing takes a fixed number R of rounds to explore the solution space .
7-71:In each round a random successor of the current solution is looked at .
7-72:If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept) .
7-73:A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution .
7-74:If it achieves higher expected processing cost, it is randomly accepted with probability e−∆ r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds .
7-75:In addition, throughout all rounds, the method keeps track of the best solution seen so far .
7-76:The solution space for the problem at hand can be efficiently explored .
7-77:As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals .
7-78:We represent such a set M as an array of n boolean variables b1 .
7-79:.
7-80:.
7-81:bn that convey the boundaries of time intervals in the set .
7-82:Note that b1 and bn are always set to true .
7-83:Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) } .
7-84:A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables .
7-85:The time complexity of the method is in O(n2 ) the expected processing cost must be computed in each round .
7-86:Its space complexity is in O(n) for keeping the n boolean variables .
7-87:As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. .
8 EXPERIMENTAL EVALUATION :
8-1:We conducted a comprehensive series of experiments on two real world datasets to evaluate the techniques proposed in this paper .
8-2:7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5 .
8-3:All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network attached RAID 5 disk array, and running Microsoft Windows Server 2003 .
8-4:All data and indexes are kept in an Oracle 10g database that runs on the same machine .
8-5:For our experiments we used two different datasets .
8-6:The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file .
8-7:This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download) .
8-8:We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.) .
8-9:This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18 .
8-10:We built a time travel query workload using the query log temporarily made available recently by AOL Research as follows we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.) .
8-11:The thus extracted queries contained a total of 422 distinct terms .
8-12:For each extracted query, we randomly picked a time point for each month covered by the dataset .
8-13:This resulted in a total of 18, 000 (= 300 × 60) time travel queries .
8-14:The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data .
8-15:We filtered out documents not belonging to MIME types text plain and text html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper .
8-16:This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79) .
8-17:We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc.), and randomly sampling a time point for every month within the two year period spanned by the dataset .
8-18:Thus, we obtained a total of 7,200 (= 300 × 24) time travel queries for the UKGOV dataset .
8-19:In total 522 terms appear in the extracted queries .
8-20:The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets .
8-21:7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index size reduction and its effect on the result quality .
8-22:For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non coalesced index as a baseline .
8-23:WIKI UKGOV # Postings Ratio # Postings Ratio 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non coalesced index ( ) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings .
8-24:As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size .
8-25:Even a small threshold value, e.g .
8-26:= 0.01, has a considerable effect by reducing the index size almost by an order of magnitude .
8-27:Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size .
8-28:Index size continues to reduce on both datasets, as we increase the value of .
8-29:How does the reduction in index size affect the query results? In order to evaluate this aspect, we compared the top k results computed using a coalesced index against the ground truth result obtained from the original index, for different cutoff levels k .
8-30:Let Gk and Ck be the top k documents from the ground truth result and from the coalesced index respectively .
8-31:We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck| k .
8-32:(ii) Kendall"s τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or 1) indicating total agreement (or disagreement) .
8-33:Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01 .
8-34:Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph .
8-35:It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits .
8-36:For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are 1 0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 Kendall's τ @ 10 Relative Recall @ 10 Kendall's τ @ 10 (a) @10 1 0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 Kendall's τ @ 100 Relative Recall @ 100 Kendall's τ @ 100 (b) @100 Figure 3: Relative recall and Kendall"s τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index .
8-37:Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95 .
8-38:For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively .
8-39:On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values .
8-40:7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6 .
8-41:For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10 .
8-42:In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations .
8-43:However, note that the postings in the materialized sublists still retain their original timestamps .
8-44:For a comparative evaluation of the four approaches Popt, Sopt, PG, and SB we measure space and performance as follows .
8-45:The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists .
8-46:To assess performance we compute the expected processing cost for all terms in the respective query workload assuming a uniform probability distribution among query time points .
8-47:We report the mean EPC, as well as the 5% and 95% percentile .
8-48:In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload .
8-49:The Sopt and Popt approaches are, by their definition, parameter free .
8-50:For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0 .
8-51:Analogously, for the SB approach the parameter κ, as an upper bound on the allowed space blowup, was varied between 1.0 and 3.0 .
8-52:Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds .
8-53:Table 2 lists the obtained space and performance figures .
8-54:Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus .
8-55:Based on the depicted results, we make the following key observations .
8-56:i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption .
8-57:Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost .
8-58:The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent .
8-59:ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets .
8-60:iii) The SB approach achieves close to optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude .
8-61:We also measured wall clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. .
9 CONCLUSIONS :
9-1:In this work we have developed an efficient solution for time travel text search over temporally versioned document collections .
9-2:Experiments on two real world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results .
9-3:The present work opens up many interesting questions for future research, e.g.: How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]? .
9-4:How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point? How can the described time travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? .
10-1:We are grateful to the anonymous reviewers for their valuable comments  in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2
10-2:10
10-3:REFERENCES
11-1:V
11-2:N
11-3:Anh and A
11-4:Moffat
11-5:Pruned Query Evaluation Using Pre Computed Impacts
11-6:In SIGIR, 2006
11-7:V
11-8:N
11-9:Anh and A
11-10:Moffat
11-11:Pruning Strategies for Mixed Mode Querying
11-12:In CIKM, 2006
11-13:WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) P
11-14:G
11-15:Anick and R
11-16:A
11-17:Flynn
11-18:Versioning a Full Text Information Retrieval System
11-19:In SIGIR, 1992
11-20:R
11-21:A
11-22:Baeza Yates and B
11-23:Ribeiro Neto
11-24:Modern Information Retrieval
11-25:Addison Wesley, 1999
11-26:K
11-27:Berberich, S
11-28:Bedathur, T
11-29:Neumann, and G
11-30:Weikum
11-31:A Time Machine for Text search
11-32:Technical Report MPI I 2007 5 002, Max Planck Institute for Informatics, 2007
11-33:M
11-34:H
11-35:B¨ohlen, R
11-36:T
11-37:Snodgrass, and M
11-38:D
11-39:Soo
11-40:Coalescing in Temporal Databases
11-41:In VLDB, 1996
11-42:P
11-43:Boldi, M
11-44:Santini, and S
11-45:Vigna
11-46:Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations
11-47:In WAW, 2004
11-48:A
11-49:Z
11-50:Broder, N
11-51:Eiron, M
11-52:Fontoura, M
11-53:Herscovici, R
11-54:Lempel, J
11-55:McPherson, R
11-56:Qi, and E
11-57:J
11-58:Shekita
11-59:Indexing Shared Content in Information Retrieval Systems
11-60:In EDBT, 2006
11-61:C
11-62:Buckley and A
11-63:F
11-64:Lewit
11-65:Optimization of Inverted Vector Searches
11-66:In SIGIR, 1985
11-67:M
11-68:Burrows and A
11-69:L
11-70:Hisgen
11-71:Method and Apparatus for Generating and Searching Range Based Index of Word Locations
11-72:U.S
11-73:Patent 5,915,251, 1999
11-74:S
11-75:B¨uttcher and C
11-76:L
11-77:A
11-78:Clarke
11-79:A Document Centric Approach to Static Index Pruning in Text Retrieval Systems
11-80:In CIKM, 2006
11-81:D
11-82:Carmel, D
11-83:Cohen, R
11-84:Fagin, E
11-85:Farchi, M
11-86:Herscovici, Y
11-87:S
11-88:Maarek, and A
11-89:Soffer
11-90:Static Index Pruning for Information Retrieval Systems
11-91:In SIGIR, 2001
11-92:http:  www.europarchive.org
11-93:R
11-94:Fagin, R
11-95:Kumar, and D
11-96:Sivakumar
11-97:Comparing Top k Lists
11-98:SIAM J
11-99:Discrete Math., 17(1):134 160, 2003
11-100:R
11-101:Fagin, A
11-102:Lotem, and M
11-103:Naor
11-104:Optimal Aggregation Algorithms for Middleware
11-105:J
11-106:Comput
11-107:Syst
11-108:Sci., 66(4):614 656, 2003
11-109:S
11-110:Guha, K
11-111:Shim, and J
11-112:Woo
11-113:REHIST: Relative Error Histogram Construction Algorithms
11-114:In VLDB, 2004
11-115:M
11-116:Hersovici, R
11-117:Lempel, and S
11-118:Yogev
11-119:Efficient Indexing of Versioned Document Sequences
11-120:In ECIR, 2007
11-121:http:  www.archive.org
11-122:Y
11-123:E
11-124:Ioannidis and V
11-125:Poosala
11-126:Balancing Histogram Optimality and Practicality for Query Result Size Estimation
11-127:In SIGMOD, 1995
11-128:H
11-129:V
11-130:Jagadish, N
11-131:Koudas, S
11-132:Muthukrishnan, V
11-133:Poosala, K
11-134:C
11-135:Sevcik, and T
11-136:Suel
11-137:Optimal Histograms with Quality Guarantees
11-138:In VLDB, 1998
11-139:E
11-140:J
11-141:Keogh, S
11-142:Chu, D
11-143:Hart, and M
11-144:J
11-145:Pazzani
11-146:An Online Algorithm for Segmenting Time Series
11-147:In ICDM, 2001
11-148:S
11-149:Kirkpatrick, D
11-150:G
11-151:Jr., and M
11-152:P
11-153:Vecchi
11-154:Optimization by Simulated Annealing
11-155:Science, 220(4598):671 680, 1983
11-156:J
11-157:Kleinberg and E
11-158:Tardos
11-159:Algorithm Design
11-160:Addison Wesley, 2005
11-161:U
11-162:Manber
11-163:Introduction to Algorithms: A Creative Approach
11-164:Addison Wesley, 1989
11-165:K
11-166:Nørv˚ag and A
11-167:O
11-168:N
11-169:Nybø
11-170:DyST: Dynamic and Scalable Temporal Text Indexing
11-171:In TIME, 2006
11-172:J
11-173:M
11-174:Ponte and W
11-175:B
11-176:Croft
11-177:A Language Modeling Approach to Information Retrieval
11-178:In SIGIR, 1998
11-179:S
11-180:E
11-181:Robertson and S
11-182:Walker
11-183:Okapi Keenbow at TREC 8
11-184:In TREC, 1999
11-185:B
11-186:Salzberg and V
11-187:J
11-188:Tsotras
11-189:Comparison of Access Methods for Time Evolving Data
11-190:ACM Comput
11-191:Surv., 31(2):158 221, 1999
11-192:M
11-193:Stack
11-194:Full Text Search of Web Archive Collections
11-195:In IWAW, 2006
11-196:E
11-197:Terzi and P
11-198:Tsaparas
11-199:Efficient Algorithms for Sequence Segmentation
11-200:In SIAM DM, 2006
11-201:M
11-202:Theobald, G
11-203:Weikum, and R
11-204:Schenkel
11-205:Top k Query Evaluation with Probabilistic Guarantees
11-206:In VLDB, 2004
11-207:http:  www.wikipedia.org
11-208:I
11-209:H
11-210:Witten, A
11-211:Moffat, and T
11-212:C
11-213:Bell
11-214:Managing Gigabytes: Compressing and Indexing Documents and Images
11-215:Morgan Kaufmann publishers Inc., 1999
11-216:J
11-217:Zhang and T
11-218:Suel
11-219:Efficient Search in Large Textual Collections with Redundancy
11-220:In WWW, 2007
11-221:J
11-222:Zobel and A
11-223:Moffat
11-224:Inverted Files for Text Search Engines
11-225:ACM Comput
11-226:Surv., 38(2):6, 2006
picture:
