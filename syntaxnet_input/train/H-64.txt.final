Machine Learning for Information Architecture in a Large 
content:
1 ABSTRACT :
1-1:This paper describes ongoing research into the application of machine learning techniques for improving access to governmental information in complex digital libraries .
1-2:Under the auspices of the GovStat Project, our goal is to identify a small number of semantically valid concepts that adequately spans the intellectual domain of a collection .
1-3:The goal of this discovery is twofold .
1-4:First we desire a practical aid for information architects .
1-5:Second, automatically derived documentconcept relationships are a necessary precondition for realworld deployment of many dynamic interfaces .
1-6:The current study compares concept learning strategies based on three document representations: keywords, titles, and full text .
1-7:In statistical and user based studies, human created keywords provide significant improvements in concept learning over both title only and full text representations .
1-8:H.3.7 [Information Storage and Retrieval]: Digital .
2 INTRODUCTION :
2-1:The GovStat Project is a joint effort of the University of North Carolina Interaction Design Lab and the University of Maryland Human Computer Interaction Lab1 .
2-2:Citing end user difficulty in finding governmental information (especially statistical data) online, the project seeks to create an integrated model of user access to US government statistical information that is rooted in realistic data models and innovative user interfaces .
2-3:To enable such models and interfaces, we propose a data driven approach, based on data mining and machine learning techniques .
2-4:In particular, our work analyzes a particular digital library the website of the Bureau of Labor Statistics2 in efforts to discover a small number of linguistically meaningful concepts, or bins, that collectively summarize the semantic domain of the site .
2-5:The project goal is to classify the site"s web content according to these inferred concepts as an initial step towards data filtering via active user interfaces (cf .
2-6:[13]) .
2-7:Many digital libraries already make use of content classification, both explicitly and implicitly; they divide their resources manually by topical relation; they organize content into hierarchically oriented file systems .
2-8:The goal of the present 1 http: www.ils.unc.edu govstat 2 http: www.bls.gov 151 research is to develop another means of browsing the content of these collections .
2-9:By analyzing the distribution of terms across documents, our goal is to supplement the agency"s pre existing information structures .
2-10:Statistical learning technologies are appealing in this context insofar as they stand to define a data driven as opposed to an agency drivennavigational structure for a site .
2-11:Our approach combines supervised and unsupervised learning techniques .
2-12:A pure document clustering [12] approach to such a large, diverse collection as BLS led to poor results in early tests [6] .
2-13:But strictly supervised techniques [5] are inappropriate, too .
2-14:Although BLS designers have defined high level subject headings for their collections, as we discuss in Section 2, this scheme is less than optimal .
2-15:Thus we hope to learn an additional set of concepts by letting the data speak for themselves .
2-16:The remainder of this paper describes the details of our concept discovery efforts and subsequent evaluation .
2-17:In Section 2 we describe the previously existing, human created conceptual structure of the BLS website .
2-18:This section also describes evidence that this structure leaves room for improvement .
2-19:Next (Sections 3 5), we turn to a description of the concepts derived via content clustering under three document representations: keyword, title only, and full text .
2-20:Section 6 describes a two part evaluation of the derived conceptual structures .
2-21:Finally, we conclude in Section 7 by outlining upcoming work on the project. .
3 STRUCTURING ACCESS TO THE BLS WEBSITE :
3-1:WEBSITE The Bureau of Labor Statistics is a federal government agency charged with compiling and publishing statistics pertaining to labor and production in the US and abroad .
3-2:Given this broad mandate, the BLS publishes a wide array of information, intended for diverse audiences .
3-3:The agency"s website acts as a clearinghouse for this process .
3-4:With over 15,000 text html documents (and many more documents if spreadsheets and typeset reports are included), providing access to the collection provides a steep challenge to information architects .
3-5:2.1 The Relation Browser The starting point of this work is the notion that access to information in the BLS website could be improved by the addition of a dynamic interface such as the relation browser described by Marchionini and Brunk [13] .
3-6:The relation browser allows users to traverse complex data sets by iteratively slicing the data along several topics .
3-7:In Figure 1 we see a prototype instantiation of the relation browser, applied to the FedStats website3 .
3-8:The relation browser supports information seeking by allowing users to form queries in a stepwise fashion, slicing and re slicing the data as their interests dictate .
3-9:Its motivation is in keeping with Shneiderman"s suggestion that queries and their results should be tightly coupled [2] .
3-10:Thus in Fig3 http: www.fedstats.gov Figure 1: Relation Browser Prototype ure 1, users might limit their search set to those documents about energy .
3-11:Within this subset of the collection, they might further eliminate documents published more than a year ago .
3-12:Finally, they might request to see only documents published in PDF format .
3-13:As Marchionini and Brunk discuss, capturing the publication date and format of documents is trivial .
3-14:But successful implementations of the relation browser also rely on topical classification .
3-15:This presents two stumbling blocks for system designers: • Information architects must define the appropriate set of topics for their collection • Site maintainers must classify each document into its appropriate categories These tasks parallel common problems in the metadata community: defining appropriate elements and marking up documents to support metadata aware information access .
3-16:Given a collection of over 15,000 documents, these hurdles are especially daunting, and automatic methods of approaching them are highly desirable .
3-17:2.2 A Pre Existing Structure Prior to our involvement with the project, designers at BLS created a shallow classificatory structure for the most important documents in their website .
3-18:As seen in Figure 2, the BLS home page organizes 65 top level documents into 15 categories .
3-19:These include topics such as Employment and Unemployment, Productivity, and Inflation and Spending .
3-20:152 Figure 2: The BLS Home Page We hoped initially that these pre defined categories could be used to train a 15 way document classifier, thus automating the process of populating the relation browser altogether .
3-21:However, this approach proved unsatisfactory .
3-22:In personal meetings, BLS officials voiced dissatisfaction with the existing topics .
3-23:Their form, it was argued, owed as much to the institutional structure of BLS as it did to the inherent topology of the website"s information space .
3-24:In other words, the topics reflected official divisions rather than semantic clusters .
3-25:The BLS agents suggested that re designing this classification structure would be desirable .
3-26:The agents" misgivings were borne out in subsequent analysis .
3-27:The BLS topics comprise a shallow classificatory structure; each of the 15 top level categories is linked to a small number of related pages .
3-28:Thus there are 7 pages associated with Inflation .
3-29:Altogether, the link structure of this classificatory system contains 65 documents; that is, excluding navigational links, there are 65 documents linked from the BLS home page, where each hyperlink connects a document to a topic (pages can be linked to multiple topics) .
3-30:Based on this hyperlink structure, we defined M, a symmetric 65×65 matrix, where mij counts the number of topics in which documents i and j are both classified on the BLS home page .
3-31:To analyze the redundancy inherent in the pre existing structure, we derived the principal components of M (cf .
3-32:[11]) .
3-33:Figure 3 shows the resultant scree plot4 .
3-34:Because all 65 documents belong to at least one BLS topic, 4 A scree plot shows the magnitude of the kth eigenvalue versus its rank .
3-35:During principal component analysis scree plots visualize the amount of variance captured by each component .
3-36:m00M0M 0 1010M10M 10 2020M20M 20 3030M30M 30 4040M40M 40 5050M50M 50 6060M60M 60 m00M0M 0 22M2M 2 44M4M 4 66M6M 6 88M8M 8 1010M10M 10 1212M12M 12 1414M14M 14 Eigenvalue RankMEigenvalue RankM Eigenvalue Rank Eigenvlue MagnitudeMEigenvlue MagnitudeM EigenvlueMagnitude Figure 3: Scree Plot of BLS Categories the rank of M is guaranteed to be less than or equal to 15 (hence, eigenvalues 16 .
3-37:.
3-38:.
3-39:65 = 0) .
3-40:What is surprising about Figure 3, however, is the precipitous decline in magnitude among the first four eigenvalues .
3-41:The four largest eigenvlaues account for 62.2% of the total variance in the data .
3-42:This fact suggests a high degree of redundancy among the topics .
3-43:Topical redundancy is not in itself problematic .
3-44:However, the documents in this very shallow classificatory structure are almost all gateways to more specific information .
3-45:Thus the listing of the Producer Price Index under three categories could be confusing to the site"s users .
3-46:In light of this potential for confusion and the agency"s own request for redesign, we undertook the task of topic discovery described in the following sections. .
4 A HYBRID APPROACH TO TOPIC DISCOVERY :
4-1:DISCOVERY To aid in the discovery of a new set of high level topics for the BLS website, we turned to unsupervised machine learning methods .
4-2:In efforts to let the data speak for themselves, we desired a means of concept discovery that would be based not on the structure of the agency, but on the content of the material .
4-3:To begin this process, we crawled the BLS website, downloading all documents of MIME type text html .
4-4:This led to a corpus of 15,165 documents .
4-5:Based on this corpus, we hoped to derive k ≈ 10 topical categories, such that each document di is assigned to one or more classes .
4-6:153 Document clustering (cf .
4-7:[16]) provided an obvious, but only partial solution to the problem of automating this type of high level information architecture discovery .
4-8:The problems with standard clustering are threefold .
4-9:identifying the topical content of documents, since documents may be about many subjects .
4-10:BLS collection (tables, lists, surveys, etc.), many documents" terms provide noisy topical information .
4-11:small number (k ≈ 10) of topics .
4-12:Without significant data reduction, term based clustering tends to deliver clusters at too fine a level of granularity .
4-13:In light of these problems, we take a hybrid approach to topic discovery .
4-14:First, we limit the clustering process to a sample of the entire collection, described in Section 4 .
4-15:Working on a focused subset of the data helps to overcome problems two and three, listed above .
4-16:To address the problem of mutual exclusivity, we combine unsupervised with supervised learning methods, as described in Section 5. .
5 FOCUSING ON CONTENT RICH DOCUMENTS :
5-1:DOCUMENTS To derive empirically evidenced topics we initially turned to cluster analysis .
5-2:Let A be the n×p data matrix with n observations in p variables .
5-3:Thus aij shows the measurement for the ith observation on the jth variable .
5-4:As described in [12], the goal of cluster analysis is to assign each of the n observations to one of a small number k groups, each of which is characterized by high intra cluster correlation and low inter cluster correlation .
5-5:Though the algorithms for accomplishing such an arrangement are legion, our analysis focuses on k means clustering5 , during which, each observation oi is assigned to the cluster Ck whose centroid is closest to it, in terms of Euclidean distance .
5-6:Readers interested in the details of the algorithm are referred to [12] for a thorough treatment of the subject .
5-7:Clustering by k means is well studied in the statistical literature, and has shown good results for text analysis (cf .
5-8:[8, 16]) .
5-9:However, k means clustering requires that the researcher specify k, the number of clusters to define .
5-10:When applying k means to our 15,000 document collection, indicators such as the gap statistic [17] and an analysis of the mean squared distance across values of k suggested that k ≈ 80 was optimal .
5-11:This paramterization led to semantically intelligible clusters .
5-12:However, 80 clusters are far too many for application to an interface such as the relation 5 We have focused on k means as opposed to other clustering algorithms for several reasons .
5-13:Chief among these is the computational efficiency enjoyed by the k means approach .
5-14:Because we need only a flat clustering there is little to be gained by the more expensive hierarchical algorithms .
5-15:In future work we will turn to model based clustering [7] as a more principled method of selecting the number of clusters and of representing clusters .
5-16:browser .
5-17:Moreover, the granularity of these clusters was unsuitably fine .
5-18:For instance, the 80 cluster solution derived a cluster whose most highly associated words (in terms of log odds ratio [1]) were drug, pharmacy, and chemist .
5-19:These words are certainly related, but they are related at a level of specificity far below what we sought .
5-20:To remedy the high dimensionality of the data, we resolved to limit the algorithm to a subset of the collection .
5-21:In consultation with employees of the BLS, we continued our analysis on documents that form a series titled From the Editor"s Desk6 .
5-22:These are brief articles, written by BLS employees .
5-23:BLS agents suggested that we focus on the Editor"s Desk because it is intended to span the intellectual domain of the agency .
5-24:The column is published daily, and each entry describes an important current issue in the BLS domain .
5-25:The Editor"s Desk column has been written daily (five times per week) since 1998 .
5-26:As such, we operated on a set of N = 1279 documents .
5-27:Limiting attention to these 1279 documents not only reduced the dimensionality of the problem .
5-28:It also allowed the clustering process to learn on a relatively clean data set .
5-29:While the entire BLS collection contains a great deal of nonprose text (i.e .
5-30:tables, lists, etc.), the Editor"s Desk documents are all written in clear, journalistic prose .
5-31:Each document is highly topical, further aiding the discovery of termtopic relations .
5-32:Finally, the Editor"s Desk column provided an ideal learning environment because it is well supplied with topical metadata .
5-33:Each of the 1279 documents contains a list of one or more keywords .
5-34:Additionally, a subset of the documents (1112) contained a subject heading .
5-35:This metadata informed our learning and evaluation, as described in Section 6.1. .
6 COMBINING SUPERVISED AND UNSUPERVISED LEARNING FORTOPIC :
6-1:UNSUPERVISED LEARNING FORTOPIC DISCOVERY To derive suitably general topics for the application of a dynamic interface to the BLS collection, we combined document clustering with text classification techniques .
6-2:Specifically, using k means, we clustered each of the 1279 documents into one of k clusters, with the number of clusters chosen by analyzing the within cluster mean squared distance at different values of k (see Section 6.1) .
6-3:Constructing mutually exclusive clusters violates our assumption that documents may belong to multiple classes .
6-4:However, these clusters mark only the first step in a two phase process of topic identification .
6-5:At the end of the process, documentcluster affinity is measured by a real valued number .
6-6:Once the Editor"s Desk documents were assigned to clusters, we constructed a k way classifier that estimates the strength of evidence that a new document di is a member of class Ck .
6-7:We tested three statistical classification techniques: probabilistic Rocchio (prind), naive Bayes, and support vector machines (SVMs) .
6-8:All were implemented using McCallum"s BOW text classification library [14] .
6-9:Prind is a probabilistic version of the Rocchio classification algorithm [9] .
6-10:Interested readers are referred to Joachims" article for 6 http: www.bls.gov opub ted 154 further details of the classification method .
6-11:Like prind, naive Bayes attempts to classify documents into the most probable class .
6-12:It is described in detail in [15] .
6-13:Finally, support vector machines were thoroughly explicated by Vapnik [18], and applied specifically to text in [10] .
6-14:They define a decision boundary by finding the maximally separating hyperplane in a high dimensional vector space in which document classes become linearly separable .
6-15:Having clustered the documents and trained a suitable classifier, the remaining 14,000 documents in the collection are labeled by means of automatic classification .
6-16:That is, for each document di we derive a k dimensional vector, quantifying the association between di and each class C1 .
6-17:.
6-18:.
6-19:Ck .
6-20:Deriving topic scores via naive Bayes for the entire 15,000document collection required less than two hours of CPU time .
6-21:The output of this process is a score for every document in the collection on each of the automatically discovered topics .
6-22:These scores may then be used to populate a relation browser interface, or they may be added to a traditional information retrieval system .
6-23:To use these weights in the relation browser we currently assign to each document the two topics on which it scored highest .
6-24:In future work we will adopt a more rigorous method of deriving documenttopic weight thresholds .
6-25:Also, evaluation of the utility of the learned topics for users will be undertaken. .
7-1:DISCOVERY Prior to implementing a relation browser interface and undertaking the attendant user studies, it is of course important to evaluate the quality of the inferred concepts, and the ability of the automatic classifier to assign documents to the appropriate subjects
7-2:To evaluate the success of the two stage approach described in Section 5, we undertook two experiments
7-3:During the first experiment we compared three methods of document representation for the clustering task
7-4:The goal here was to compare the quality of document clusters derived by analysis of full text documents, documents represented only by their titles, and documents represented by human created keyword metadata
7-5:During the second experiment, we analyzed the ability of the statistical classifiers to discern the subject matter of documents from portions of the database in addition to the Editor"s Desk
7-6:6.1 Comparing Document Representations Documents from The Editor"s Desk column came supplied with human generated keyword metadata
7-7:Additionally, The titles of the Editor"s Desk documents tend to be germane to the topic of their respective articles
7-8:With such an array of distilled evidence of each document"s subject matter, we undertook a comparison of document representations for topic discovery by clustering
7-9:We hypothesized that keyword based clustering would provide a useful model
7-10:But we hoped to see whether comparable performance could be attained by methods that did not require extensive human indexing, such as the title only or full text representations
7-11:To test this hypothesis, we defined three modes of document representation full text, title only, and keyword only we generated three sets of topics, Tfull, Ttitle, and Tkw, respectively
7-12:Topics based on full text documents were derived by application of k means clustering to the 1279 Editor"s Desk documents, where each document was represented by a 1908dimensional vector
7-13:These 1908 dimensions captured the TF.IDF weights [3] of each term ti in document dj, for all terms that occurred at least three times in the data
7-14:To arrive at the appropriate number of clusters for these data, we inspected the within cluster mean squared distance for each value of k = 1
7-15:
7-16:
7-17:20
7-18:As k approached 10 the reduction in error with the addition of more clusters declined notably, suggesting that k ≈ 10 would yield good divisions
7-19:To select a single integer value, we calculated which value of k led to the least variation in cluster size
7-20:This metric stemmed from a desire to suppress the common result where one large cluster emerges from the k means algorithm, accompanied by several accordingly small clusters
7-21:Without reason to believe that any single topic should have dramatically high prior odds of document membership, this heuristic led to kfull = 10
7-22:Clusters based on document titles were constructed similarly
7-23:However, in this case, each document was represented in the vector space spanned by the 397 terms that occur at least twice in document titles
7-24:Using the same method of minimizing the variance in cluster membership ktitle the number of clusters in the title based representation was also set to 10
7-25:The dimensionality of the keyword based clustering was very similar to that of the title based approach
7-26:There were 299 keywords in the data, all of which were retained
7-27:The median number of keywords per document was 7, where a keyword is understood to be either a single word, or a multiword term such as consumer price index
7-28:It is worth noting that the keywords were not drawn from any controlled vocabulary; they were assigned to documents by publishers at the BLS
7-29:Using the keywords, the documents were clustered into 10 classes
7-30:To evaluate the clusters derived by each method of document representation, we used the subject headings that were included with 1112 of the Editor"s Desk documents
7-31:Each of these 1112 documents was assigned one or more subject headings, which were withheld from all of the cluster applications
7-32:Like the keywords, subject headings were assigned to documents by BLS publishers
7-33:Unlike the keywords, however, subject headings were drawn from a controlled vocabulary
7-34:Our analysis began with the assumption that documents with the same subject headings should cluster together
7-35:To facilitate this analysis, we took a conservative approach; we considered multi subject classifications to be unique
7-36:Thus if document di was assigned to a single subject prices, while document dj was assigned to two subjects, international comparisons, prices, documents di and dj are not considered to come from the same class
7-37:Table 1 shows all Editor"s Desk subject headings that were assigned to at least 10 documents
7-38:As noted in the table, 155 Table 1: Top Editor"s Desk Subject Headings Subject Count prices 92 unemployment 55 occupational safety & health 53 international comparisons, prices 48 manufacturing, prices 45 employment 44 productivity 40 consumer expenditures 36 earnings & wages 27 employment & unemployment 27 compensation costs 25 earnings & wages, metro
7-39:areas 18 benefits, compensation costs 18 earnings & wages, occupations 17 employment, occupations 14 benefits 14 earnings & wage, regions 13 work stoppages 12 earnings & wages, industries 11 Total 609 Table 2: Contingecy Table for Three Document Representations Representation Right Wrong Accuracy Full text 392 217 0.64 Title 441 168 0.72 Keyword 601 8 0.98 there were 19 such subject headings, which altogether covered 609 (54%) of the documents with subjects assigned
7-40:These document subject pairings formed the basis of our analysis
7-41:Limiting analysis to subjects with N > 10 kept the resultant χ2 tests suitably robust
7-42:The clustering derived by each document representation was tested by its ability to collocate documents with the same subjects
7-43:Thus for each of the 19 subject headings in Table 1, Si, we calculated the proportion of documents assigned to Si that each clustering co classified
7-44:Further, we assumed that whichever cluster captured the majority of documents for a given class constituted the right answer for that class
7-45:For instance, There were 92 documents whose subject heading was prices
7-46:Taking the BLS editors" classifications as ground truth, all 92 of these documents should have ended up in the same cluster
7-47:Under the full text representation 52 of these documents were clustered into category 5, while 35 were in category 3, and 5 documents were in category 6
7-48:Taking the majority cluster as the putative right home for these documents, we consider the accuracy of this clustering on this subject to be 52 92 = 0.56
7-49:Repeating this process for each topic across all three representations led to the contingency table shown in Table 2
7-50:The obvious superiority of the keyword based clustering evidenced by Table 2 was borne out by a χ2 test on the accuracy proportions
7-51:Comparing the proportion right and Table 3: Keyword Based Clusters benefits costs international jobs plans compensation import employment benefits costs prices jobs employees benefits petroleum youth occupations prices productivity safety workers prices productivity safety earnings index output health operators inflation nonfarm occupational spending unemployment expenditures unemployment consumer mass spending jobless wrong achieved by keyword and title based clustering led to p 0.001
7-52:Due to this result, in the remainder of this paper, we focus our attention on the clusters derived by analysis of the Editor"s Desk keywords
7-53:The ten keyword based clusters are shown in Table 3, represented by the three terms most highly associated with each cluster, in terms of the log odds ratio
7-54:Additionally, each cluster has been given a label by the researchers
7-55:Evaluating the results of clustering is notoriously difficult
7-56:In order to lend our analysis suitable rigor and utility, we made several simplifying assumptions
7-57:Most problematic is the fact that we have assumed that each document belongs in only a single category
7-58:This assumption is certainly false
7-59:However, by taking an extremely rigid view of what constitutes a subject that is, by taking a fully qualified and often multipart subject heading as our unit of analysis we mitigate this problem
7-60:Analogically, this is akin to considering the location of books on a library shelf
7-61:Although a given book may cover many subjects, a classification system should be able to collocate books that are extremely similar, say books about occupational safety and health
7-62:The most serious liability with this evaluation, then, is the fact that we have compressed multiple subject headings, say prices : international into single subjects
7-63:This flattening obscures the multivalence of documents
7-64:We turn to a more realistic assessment of document class relations in Section 6.2
7-65:6.2 Accuracy of the Document Classifiers Although the keyword based clusters appear to classify the Editor"s Desk documents very well, their discovery only solved half of the problem required for the successful implementation of a dynamic user interface such as the relation browser
7-66:The matter of roughly fourteen thousand unclassified documents remained to be addressed
7-67:To solve this problem, we trained the statistical classifiers described above in Section 5
7-68:For each document in the collection di, these classifiers give pi, a k vector of probabilities or distances (depending on the classification method used), where pik quantifies the strength of association between the ith document and the kth class
7-69:All classifiers were trained on the full text of each document, regardless of the representation used to discover the initial clusters
7-70:The different training sets were thus constructed simply by changing the 156 Table 4: Cross Validation Results for 3 Classifiers Method Av
7-71:Percent Accuracy SE Prind 59.07 1.07 Naive Bayes 75.57 0.4 SVM 75.08 0.68 class variable for each instance (document) to reflect its assigned cluster under a given model
7-72:To test the ability of each classifier to locate documents correctly, we first performed a 10 fold cross validation on the Editor"s Desk documents
7-73:During cross validation the data are split randomly into n subsets (in this case n = 10)
7-74:The process proceeds by iteratively holding out each of the n subsets as a test collection for a model trained on the remaining n − 1 subsets
7-75:Cross validation is described in [15]
7-76:Using this methodology, we compared the performance of the three classification models described above
7-77:Table 4 gives the results from cross validation
7-78:Although naive Bayes is not significantly more accurate for these data than the SVM classifier, we limit the remainder of our attention to analysis of its performance
7-79:Our selection of naive Bayes is due to the fact that it appears to work comparably to the SVM approach for these data, while being much simpler, both in theory and implementation
7-80:Because we have only 1279 documents and 10 classes, the number of training documents per class is relatively small
7-81:In addition to models fitted to the Editor"s Desk data, then, we constructed a fourth model, supplementing the training sets of each class by querying the Google search engine7 and applying naive Bayes to the augmented test set
7-82:For each class, we created a query by submitting the three terms with the highest log odds ratio with that class
7-83:Further, each query was limited to the domain www.bls.gov
7-84:For each class we retrieved up to 400 documents from Google (the actual number varied depending on the size of the result set returned by Google)
7-85:This led to a training set of 4113 documents in the augmented model, as we call it below8
7-86:Cross validation suggested that the augmented model decreased classification accuracy (accuracy= 58.16%, with standard error= 0.32)
7-87:As we discuss below, however, augmenting the training set appeared to help generalization during our second experiment
7-88:The results of our cross validation experiment are encouraging
7-89:However, the success of our classifiers on the Editor"s Desk documents that informed the cross validation study may not be good predictors of the models" performance on the remainder to the BLS website
7-90:To test the generality of the naive Bayes classifier, we solicited input from 11 human judges who were familiar with the BLS website
7-91:The sample was chosen by convenience, and consisted of faculty and graduate students who work on the GovStat project
7-92:However, none of the reviewers had prior knowledge of the outcome of the classification before their participation
7-93:For the experiment, a random sample of 100 documents was drawn from the entire BLS collection
7-94:On average each re7 http:  www.google.com 8 A more formal treatment of the combination of labeled and unlabeled data is available in [4]
7-95:Table 5: Human Model Agreement on 100 Sample Docs
7-96:Human Judge 1st Choice Model Model 1st Choice Model 2nd Choice Many developers and maintainers of digital libraries share the basic problem pursued here
7-97:Given increasingly large, complex bodies of data, how may we improve access to collections without incurring extraordinary cost, and while also keeping systems receptive to changes in content over time? Data mining and machine learning methods hold a great deal of promise with respect to this problem
7-98:Empirical methods of knowledge discovery can aid in the organization and retrieval of information
7-99:As we have argued in this paper, these methods may also be brought to bear on the design and implementation of advanced user interfaces
7-100:This study explored a hybrid technique for aiding information architects as they implement dynamic interfaces such as the relation browser
7-101:Our approach combines unsupervised learning techniques, applied to a focused subset of the BLS website
7-102:The goal of this initial stage is to discover the most basic and far reaching topics in the collection
7-103:Based mjobsjobsMjobsM jobs benefitsunemploymentpricespricesMpricesM prices safetyinternationalspendingspendingMspendingM spending occupationscostscostsMcostsM costs productivityHuman ClassificationsMHuman ClassificationsM Human Classifications m0.000.00M0.00M 0.00 0.050.100.150.15M0.15M 0.15 0.200.25mjobsjobsMjobsM jobs benefitsunemploymentpricespricesMpricesM prices safetyinternationalspendingspendingMspendingM spending occupationscostscostsMcostsM costs productivityMachine ClassificationsMMachine ClassificationsM Machine Classifications m0.000.00M0.00M 0.00 0.050.100.10M0.10M 0.10 0.15 Figure 5: Distribution of Classes Across Documents on a statistical model of these topics, the second phase of our approach uses supervised learning (in particular, a naive Bayes classifier, trained on individual words), to assign topical relations to the remaining documents in the collection
7-104:In the study reported here, this approach has demonstrated promise
7-105:In its favor, our approach is highly scalable
7-106:It also appears to give fairly good results
7-107:Comparing three modes of document representation full text, title only, and keyword we found 98% accuracy as measured by collocation of documents with identical subject headings
7-108:While it is not surprising that editor generated keywords should give strong evidence for such learning, their superiority over fulltext and titles was dramatic, suggesting that even a small amount of metadata can be very useful for data mining
7-109:However, we also found evidence that learning topics from a subset of the collection may lead to overfitted models
7-110:After clustering 1279 Editor"s Desk documents into 10 categories, we fitted a 10 way naive Bayes classifier to categorize the remaining 14,000 documents in the collection
7-111:While we saw fairly good results (classification accuracy of 75% with respect to a small sample of human judges), this experiment forced us to reconsider the quality of the topics learned by clustering
7-112:The high correlation among human judgments in our sample suggests that the topics discovered by analysis of the Editor"s Desk were not independent
7-113:While we do not desire mutually exclusive categories in our setting, we do desire independence among the topics we model
7-114:Overall, then, the techniques described here provide an encouraging start to our work on acquiring subject metadata for dynamic interfaces automatically
7-115:It also suggests that a more sophisticated modeling approach might yield 158 better results in the future
7-116:In upcoming work we will experiment with streamlining the two phase technique described here
7-117:Instead of clustering documents to find topics and then fitting a model to the learned clusters, our goal is to expand the unsupervised portion of our analysis beyond a narrow subset of the collection, such as The Editor"s Desk
7-118:In current work we have defined algorithms to identify documents likely to help the topic discovery task
7-119:Supplied with a more comprehensive training set, we hope to experiment with model based clustering, which combines the clustering and classification processes into a single modeling procedure
7-120:Topic discovery and document classification have long been recognized as fundamental problems in information retrieval and other forms of text mining
7-121:What is increasingly clear, however, as digital libraries grow in scope and complexity, is the applicability of these techniques to problems at the front end of systems such as information architecture and interface design
7-122:Finally, then, in future work we will build on the user studies undertaken by Marchionini and Brunk in efforts to evaluate the utility of automatically populated dynamic interfaces for the users of digital libraries.
8-1:A
8-2:Agresti
8-3:An Introduction to Categorical Data Analysis
8-4:Wiley, New York, 1996
8-5:C
8-6:Ahlberg, C
8-7:Williamson, and B
8-8:Shneiderman
8-9:Dynamic queries for information exploration: an implementation and evaluation
8-10:In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 619 626, 1992
8-11:R
8-12:Baeza Yates and B
8-13:Ribeiro Neto
8-14:Modern Information Retrieval
8-15:ACM Press, 1999
8-16:A
8-17:Blum and T
8-18:Mitchell
8-19:Combining labeled and unlabeled data with co training
8-20:In Proceedings of the eleventh annual conference on Computational learning theory, pages 92 100
8-21:ACM Press, 1998
8-22:H
8-23:Chen and S
8-24:Dumais
8-25:Hierarchical classification of web content
8-26:In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 256 263, 2000
8-27:M
8-28:Efron, G
8-29:Marchionini, and J
8-30:Zhang
8-31:Implications of the recursive representation problem for automatic concept identification in on line governmental information
8-32:In Proceedings of the ASIST Special Interest Group on Classification Research (ASIST SIG CR), 2003
8-33:C
8-34:Fraley and A
8-35:E
8-36:Raftery
8-37:How many clusters? which clustering method? answers via model based cluster analysis
8-38:The Computer Journal, 41(8):578 588, 1998
8-39:A
8-40:K
8-41:Jain, M
8-42:N
8-43:Murty, and P
8-44:J
8-45:Flynn
8-46:Data clustering: a review
8-47:ACM Computing Surveys, 31(3):264 323, September 1999
8-48:T
8-49:Joachims
8-50:A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization
8-51:In D
8-52:H
8-53:Fisher, editor, Proceedings of ICML 97, 14th International Conference on Machine Learning, pages 143 151, Nashville, US, 1997
8-54:Morgan Kaufmann Publishers, San Francisco, US
8-55:T
8-56:Joachims
8-57:Text categorization with support vector machines: learning with many relevant features
8-58:In C
8-59:N´edellec and C
8-60:Rouveirol, editors, Proceedings of ECML 98, 10th European Conference on Machine Learning, pages 137 142, Chemnitz, DE, 1998
8-61:Springer Verlag, Heidelberg, DE
8-62:I
8-63:T
8-64:Jolliffe
8-65:Principal Component Analysis
8-66:Springer, 2nd edition, 2002
8-67:L
8-68:Kaufman and P
8-69:J
8-70:Rosseeuw
8-71:Finding Groups in Data: an Introduction to Cluster Analysis
8-72:Wiley, 1990
8-73:G
8-74:Marchionini and B
8-75:Brunk
8-76:Toward a general relation browser: a GUI for information architects
8-77:Journal of Digital Information, 4(1), 2003
8-78:http:  jodi.ecs.soton.ac.uk Articles v04 i01 Marchionini
8-79:A
8-80:K
8-81:McCallum
8-82:Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering
8-83:http:  www.cs.cmu.edu ˜mccallum bow, 1996
8-84:T
8-85:Mitchell
8-86:Machine Learning
8-87:McGraw Hill, 1997
8-88:E
8-89:Rasmussen
8-90:Clustering algorithms
8-91:In W
8-92:B
8-93:Frakes and R
8-94:Baeza Yates, editors, Information Retrieval: Data Structures and Algorithms, pages 419 442
8-95:Prentice Hall, 1992
8-96:R
8-97:Tibshirani, G
8-98:Walther, and T
8-99:Hastie
8-100:Estimating the number of clusters in a dataset via the gap statistic, 2000
8-101:http:  citeseer.nj.nec.com tibshirani00estimating.html
8-102:V
8-103:N
8-104:Vapnik
8-105:The Nature of Statistical Learning Theory
8-106:Springer, 2000
8-107:159
picture:
