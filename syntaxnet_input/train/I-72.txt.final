Learning Consumer Preferences Using Semantic 
content:
1 ABSTRACT :
1-1:In online, dynamic environments, the services requested by consumers may not be readily served by the providers .
1-2:This requires the service consumers and providers to negotiate their service needs and offers .
1-3:Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price .
1-4:In contrast, this work develops an approach through which the parties can negotiate the content of a service .
1-5:This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each other"s preferences incrementally over time .
1-6:Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service .
1-7:Through repetitive interactions, the provider learns consumers" needs accurately and can make better targeted offers .
1-8:To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques .
1-9:We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics .
1-10:I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems .
2 INTRODUCTION :
2-1:Current approaches to e commerce treat service price as the primary construct for negotiation by assuming that the service content is fixed [9] .
2-2:However, negotiation on price presupposes that other properties of the service have already been agreed upon .
2-3:Nevertheless, many times the service provider may not be offering the exact requested service due to lack of resources, constraints in its business policy, and so on [3] .
2-4:When this is the case, the producer and the consumer need to negotiate the content of the requested service [15] .
2-5:However, most existing negotiation approaches assume that all features of a service are equally important and concentrate on the price [5, 2] .
2-6:However, in reality not all features may be relevant and the relevance of a feature may vary from consumer to consumer .
2-7:For instance, completion time of a service may be important for one consumer whereas the quality of the service may be more important for a second consumer .
2-8:Without doubt, considering the preferences of the consumer has a positive impact on the negotiation process .
2-9:For this purpose, evaluation of the service components with different weights can be useful .
2-10:Some studies take these weights as a priori and uses the fixed weights [4] .
2-11:On the other hand, mostly the producer does not know the consumer"s preferences before the negotiation .
2-12:Hence, it is more appropriate for the producer to learn these preferences for each consumer .
2-13:Preference Learning: As an alternative, we propose an architecture in which the service providers learn the relevant features of a service for a particular customer over time .
2-14:We represent service requests as a vector of service features .
2-15:We use an ontology in order to capture the relations between services and to construct the features for a given service .
2-16:By using a common ontology, we enable the consumers and producers to share a common vocabulary for negotiation .
2-17:The particular service we have used is a wine selling service .
2-18:The wine seller learns the wine preferences of the customer to sell better targeted wines .
2-19:The producer models the requests of the consumer and its counter offers to learn which features are more important for the consumer .
2-20:Since no information is present before the interactions start, the learning algorithm has to be incremental so that it can be trained at run time and can revise itself with each new interaction .
2-21:Service Generation: Even after the producer learns the important features for a consumer, it needs a method to generate offers that are the most relevant for the consumer among its set of possible services .
2-22:In other words, the question is how the producer uses the information that was learned from the dialogues to make the best offer to the consumer .
2-23:For instance, assume that the producer has learned that the consumer wants to buy a red wine but the producer can only offer rose or white wine .
2-24:What should the producer"s offer 1301 978 81 904262 7 5 c 2007 IFAAMAS contain; white wine or rose wine? If the producer has some domain knowledge about semantic similarity (e.g., knows that the red and rose wines are taste wise more similar than white wine), then it can generate better offers .
2-25:However, in addition to domain knowledge, this derivation requires appropriate metrics to measure similarity between available services and learned preferences .
2-26:The rest of this paper is organized as follows: Section 2 explains our proposed architecture .
2-27:Section 3 explains the learning algorithms that were studied to learn consumer preferences .
2-28:Section 4 studies the different service offering mechanisms .
2-29:Section 5 contains the similarity metrics used in the experiments .
2-30:The details of the developed system is analyzed in Section 6 .
2-31:Section 7 provides our experimental setup, test cases, and results .
2-32:Finally, Section 8 discusses and compares our work with other related work. .
3 ARCHITECTURE :
3-1:Our main components are consumer and producer agents, which communicate with each other to perform content oriented negotiation .
3-2:Figure 1 depicts our architecture .
3-3:The consumer agent represents the customer and hence has access to the preferences of the customer .
3-4:The consumer agent generates requests in accordance with these preferences and negotiates with the producer based on these preferences .
3-5:Similarly, the producer agent has access to the producer"s inventory and knows which wines are available or not .
3-6:A shared ontology provides the necessary vocabulary and hence enables a common language for agents .
3-7:This ontology describes the content of the service .
3-8:Further, since an ontology can represent concepts, their properties and their relationships semantically, the agents can reason the details of the service that is being negotiated .
3-9:Since a service can be anything such as selling a car, reserving a hotel room, and so on, the architecture is independent of the ontology used .
3-10:However, to make our discussion concrete, we use the well known Wine ontology [19] with some modification to illustrate our ideas and to test our system .
3-11:The wine ontology describes different types of wine and includes features such as color, body, winery of the wine and so on .
3-12:With this ontology, the service that is being negotiated between the consumer and the producer is that of selling wine .
3-13:The data repository in Figure 1 is used solely by the producer agent and holds the inventory information of the producer .
3-14:The data repository includes information on the products the producer owns, the number of the products and ratings of those products .
3-15:Ratings indicate the popularity of the products among customers .
3-16:Those are used to decide which product will be offered when there exists more than one product having same similarity to the request of the consumer agent .
3-17:The negotiation takes place in a turn taking fashion, where the consumer agent starts the negotiation with a particular service request .
3-18:The request is composed of significant features of the service .
3-19:In the wine example, these features include color, winery and so on .
3-20:This is the particular wine that the customer is interested in purchasing .
3-21:If the producer has the requested wine in its inventory, the producer offers the wine and the negotiation ends .
3-22:Otherwise, the producer offers an alternative wine from the inventory .
3-23:When the consumer receives a counter offer from the producer, it will evaluate it .
3-24:If it is acceptable, then the negotiation will end .
3-25:Otherwise, the customer will generate a new request or stick to the previous request .
3-26:This process will continue until some service is accepted by the consumer agent or all possible offers are put forward to the consumer by the producer .
3-27:One of the crucial challenges of the content oriented negotiation is the automatic generation of counter offers by the service producer .
3-28:When the producer constructs its offer, it should consider Figure 1: Proposed Negotiation Architecture three important things: the current request, consumer preferences and the producer"s available services .
3-29:Both the consumer"s current request and the producer"s own available services are accessible by the producer .
3-30:However, the consumer"s preferences in most cases will not be available .
3-31:Hence, the producer will have to understand the needs of the consumer from their interactions and generate a counter offer that is likely to be accepted by the consumer .
3-32:This challenge can be studied in three stages: • Preference Learning: How can the producers learn about each customer"s preferences based on requests and counter offers? (Section 3) • Service Offering: How can the producers revise their offers based on the consumer"s preferences that they have learned so far? (Section 4) • Similarity Estimation: How can the producer agent estimate similarity between the request and available services? (Section 5) .
4 PREFERENCE LEARNING :
4-1:The requests of the consumer and the counter offers of the producer are represented as vectors, where each element in the vector corresponds to the value of a feature .
4-2:The requests of the consumers represent individual wine products whereas their preferences are constraints over service features .
4-3:For example, a consumer may have preference for red wine .
4-4:This means that the consumer is willing to accept any wine offered by the producers as long as the color is red .
4-5:Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g .
4-6:(Medium, Strong, Red) .
4-7:At the beginning of negotiation, the producer agent does not know the consumer"s preferences but will need to learn them using information obtained from the dialogues between the producer and the consumer .
4-8:The preferences denote the relative importance of the features of the services demanded by the consumer agents .
4-9:For instance, the color of the wine may be important so the consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl .
4-10:Joint Conf .
4-11:on Autonomous Agents and Multi Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)} {{(? Full), ?, ? }, (Full,Delicate,Rose) {?, (? Delicate), ?}, {(Full,Strong,White)} {?, ?, (? Rose)}} {{(? Full), ?, ?}, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(? Delicate), ?}, {(Medium,Moderate,Red)}} {?, ?, (? Rose)}} the offers involving the wine whose color is white or rose .
4-12:On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red .
4-13:To tackle this problem, we propose to use incremental learning algorithms [6] .
4-14:This is necessary since no training data is available before the interactions start .
4-15:We particularly investigate two approaches .
4-16:The first one is inductive learning .
4-17:This technique is applied to learn the preferences as concepts .
4-18:We elaborate on Candidate Elimination Algorithm for Version Space [10] .
4-19:CEA is known to perform poorly if the information to be learned is disjunctive .
4-20:Interestingly, most of the time consumer preferences are disjunctive .
4-21:Say, we are considering an agent that is buying wine .
4-22:The consumer may prefer red wine or rose wine but not white wine .
4-23:To use CEA with such preferences, a solid modification is necessary .
4-24:The second approach is decision trees .
4-25:Decision trees can learn from examples easily and classify new instances as positive or negative .
4-26:A well known incremental decision tree is ID5R [18] .
4-27:However, ID5R is known to suffer from high computational complexity .
4-28:For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning .
4-29:3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples .
4-30:The algorithm maintains two sets to model the concept to be learned .
4-31:The first set is the most general set G .
4-32:G contains hypotheses about all the possible values that the concept may obtain .
4-33:As the name suggests, it is a generalization and contains all possible values unless the values have been identified not to represent the concept .
4-34:The second set is the most specific set S .
4-35:S contains only hypotheses that are known to identify the concept that is being learned .
4-36:At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty .
4-37:During the interactions, each request of the consumer can be considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example .
4-38:At each interaction between the producer and the consumer, both G and S are modified .
4-39:The negative samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as positive .
4-40:When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance .
4-41:As a result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones .
4-42:Incrementally, G specializes and S generalizes until G and S are equal to each other .
4-43:When these sets are equal, the algorithm converges by means of reaching the target concept .
4-44:3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive concepts .
4-45:On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several alternative wishes .
4-46:There are several studies on learning disjunctive concepts via Version Space .
4-47:Some of these approaches use multiple version space .
4-48:For instance, Hong et al .
4-49:maintain several version spaces by split and merge operation [7] .
4-50:To be able to learn disjunctive concepts, they create new version spaces by examining the consistency between G and S .
4-51:We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include disjunctive hypothesis in addition to the conjunctives and negation .
4-52:Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature .
4-53:EXAMPLE 1 .
4-54:Assume that the most specific set is {(Light, Delicate, Red)} and a positive example, (Light, Delicate, White) comes .
4-55:The original CEA will generalize this as (Light, Delicate, ?), meaning the color can take any value .
4-56:However, in fact, we only know that the color can be red or white .
4-57:In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )} .
4-58:Only when all the values exist in the list, they will be replaced by ? .
4-59:In other words, we let the algorithm generalize more slowly than before .
4-60:We modify the CEA algorithm to deal with this change .
4-61:The modified algorithm, DCEA, is given as Algorithm 1 .
4-62:Note that compared to the previous studies of disjunctive versions, our approach uses only a single version space rather than multiple version space .
4-63:The initialization phase is the same as the original algorithm (lines 1, 2) .
4-64:If any positive sample comes, we add the sample to the special set as before (line 4) .
4-65:However, we do not eliminate the hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other .
4-66:Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it .
4-67:After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7) .
4-68:When a negative sample comes, we do not change S as before .
4-69:We only modify the most general hypotheses not to cover this negative sample (lines 11 15) .
4-70:Different from the original CEA, we try to specialize the G minimally .
4-71:The algorithm removes the hypothesis covering the negative sample (line 13) .
4-72:Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis .
4-73:For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis .
4-74:Thus, all possible hypotheses that do not cover the negative sample are generated (line 14) .
4-75:Note that, exclusive list contains the values that the attribute cannot take .
4-76:For example, consider the color attribute .
4-77:If a hypothesis includes red in its exclusive list and ? in its inclusive list, this means that color may take any value except red .
4-78:The Sixth Intl .
4-79:Joint Conf .
4-80:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: Remove g from G 14: Add hypotheses g1, g2, gn where g1= (x1 d1, x2,..., xn), g2= (x1, x2 d2,..., xn),..., and gn= (x1, x2,..., xn dn) 15: Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2 .
4-81:Table 1 illustrates the first three interactions and the workings of DCEA .
4-82:The most general set and the most specific set show the contents of G and S after the sample comes in .
4-83:After the first positive sample, S is generalized to also cover the instance .
4-84:The second sample is negative .
4-85:Thus, we replace (?, ?, ?) by three disjunctive hypotheses; each hypothesis being minimally specialized .
4-86:In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set .
4-87:The third sample is positive and generalizes S even more .
4-88:Note that in Table 1, we do not eliminate {(? Full), ?, ?} from the general set while having a positive sample such as (Full, Strong, White) .
4-89:This stems from the possibility of using this rule in the generation of other hypotheses .
4-90:For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(? Full), ?, (? Red)} .
4-91:By Algorithm 1, we do not miss any information .
4-92:3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a topdown fashion from the observed examples represented in a vector with attribute value pairs .
4-93:Applying this algorithm to our system with the intention of learning the consumer"s preferences is appropriate since this algorithm also supports learning disjunctive concepts in addition to conjunctive concepts .
4-94:The ID3 algorithm is used in the learning process with the purpose of classification of offers .
4-95:There are two classes: positive and negative .
4-96:Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer .
4-97:Consumer"s requests are considered as positive training examples and all rejected counter offers are thought as negative ones .
4-98:The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non leaf nodes in which test attributes are held .
4-99:The test attribute in a non leaf node is one of the attributes making up the service description .
4-100:For instance, body, flavor, color and so on are potential test attributes for wine service .
4-101:When we want to find whether the given service description is acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node .
4-102:The problem with this algorithm is that it is not an incremental algorithm, which means all the training examples should exist before learning .
4-103:To overcome this problem, the system keeps consumer"s requests throughout the negotiation interaction as positive examples and all counter offers rejected by the consumer as negative examples .
4-104:After each coming request, the decision tree is rebuilt .
4-105:Without doubt, there is a drawback of reconstruction such as additional process load .
4-106:However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. .
5 SERVICE OFFERING :
5-1:After learning the consumer"s preferences, the producer needs to make a counter offer that is compatible with the consumer"s preferences .
5-2:4.1 Service Offering via CEA and DCEA To generate the best offer, the producer agent uses its service ontology and the CEA algorithm .
5-3:The service offering mechanism is the same for both the original CEA and DCEA, but as explained before their methods for updating G and S are different .
5-4:When producer receives a request from the consumer, the learning set of the producer is trained with this request as a positive sample .
5-5:The learning components, the most specific set S and the most general set G are actively used in offering service .
5-6:The most general set, G is used by the producer in order to avoid offering the services, which will be rejected by the consumer agent .
5-7:In other words, it filters the service set from the undesired services, since G contains hypotheses that are consistent with the requests of the consumer .
5-8:The most specific set, S is used in order to find best offer, which is similar to the consumer"s preferences .
5-9:Since the most specific set S holds the previous requests and the current request, estimating similarity between this set and every service in the service list is very convenient to find the best offer from the service list .
5-10:When the consumer starts the interaction with the producer agent, producer agent loads all related services to the service list object .
5-11:This list constitutes the provider"s inventory of services .
5-12:Upon receiving a request, if the producer can offer an exactly matching service, then it does so .
5-13:For example, for a wine this corresponds to selling a wine that matches the specified features of the consumer"s request identically .
5-14:When the producer cannot offer the service as requested, it tries to find the service that is most similar to the services that have been requested by the consumer during the negotiation .
5-15:To do this, the producer has to compute the similarity between the services it can offer and the services that have been requested (in S) .
5-16:We compute the similarities in various ways as will be explained in Section 5 .
5-17:After the similarity of the available services with the current S is calculated, there may be more than one service with the maximum similarity .
5-18:The producer agent can break the tie in a number of ways .
5-19:Here, we have associated a rating value with each service and the producer prefers the higher rated service to others .
5-20:4.2 Service Offering via ID3 If the producer learns the consumer"s preferences with ID3, a similar mechanism is applied with two differences .
5-21:First, since ID3 does not maintain G, the list of unaccepted services that are classified as negative are removed from the service list .
5-22:Second, the similarities of possible services are not measured with respect to S, but instead to all previously made requests .
5-23:4.3 Alternative Service Offering Mechanisms In addition to these three service offering mechanisms (Service Offering with CEA, Service Offering with DCEA, and Service Offering with ID3), we include two other mechanisms. .
5-24:1304 The Sixth Intl .
5-25:Joint Conf .
5-26:on Autonomous Agents and Multi Agent Systems (AAMAS 07) • Random Service Offering (RO): The producer generates a counter offer randomly from the available service list, without considering the consumer"s preferences .
5-27:• Service Offering considering only the current request (SCR): The producer selects a counter offer according to the similarity of the consumer"s current request but does not consider previous requests. .
6 SIMILARITY ESTIMATION :
6-1:Similarity can be estimated with a similarity metric that takes two entries and returns how similar they are .
6-2:There are several similarity metrics used in case based reasoning system such as weighted sum of Euclidean distance, Hamming distance and so on [12] .
6-3:The similarity metric affects the performance of the system while deciding which service is the closest to the consumer"s request .
6-4:We first analyze some existing metrics and then propose a new semantic similarity metric named RP Similarity .
6-5:5.1 Tversky"s Similarity Metric Tversky"s similarity metric compares two vectors in terms of the number of exactly matching features [17] .
6-6:In Equation (1), common represents the number of matched attributes whereas different represents the number of the different attributes .
6-7:Our current assumption is that α and β is equal to each other .
6-8:SMpq = α(common) α(common) + β(different) (1) Here, when two features are compared, we assign zero for dissimilarity and one for similarity by omitting the semantic closeness among the feature values .
6-9:Tversky"s similarity metric is designed to compare two feature vectors .
6-10:In our system, whereas the list of services that can be offered by the producer are each a feature vector, the most specific set S is not a feature vector .
6-11:S consists of hypotheses of feature vectors .
6-12:Therefore, we estimate the similarity of each hypothesis inside the most specific set S and then take the average of the similarities .
6-13:EXAMPLE 3 .
6-14:Assume that S contains the following two hypothesis: { {Light, Moderate, (Red, White)} , {Full, Strong, Rose}} .
6-15:Take service s as (Light, Strong, Rose) .
6-16:Then the similarity of the first one is equal to 1 3 and the second one is equal to 2 3 in accordance with Equation (1) .
6-17:Normally, we take the average of it and obtain (1 3 + 2 3) 2, equally 1 2 .
6-18:However, the first hypothesis involves the effect of two requests and the second hypothesis involves only one request .
6-19:As a result, we expect the effect of the first hypothesis to be greater than that of the second .
6-20:Therefore, we calculate the average similarity by considering the number of samples that hypotheses cover .
6-21:Let ch denote the number of samples that hypothesis h covers and (SM(h,service)) denote the similarity of hypothesis h with the given service .
6-22:We compute the similarity of each hypothesis with the given service and weight them with the number of samples they cover .
6-23:We find the similarity by dividing the weighted sum of the similarities of all hypotheses in S with the service by the number of all samples that are covered in S .
6-24:AV G−SM(service,S) = |S| |h| (ch ∗ SM(h,service)) |S| |h| ch (2) Figure 2: Sample taxonomy for similarity estimation EXAMPLE 4 .
6-25:For the above example, the similarity of (Light, Strong, Rose) with the specific set is (2 ∗ 1 3 + 2 3) 3, equally 4 9 .
6-26:The possible number of samples that a hypothesis covers can be estimated with multiplying cardinalities of each attribute .
6-27:For example, the cardinality of the first attribute is two and the others is equal to one for the given hypothesis such as {Light, Moderate, (Red, White)} .
6-28:When we multiply them, we obtain two (2 ∗ 1 ∗ 1 = 2) .
6-29:5.2 Lin"s Similarity Metric A taxonomy can be used while estimating semantic similarity between two concepts .
6-30:Estimating semantic similarity in a Is A taxonomy can be done by calculating the distance between the nodes related to the compared concepts .
6-31:The links among the nodes can be considered as distances .
6-32:Then, the length of the path between the nodes indicates how closely similar the concepts are .
6-33:An alternative estimation to use information content in estimation of semantic similarity rather than edge counting method, was proposed by Lin [8] .
6-34:The equation (3) [8] shows Lin"s similarity where c1 and c2 are the compared concepts and c0 is the most specific concept that subsumes both of them .
6-35:Besides, P(C) represents the probability of an arbitrary selected object belongs to concept C .
6-36:Similarity(c1, c2) = 2 × log P(c0) log P(c1) + log P(c2) (3) 5.3 Wu & Palmer"s Similarity Metric Different from Lin, Wu and Palmer use the distance between the nodes in IS A taxonomy [20] .
6-37:The semantic similarity is represented with Equation (4) [20] .
6-38:Here, the similarity between c1 and c2 is estimated and c0 is the most specific concept subsuming these classes .
6-39:N1 is the number of edges between c1 and c0 .
6-40:N2 is the number of edges between c2 and c0 .
6-41:N0 is the number of IS A links of c0 from the root of the taxonomy .
6-42:SimW u&P almer(c1, c2) = 2 × N0 N1 + N2 + 2 × N0 (4) 5.4 RP Semantic Metric We propose to estimate the relative distance in a taxonomy between two concepts using the following intuitions .
6-43:We use Figure 2 to illustrate these intuitions .
6-44:• Parent versus grandparent: Parent of a node is more similar to the node than grandparents of that .
6-45:Generalization of The Sixth Intl .
6-46:Joint Conf .
6-47:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 1305 a concept reasonably results in going further away that concept .
6-48:The more general concepts are, the less similar they are .
6-49:For example, AnyWineColor is parent of ReddishColor and ReddishColor is parent of Red .
6-50:Then, we expect the similarity between ReddishColor and Red to be higher than that of the similarity between AnyWineColor and Red .
6-51:• Parent versus sibling: A node would have higher similarity to its parent than to its sibling .
6-52:For instance, Red and Rose are children of ReddishColor .
6-53:In this case, we expect the similarity between Red and ReddishColor to be higher than that of Red and Rose .
6-54:• Sibling versus grandparent: A node is more similar to it"s sibling then to its grandparent .
6-55:To illustrate, AnyWineColor is grandparent of Red, and Red and Rose are siblings .
6-56:Therefore, we possibly anticipate that Red and Rose are more similar than AnyWineColor and Red .
6-57:As a taxonomy is represented in a tree, that tree can be traversed from the first concept being compared through the second concept .
6-58:At starting node related to the first concept, the similarity value is constant and equal to one .
6-59:This value is diminished by a constant at each node being visited over the path that will reach to the node including the second concept .
6-60:The shorter the path between the concepts, the higher the similarity between nodes .
6-61:Algorithm 2 Estimate RP Similarity(c1,c2) Require: The constants should be m > n > m2 where m, n ∈ R[0, 1] 1: Similarity ← 1 2: if c1 is equal to c2 then 3: Return Similarity 4: end if 5: commonParent ← findCommonParent(c1, c2) {commonParent is the most specific concept that covers both c1 and c2} 6: N1 ← findDistance(commonParent, c1) 7: N2 ← findDistance(commonParent, c2) {N1 & N2 are the number of links between the concept and parent concept} 8: if (commonParent == c1) or (commonParent == c2) then 9: Similarity ← Similarity ∗ m(N1+N2) 10: else 11: Similarity ← Similarity ∗ n ∗ m(N1+N2−2) 12: end if 13: Return Similarity Relative distance between nodes c1 and c2 is estimated in the following way .
6-62:Starting from c1, the tree is traversed to reach c2 .
6-63:At each hop, the similarity decreases since the concepts are getting farther away from each other .
6-64:However, based on our intuitions, not all hops decrease the similarity equally .
6-65:Let m represent the factor for hopping from a child to a parent and n represent the factor for hopping from a sibling to another sibling .
6-66:Since hopping from a node to its grandparent counts as two parent hops, the discount factor of moving from a node to its grandparent is m2 .
6-67:According to the above intuitions, our constants should be in the form m > n > m2 where the value of m and n should be between zero and one .
6-68:Algorithm 2 shows the distance calculation .
6-69:According to the algorithm, firstly the similarity is initialized with the value of one (line 1) .
6-70:If the concepts are equal to each other then, similarity will be one (lines 2 4) .
6-71:Otherwise, we compute the common parent of the two nodes and the distance of each concept to the common parent without considering the sibling (lines 5 7) .
6-72:If one of the concepts is equal to the common parent, then there is no sibling relation between the concepts .
6-73:For each level, we multiply the similarity by m and do not consider the sibling factor in the similarity estimation .
6-74:As a result, we decrease the similarity at each level with the rate of m (line9) .
6-75:Otherwise, there has to be a sibling relation .
6-76:This means that we have to consider the effect of n when measuring similarity .
6-77:Recall that we have counted N1+N2 edges between the concepts .
6-78:Since there is a sibling relation, two of these edges constitute the sibling relation .
6-79:Hence, when calculating the effect of the parent relation, we use N1+N2 −2 edges (line 11) .
6-80:Some similarity estimations related to the taxonomy in Figure 2 are given in Table 2 .
6-81:In this example, m is taken as 2 3 and n is taken as 4 7 .
6-82:Table 2: Sample similarity estimation over sample taxonomy Similarity(ReddishColor, Rose) = 1 ∗ (2 3) = 0.6666667 Similarity(Red, Rose) = 1 ∗ (4 7) = 0.5714286 Similarity(AnyW ineColor,Rose) = 1 ∗ (2 3)2 = 0.44444445 Similarity(W hite,Rose) = 1 ∗ (2 3) ∗ (4 7) = 0.3809524 For all semantic similarity metrics in our architecture, the taxonomy for features is held in the shared ontology .
6-83:In order to evaluate the similarity of feature vector, we firstly estimate the similarity for feature one by one and take the average sum of these similarities .
6-84:Then the result is equal to the average semantic similarity of the entire feature vector. .
7 DEVELOPED SYSTEM :
7-1:We have implemented our architecture in Java .
7-2:To ease testing of the system, the consumer agent has a user interface that allows us to enter various requests .
7-3:The producer agent is fully automated and the learning and service offering operations work as explained before .
7-4:In this section, we explain the implementation details of the developed system .
7-5:We use OWL [11] as our ontology language and JENA as our ontology reasoner .
7-6:The shared ontology is the modified version of the Wine Ontology [19] .
7-7:It includes the description of wine as a concept and different types of wine .
7-8:All participants of the negotiation use this ontology for understanding each other .
7-9:According to the ontology, seven properties make up the wine concept .
7-10:The consumer agent and the producer agent obtain the possible values for the these properties by querying the ontology .
7-11:Thus, all possible values for the components of the wine concept such as color, body, sugar and so on can be reached by both agents .
7-12:Also a variety of wine types are described in this ontology such as Burgundy, Chardonnay, CheninBlanc and so on .
7-13:Intuitively, any wine type described in the ontology also represents a wine concept .
7-14:This allows us to consider instances of Chardonnay wine as instances of Wine class .
7-15:In addition to wine description, the hierarchical information of some features can be inferred from the ontology .
7-16:For instance, we can represent the information Europe Continent covers Western Country .
7-17:Western Country covers French Region, which covers some territories such as Loire, Bordeaux and so on .
7-18:This hierarchical information is used in estimation of semantic similarity .
7-19:In this part, some reasoning can be made such as if a concept X covers Y and Y covers Z, then concept X covers Z .
7-20:For example, Europe Continent covers Bordeaux .
7-21:1306 The Sixth Intl .
7-22:Joint Conf .
7-23:on Autonomous Agents and Multi Agent Systems (AAMAS 07) For some features such as body, flavor and sugar, there is no hierarchical information, but their values are semantically leveled .
7-24:When that is the case, we give the reasonable similarity values for these features .
7-25:For example, the body can be light, medium, or strong .
7-26:In this case, we assume that light is 0.66 similar to medium but only 0.33 to strong .
7-27:WineStock Ontology is the producer"s inventory and describes a product class as WineProduct .
7-28:This class is necessary for the producer to record the wines that it sells .
7-29:Ontology involves the individuals of this class .
7-30:The individuals represent available services that the producer owns .
7-31:We have prepared two separate WineStock ontologies for testing .
7-32:In the first ontology, there are 19 available wine products and in the second ontology, there are 50 products. .
8 PERFORMANCE EVALUATION :
8-1:We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only) .
8-2:We apply a variety of scenarios on this dataset in order to see the performance differences .
8-3:Each test scenario contains a list of preferences for the user and number of matches from the product list .
8-4:Table 3 shows these preferences and availability of those products in the inventory for first five scenarios .
8-5:Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation .
8-6:Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3 .
8-7:Here, first we use Tversky"s similarity measure .
8-8:With these test cases, we are interested in finding the number of iterations that are required for the producer to generate an acceptable offer for the consumer .
8-9:Since the performance also depends on the initial request, we repeat our experiments with different initial requests .
8-10:Consequently, for each case, we run the algorithms five times with several variations of the initial requests .
8-11:In each experiment, we count the number of iterations that were needed to reach an agreement .
8-12:We take the average of these numbers in order to evaluate these systems fairly .
8-13:As is customary, we test each algorithm with the same initial requests .
8-14:Table 4 compares the approaches using different learning algorithm .
8-15:When the large parts of inventory is compatible with the customer"s preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1) .
8-16:As the number of compatible services drops, RO performs poorly as expected .
8-17:The second worst method is SCR since it only considers the customer"s most recent request and does not learn from previous requests .
8-18:CEA gives the best results when it can generate an answer but cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5 .
8-19:ID3 and DCEA achieve the best results .
8-20:Their performance is comparable and they can handle all cases including Scenario 5 .
8-21:Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg .
8-22:of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in Section 5, we fix the learning algorithm to DCEA .
8-23:In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information .
8-24:• The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape .
8-25:Moreover, the winery of the wine should not be expensive .
8-26:There are only four products meeting these conditions .
8-27:• The customer wants to buy wine whose color is red or rose and grape type is red grape .
8-28:In addition, the location of wine should be in Europe .
8-29:The sweetness degree is wished to be dry or off dry .
8-30:The flavor should be delicate or moderate where the body should be medium or light .
8-31:Furthermore, the winery of the wine should be an expensive winery .
8-32:There are two products meeting all these requirements .
8-33:• The customer wants to buy moderate rose wine, which is located around French Region .
8-34:The category of winery should be Moderate Winery .
8-35:There is only one product meeting these requirements .
8-36:• The customer wants to buy expensive red wine, which is located around California Region or cheap white wine, which is located in around Texas Region .
8-37:There are five available products .
8-38:• The customer wants to buy delicate white wine whose producer in the category of Expensive Winery .
8-39:There are two available products .
8-40:The first seven scenarios are tested with the first dataset that contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services .
8-41:Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus .
8-42:Tversky"s metric gives the worst results since it does not consider the semantic similarity .
8-43:Lin"s performance are better than Tversky but worse than others .
8-44:Wu Palmer"s metric and RP similarity measure nearly give the same performance and better than others .
8-45:When the results are examined, considering semantic closeness increases the performance. .
9-1:We review the recent literature in comparison to our work
9-2:Tama et al
9-3:[16] propose a new approach based on ontology for negotiation
9-4:According to their approach, the negotiation protocols used in e commerce can be modeled as ontologies
9-5:Thus, the agents can perform negotiation protocol by using this shared ontology without the need of being hard coded of negotiation protocol details
9-6:While The Sixth Intl
9-7:Joint Conf
9-8:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 1307 Table 5: Comparison of similarity metrics in terms of number of interactions Run Tversky Lin Wu Palmer RP Scenario 1: 1.2 1.2 1 1 Scenario 2: 1.4 1.4 1.6 1.6 Scenario 3: 1.4 1.8 2 2 Scenario 4: 2.2 1 1.2 1.2 Scenario 5: 2 1.6 1.6 1.6 Scenario 6: 5 3.8 2.4 2.6 Scenario 7: 3.2 1.2 1 1 Scenario 8: 5.6 2 2 2.2 Scenario 9: 2.6 2.2 2.2 2.6 Scenario 10: 4.4 2 2 1.8 Average of all cases: 2.9 1.82 1.7 1.76 Tama et al
9-9:model the negotiation protocol using ontologies, we have instead modeled the service to be negotiated
9-10:Further, we have built a system with which negotiation preferences can be learned
9-11:Sadri et al
9-12:study negotiation in the context of resource allocation [14]
9-13:Agents have limited resources and need to require missing resources from other agents
9-14:A mechanism which is based on dialogue sequences among agents is proposed as a solution
9-15:The mechanism relies on observe think action agent cycle
9-16:These dialogues include offering resources, resource exchanges and offering alternative resource
9-17:Each agent in the system plans its actions to reach a goal state
9-18:Contrary to our approach, Sadri et al."s study is not concerned with learning preferences of each other
9-19:Brzostowski and Kowalczyk propose an approach to select an appropriate negotiation partner by investigating previous multi attribute negotiations [1]
9-20:For achieving this, they use case based reasoning
9-21:Their approach is probabilistic since the behavior of the partners can change at each iteration
9-22:In our approach, we are interested in negotiation the content of the service
9-23:After the consumer and producer agree on the service, price oriented negotiation mechanisms can be used to agree on the price
9-24:Fatima et al
9-25:study the factors that affect the negotiation such as preferences, deadline, price and so on, since the agent who develops a strategy against its opponent should consider all of them [5]
9-26:In their approach, the goal of the seller agent is to sell the service for the highest possible price whereas the goal of the buyer agent is to buy the good with the lowest possible price
9-27:Time interval affects these agents differently
9-28:Compared to Fatima et al
9-29:our focus is different
9-30:While they study the effect of time on negotiation, our focus is on learning preferences for a successful negotiation
9-31:Faratin et al
9-32:propose a multi issue negotiation mechanism, where the service variables for the negotiation such as price, quality of the service, and so on are considered traded offs against each other (i.e., higher price for earlier delivery) [4]
9-33:They generate a heuristic model for trade offs including fuzzy similarity estimation and a hill climbing exploration for possibly acceptable offers
9-34:Although we address a similar problem, we learn the preferences of the customer by the help of inductive learning and generate counter offers in accordance with these learned preferences
9-35:Faratin et al
9-36:only use the last offer made by the consumer in calculating the similarity for choosing counter offer
9-37:Unlike them, we also take into account the previous requests of the consumer
9-38:In their experiments, Faratin et al
9-39:assume that the weights for service variables are fixed a priori
9-40:On the contrary, we learn these preferences over time
9-41:In our future work, we plan to integrate ontology reasoning into the learning algorithm so that hierarchical information can be learned from subsumption hierarchy of relations
9-42:Further, by using relationships among features, the producer can discover new knowledge from the existing knowledge
9-43:These are interesting directions that we will pursue in our future work.
10-1:J
10-2:Brzostowski and R
10-3:Kowalczyk
10-4:On possibilistic case based reasoning for selecting partners for multi attribute agent negotiation
10-5:In Proceedings of the 4th Intl
10-6:Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 273 278, 2005
10-7:L
10-8:Busch and I
10-9:Horstman
10-10:A comment on issue by issue negotiations
10-11:Games and Economic Behavior, 19:144 148, 1997
10-12:J
10-13:K
10-14:Debenham
10-15:Managing e market negotiation in context with a multiagent system
10-16:In Proceedings 21st International Conference on Knowledge Based Systems and Applied Artificial Intelligence, ES"2002:, 2002
10-17:P
10-18:Faratin, C
10-19:Sierra, and N
10-20:R
10-21:Jennings
10-22:Using similarity criteria to make issue trade offs in automated negotiations
10-23:Artificial Intelligence, 142:205 237, 2002
10-24:S
10-25:Fatima, M
10-26:Wooldridge, and N
10-27:Jennings
10-28:Optimal agents for multi issue negotiation
10-29:In Proceeding of the 2nd Intl
10-30:Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), pages 129 136, 2003
10-31:C
10-32:Giraud Carrier
10-33:A note on the utility of incremental learning
10-34:AI Communications, 13(4):215 223, 2000
10-35:T. P
10-36:Hong and S. S
10-37:Tseng
10-38:Splitting and merging version spaces to learn disjunctive concepts
10-39:IEEE Transactions on Knowledge and Data Engineering, 11(5):813 815, 1999
10-40:D
10-41:Lin
10-42:An information theoretic definition of similarity
10-43:In Proc
10-44:15th International Conf
10-45:on Machine Learning, pages 296 304
10-46:Morgan Kaufmann, San Francisco, CA, 1998
10-47:P
10-48:Maes, R
10-49:H
10-50:Guttman, and A
10-51:G
10-52:Moukas
10-53:Agents that buy and sell
10-54:Communications of the ACM, 42(3):81 91, 1999
10-55:T
10-56:M
10-57:Mitchell
10-58:Machine Learning
10-59:McGraw Hill, NY, 1997
10-60:OWL
10-61:OWL: Web ontology language guide, 2003
10-62:http:  www.w3.org TR 2003 CR owl guide 20030818
10-63:S
10-64:K
10-65:Pal and S
10-66:C
10-67:K
10-68:Shiu
10-69:Foundations of Soft Case Based Reasoning
10-70:John Wiley & Sons, New Jersey, 2004
10-71:J
10-72:R
10-73:Quinlan
10-74:Induction of decision trees
10-75:Machine Learning, 1(1):81 106, 1986
10-76:F
10-77:Sadri, F
10-78:Toni, and P
10-79:Torroni
10-80:Dialogues for negotiation: Agent varieties and dialogue sequences
10-81:In ATAL 2001, Revised Papers, volume 2333 of LNAI, pages 405 421
10-82:Springer Verlag, 2002
10-83:M
10-84:P
10-85:Singh
10-86:Value oriented electronic commerce
10-87:IEEE Internet Computing, 3(3):6 7, 1999
10-88:V
10-89:Tamma, S
10-90:Phelps, I
10-91:Dickinson, and M
10-92:Wooldridge
10-93:Ontologies for supporting negotiation in e commerce
10-94:Engineering Applications of Artificial Intelligence, 18:223 236, 2005
10-95:A
10-96:Tversky
10-97:Features of similarity
10-98:Psychological Review, 84(4):327 352, 1977
10-99:P
10-100:E
10-101:Utgoff
10-102:Incremental induction of decision trees
10-103:Machine Learning, 4:161 186, 1989
10-104:Wine, 2003
10-105:http:  www.w3.org TR 2003 CR owl guide20030818 wine.rdf
10-106:Z
10-107:Wu and M
10-108:Palmer
10-109:Verb semantics and lexical selection
10-110:In 32nd
10-111:Annual Meeting of the Association for Computational Linguistics, pages 133 138, 1994
10-112:1308 The Sixth Intl
10-113:Joint Conf
10-114:on Autonomous Agents and Multi Agent Systems (AAMAS 07)
picture:
