Graphical Models for Online Solutions to Interactive 
content:
1 ABSTRACT :
1-1:We develop a new graphical representation for interactive partially observable Markov decision processes (I POMDPs) that is significantly more transparent and semantically clear than the previous representation .
1-2:These graphical models called interactive dynamic influence diagrams (I DIDs) seek to explicitly model the structure that is often present in real world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables .
1-3:I DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I POMDPs generalize POMDPs .
1-4:I DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents .
1-5:Using several examples, we show how I DIDs may be applied and demonstrate their usefulness .
1-6:I.2.11 [Distributed Artificial Intelligence]: Multiagent Systems .
2 INTRODUCTION :
2-1:Interactive partially observable Markov decision processes (IPOMDPs) [9] provide a framework for sequential decision making in partially observable multiagent environments .
2-2:They generalize POMDPs [13] to multiagent settings by including the other agents" computable models in the state space along with the states of the physical environment .
2-3:The models encompass all information influencing the agents" behaviors, including their preferences, capabilities, and beliefs, and are thus analogous to types in Bayesian games [11] .
2-4:I POMDPs adopt a subjective approach to understanding strategic behavior, rooted in a decision theoretic framework that takes a decision maker"s perspective in the interaction .
2-5:In [15], Polich and Gmytrasiewicz introduced interactive dynamic influence diagrams (I DIDs) as the computational representations of I POMDPs .
2-6:I DIDs generalize DIDs [12], which may be viewed as computational counterparts of POMDPs, to multiagents settings in the same way that I POMDPs generalize POMDPs .
2-7:I DIDs contribute to a growing line of work [19] that includes multi agent influence diagrams (MAIDs) [14], and more recently, networks of influence diagrams (NIDs) [8] .
2-8:These formalisms seek to explicitly model the structure that is often present in real world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables .
2-9:MAIDs provide an alternative to normal and extensive game forms using a graphical formalism to represent games of imperfect information with a decision node for each agent"s actions and chance nodes capturing the agent"s private information .
2-10:MAIDs objectively analyze the game, efficiently computing the Nash equilibrium profile by exploiting the independence structure .
2-11:NIDs extend MAIDs to include agents" uncertainty over the game being played and over models of the other agents .
2-12:Each model is a MAID and the network of MAIDs is collapsed, bottom up, into a single MAID for computing the equilibrium of the game keeping in mind the different models of each agent .
2-13:Graphical formalisms such as MAIDs and NIDs open up a promising area of research that aims to represent multiagent interactions more transparently .
2-14:However, MAIDs provide an analysis of the game from an external viewpoint and the applicability of both is limited to static single play games .
2-15:Matters are more complex when we consider interactions that are extended over time, where predictions about others" future actions must be made using models that change as the agents act and observe .
2-16:I DIDs address this gap by allowing the representation of other agents" models as the values of a special model node .
2-17:Both, other agents" models and the original agent"s beliefs over these models are updated over time using special purpose implementations .
2-18:In this paper, we improve on the previous preliminary representation of the I DID shown in [15] by using the insight that the static I ID is a type of NID .
2-19:Thus, we may utilize NID specific language constructs such as multiplexers to represent the model node, and subsequently the I ID, more transparently .
2-20:Furthermore, we clarify the semantics of the special purpose policy link introduced in the representation of I DID by [15], and show that it could be replaced by traditional dependency links .
2-21:In the previous representation of the I DID, the update of the agent"s belief over the models of others as the agents act and receive observations was denoted using a special link called the model update link that connected the model nodes over time .
2-22:We explicate the semantics of this link by showing how it can be implemented using the traditional dependency links between the chance nodes that constitute the model nodes .
2-23:The net result is a representation of I DID that is significantly more transparent, semantically clear, and capable of being implemented using the standard algorithms for solving DIDs .
2-24:We show how IDIDs may be used to model an agent"s uncertainty over others" models, that may themselves be I DIDs .
2-25:Solution to the I DID is a policy that prescribes what the agent should do over time, given its beliefs over the physical state and others" models .
2-26:Analogous to DIDs, I DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. .
3 BACKGROUND: FINITELY NESTED  IPOMDPS :
3-1:IPOMDPS Interactive POMDPs generalize POMDPs to multiagent settings by including other agents" models as part of the state space [9] .
3-2:Since other agents may also reason about others, the interactive state space is strategically nested; it contains beliefs about other agents" models and their beliefs about others .
3-3:For simplicity of presentation we consider an agent, i, that is interacting with one other agent, j .
3-4:A finitely nested I POMDP of agent i with a strategy level l is defined as the tuple: I POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri where: • ISi,l denotes a set of interactive states defined as, ISi,l = S × Mj,l−1, where Mj,l−1 = {Θj,l−1 ∪ SMj}, for l ≥ 1, and ISi,0 = S, where S is the set of states of the physical environment .
3-5:Θj,l−1 is the set of computable intentional models of agent j: θj,l−1 = bj,l−1, ˆθj where the frame, ˆθj = A, Ωj, Tj, Oj, Rj, OCj .
3-6:Here, j is Bayes rational and OCj is j"s optimality criterion .
3-7:SMj is the set of subintentional models of j .
3-8:Simple examples of subintentional models include a no information model [10] and a fictitious play model [6], both of which are history independent .
3-9:We give a recursive bottom up construction of the interactive state space below .
3-10:ISi,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} ISi,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} .
3-11:.
3-12:.
3-13:.
3-14:.
3-15:.
3-16:ISi,l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Similar formulations of nested spaces have appeared in [1, 3] .
3-17:• A = Ai × Aj is the set of joint actions of all agents in the environment; • Ti : S ×A×S → [0, 1], describes the effect of the joint actions on the physical states of the environment; • Ωi is the set of observations of agent i; • Oi : S × A × Ωi → [0, 1] gives the likelihood of the observations given the physical state and joint action; • Ri : ISi × A → R describes agent i"s preferences over its interactive states .
3-18:Usually only the physical states will matter .
3-19:Agent i"s policy is the mapping, Ω∗ i → Δ(Ai), where Ω∗ i is the set of all observation histories of agent i .
3-20:Since belief over the interactive states forms a sufficient statistic [9], the policy can also be represented as a mapping from the set of all beliefs of agent i to a distribution over its actions, Δ(ISi) → Δ(Ai) .
3-21:2.1 Belief Update Analogous to POMDPs, an agent within the I POMDP framework updates its belief as it acts and observes .
3-22:However, there are two differences that complicate the belief update in multiagent settings when compared to single agent ones .
3-23:First, since the state of the physical environment depends on the actions of both agents, i"s prediction of how the physical state changes has to be made based on its prediction of j"s actions .
3-24:Second, changes in j"s models have to be included in i"s belief update .
3-25:Specifically, if j is intentional then an update of j"s beliefs due to its action and observation has to be included .
3-26:In other words, i has to update its belief based on its prediction of what j would observe and how j would update its belief .
3-27:If j"s model is subintentional, then j"s probable observations are appended to the observation history contained in the model .
3-28:Formally, we have: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) where β is the normalizing constant, τ is 1 if its argument is 0 otherwise it is 0, Pr(at−1 j |θt−1 j,l−1) is the probability that at−1 j is Bayes rational for the agent described by model θt−1 j,l−1, and SE(·) is an abbreviation for the belief update .
3-29:For a version of the belief update when j"s model is subintentional, see [9] .
3-30:If agent j is also modeled as an I POMDP, then i"s belief update invokes j"s belief update (via the term SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), which in turn could invoke i"s belief update and so on .
3-31:This recursion in belief nesting bottoms out at the 0th level .
3-32:At this level, the belief update of the agent reduces to a POMDP belief update .
3-33:1 For illustrations of the belief update, additional details on I POMDPs, and how they compare with other multiagent frameworks, see [9] .
3-34:2.2 Value Iteration Each belief state in a finitely nested I POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) where, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (since is = (s, mj,l−1)) .
3-35:Eq .
3-36:2 is a basis for value iteration in I POMDPs .
3-37:Agent i"s optimal action, a∗ i , for the case of finite horizon with discounting, is an element of the set of optimal actions for the belief state, OPT(θi), defined as: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) .
4 INTERACTIVEINFLUENCEDIAGRAMS :
4-1:A naive extension of influence diagrams (IDs) to settings populated by multiple agents is possible by treating other agents as automatons, represented using chance nodes .
4-2:However, this approach assumes that the agents" actions are controlled using a probability distribution that does not change over time .
4-3:Interactive influence diagrams (I IDs) adopt a more sophisticated approach by generalizing IDs to make them applicable to settings shared with other agents who may act and observe, and update their beliefs .
4-4:3.1 Syntax In addition to the usual chance, decision, and utility nodes, IIDs include a new type of node called the model node .
4-5:We show a general level l I ID in Fig .
4-6:1(a), where the model node (Mj,l−1) is denoted using a hexagon .
4-7:We note that the probability distribution over the chance node, S, and the model node together represents agent i"s belief over its interactive states .
4-8:In addition to the model 1 The 0th level model is a POMDP: Other agent"s actions are treated as exogenous events and folded into the T, O, and R functions .
4-9:The Sixth Intl .
4-10:Joint Conf .
4-11:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 815 Figure 1: (a) A generic level l I ID for agent i situated with one other agent j .
4-12:The hexagon is the model node (Mj,l−1) whose structure we show in (b) .
4-13:Members of the model node are I IDs themselves (m1 j,l−1, m2 j,l−1; diagrams not shown here for simplicity) whose decision nodes are mapped to the corresponding chance nodes (A1 j , A2 j ) .
4-14:Depending on the value of the node, Mod[Mj], the distribution of each of the chance nodes is assigned to the node Aj .
4-15:(c) The transformed I ID with the model node replaced by the chance nodes and the relationships between them .
4-16:node, I IDs differ from IDs by having a dashed link (called the policy link in [15]) between the model node and a chance node, Aj, that represents the distribution over the other agent"s actions given its model .
4-17:In the absence of other agents, the model node and the chance node, Aj, vanish and I IDs collapse into traditional IDs .
4-18:The model node contains the alternative computational models ascribed by i to the other agent from the set, Θj,l−1 ∪ SMj, where Θj,l−1 and SMj were defined previously in Section 2 .
4-19:Thus, a model in the model node may itself be an I ID or ID, and the recursion terminates when a model is an ID or subintentional .
4-20:Because the model node contains the alternative models of the other agent as its values, its representation is not trivial .
4-21:In particular, some of the models within the node are I IDs that when solved generate the agent"s optimal policy in their decision nodes .
4-22:Each decision node is mapped to the corresponding chance node, say A1 j , in the following way: if OPT is the set of optimal actions obtained by solving the I ID (or ID), then Pr(aj ∈ A1 j ) = 1 |OP T | if aj ∈ OPT, 0 otherwise .
4-23:Borrowing insights from previous work [8], we observe that the model node and the dashed policy link that connects it to the chance node, Aj, could be represented as shown in Fig .
4-24:1(b) .
4-25:The decision node of each level l − 1 I ID is transformed into a chance node, as we mentioned previously, so that the actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability .
4-26:The different chance nodes (A1 j , A2 j ), one for each model, and additionally, the chance node labeled Mod[Mj] form the parents of the chance node, Aj .
4-27:Thus, there are as many action nodes (A1 j , A2 j ) in Mj,l−1 as the number of models in the support of agent i"s beliefs .
4-28:The conditional probability table of the chance node, Aj, is a multiplexer that assumes the distribution of each of the action nodes (A1 j , A2 j ) depending on the value of Mod[Mj] .
4-29:The values of Mod[Mj] denote the different models of j .
4-30:In other words, when Mod[Mj] has the value m1 j,l−1, the chance node Aj assumes the distribution of the node A1 j , and Aj assumes the distribution of A2 j when Mod[Mj] has the value m2 j,l−1 .
4-31:The distribution over the node, Mod[Mj], is the agent i"s belief over the models of j given a physical state .
4-32:For more agents, we will have as many model nodes as there are agents .
4-33:Notice that Fig .
4-34:1(b) clarifies the semantics of the policy link, and shows how it can be represented using the traditional dependency links .
4-35:In Fig .
4-36:1(c), we show the transformed I ID when the model node is replaced by the chance nodes and relationships between them .
4-37:In contrast to the representation in [15], there are no special purpose policy links, rather the I ID is composed of only those types of nodes that are found in traditional IDs and dependency relationships between the nodes .
4-38:This allows I IDs to be represented and implemented using conventional application tools that target IDs .
4-39:Note that we may view the level l I ID as a NID .
4-40:Specifically, each of the level l − 1 models within the model node are blocks in the NID (see Fig .
4-41:2) .
4-42:If the level l = 1, each block is a traditional ID, otherwise if l > 1, each block within the NID may itself be a NID .
4-43:Note that within the I IDs (or IDs) at each level, there is only a single decision node .
4-44:Thus, our NID does not contain any MAIDs .
4-45:Figure 2: A level l I ID represented as a NID .
4-46:The probabilities assigned to the blocks of the NID are i"s beliefs over j"s models conditioned on a physical state .
4-47:3.2 Solution The solution of an I ID proceeds in a bottom up manner, and is implemented recursively .
4-48:We start by solving the level 0 models, which, if intentional, are traditional IDs .
4-49:Their solutions provide probability distributions over the other agents" actions, which are entered in the corresponding chance nodes found in the model node of the level 1 I ID .
4-50:The mapping from the level 0 models" decision nodes to the chance nodes is carried out so that actions with the largest value in the decision node are assigned uniform probabilities in the chance node while the rest are assigned zero probability .
4-51:Given the distributions over the actions within the different chance nodes (one for each model of the other agent), the level 1 I ID is transformed as shown in Fig .
4-52:1(c) .
4-53:During the transformation, the conditional probability table of the node, Aj, is populated such that the node assumes the distribution of each of the chance nodes depending on the value of the node, Mod[Mj] .
4-54:As we mentioned previously, the values of the node Mod[Mj] denote the different models of the other agent, and its distribution is the agent i"s belief over the models of j conditioned on the physical state .
4-55:The transformed level 1 I ID is a traditional ID that may be solved us816 The Sixth Intl .
4-56:Joint Conf .
4-57:on Autonomous Agents and Multi Agent Systems (AAMAS 07) (a) (b) Figure 3: (a) A generic two time slice level l I DID for agent i in a setting with one other agent j .
4-58:Notice the dotted model update link that denotes the update of the models of j and the distribution over the models over time .
4-59:(b) The semantics of the model update link .
4-60:ing the standard expected utility maximization method [18] .
4-61:This procedure is carried out up to the level l I ID whose solution gives the non empty set of optimal actions that the agent should perform given its belief .
4-62:Notice that analogous to IDs, I IDs are suitable for online decision making when the agent"s current belief is known. .
5 INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS :
5-1:DIAGRAMS Interactive dynamic influence diagrams (I DIDs) extend I IDs (and NIDs) to allow sequential decision making over several time steps .
5-2:Just as DIDs are structured graphical representations of POMDPs, I DIDs are the graphical online analogs for finitely nested I POMDPs .
5-3:I DIDs may be used to optimize over a finite look ahead given initial beliefs while interacting with other, possibly similar, agents .
5-4:4.1 Syntax We depict a general two time slice I DID in Fig .
5-5:3(a) .
5-6:In addition to the model nodes and the dashed policy link, what differentiates an I DID from a DID is the model update link shown as a dotted arrow in Fig .
5-7:3(a) .
5-8:We explained the semantics of the model node and the policy link in the previous section; we describe the model updates next .
5-9:The update of the model node over time involves two steps: First, given the models at time t, we identify the updated set of models that reside in the model node at time t + 1 .
5-10:Recall from Section 2 that an agent"s intentional model includes its belief .
5-11:Because the agents act and receive observations, their models are updated to reflect their changed beliefs .
5-12:Since the set of optimal actions for a model could include all the actions, and the agent may receive any one of |Ωj| possible observations, the updated set at time step t + 1 will have at most |Mt j,l−1||Aj||Ωj| models .
5-13:Here, |Mt j,l−1| is the number of models at time step t, |Aj| and |Ωj| are the largest spaces of actions and observations respectively, among all the models .
5-14:Second, we compute the new distribution over the updated models given the original distribution and the probability of the agent performing the action and receiving the observation that led to the updated model .
5-15:These steps are a part of agent i"s belief update formalized using Eq .
5-16:1 .
5-17:In Fig .
5-18:3(b), we show how the dotted model update link is implemented in the I DID .
5-19:If each of the two level l − 1 models ascribed to j at time step t results in one action, and j could make one of two possible observations, then the model node at time step t + 1 contains four updated models (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , and mt+1,4 j,l−1 ) .
5-20:These models differ in their initial beliefs, each of which is the result of j updating its beliefs due to its action and a possible observation .
5-21:The decision nodes in each of the I DIDs or DIDs that represent the lower level models are mapped to the corresponding Figure 4: Transformed I DID with the model nodes and model update link replaced with the chance nodes and the relationships (in bold) .
5-22:chance nodes, as mentioned previously .
5-23:Next, we describe how the distribution over the updated set of models (the distribution over the chance node Mod[Mt+1 j ] in Mt+1 j,l−1) is computed .
5-24:The probability that j"s updated model is, say mt+1,1 j,l−1 , depends on the probability of j performing the action and receiving the observation that led to this model, and the prior distribution over the models at time step .
6 If l ≥ 1 then :
6-1:Populate Mt+1 j,l−1 .
7 For each mt :
7-1:j in Range(Mt j,l−1) do that represents mt j and the horizon, T − t + 1 OPT(mt j), to a chance node Aj .
8 For each aj in OPT(mt :
8-1:j) do .
9 For each oj in Oj (part of mt :
9-1:j) do .
10 Update j"s belief, bt+1 :
10-1:j ← SE(bt j, aj, oj) .
11 mt+1 :
11-1:j ← New I ID (or ID) with bt+1 j as the initial belief 10 .
11-2:Range(Mt+1 j,l−1) ∪ ← {mt+1 j } 11 .
11-3:Add the model node, Mt+1 j,l−1, and the dependency links between Mt j,l−1 and Mt+1 j,l−1 (shown in Fig .
11-4:3(b)) 12 .
11-5:Add the chance, decision, and utility nodes for t + 1 time slice and the dependency links between them 13 .
11-6:Establish the CPTs for each chance node and utility node Look Ahead Phase 14 .
11-7:Apply the standard look ahead and backup method to solve the expanded I DID Figure 5: Algorithm for solving a level l ≥ 0 I DID .
11-8:level l I DID expanded over T time steps with one other agent j in Fig .
11-9:5 .
11-10:We adopt a two phase approach: Given an I ID of level l (described previously in Section 3) with all lower level models also represented as I IDs or IDs (if level 0), the first step is to expand the level l I ID over T time steps adding the dependency links and the conditional probability tables for each node .
11-11:We particularly focus on establishing and populating the model nodes (lines 3 11) .
11-12:Note that Range(·) returns the values (lower level models) of the random variable given as input (model node) .
11-13:In the second phase, we use a standard look ahead technique projecting the action and observation sequences over T time steps in the future, and backing up the utility values of the reachable beliefs .
11-14:Similar to I IDs, the I DIDs reduce to DIDs in the absence of other agents .
11-15:As we mentioned previously, the 0 th level models are the traditional DIDs .
11-16:Their solutions provide probability distributions over actions of the agent modeled at that level to I DIDs at level 1 .
11-17:Given probability distributions over other agent"s actions the level 1 IDIDs can themselves be solved as DIDs, and provide probability distributions to yet higher level models .
11-18:Assume that the number of models considered at each level is bound by a number, M .
11-19:Solving an I DID of level l in then equivalent to solving O(Ml ) DIDs. .
12 EXAMPLE APPLICATIONS :
12-1:To illustrate the usefulness of I DIDs, we apply them to three problem domains .
12-2:We describe, in particular, the formulation of the I DID and the optimal prescriptions obtained on solving it .
12-3:5.1 Followership Leadership in the Multiagent Tiger Problem We begin our illustrations of using I IDs and I DIDs with a slightly modified version of the multiagent tiger problem discussed in [9] .
12-4:The problem has two agents, each of which can open the right door (OR), the left door or listen .
12-5:In addition to hearing growls (from the left or from the right (GR)) when they listen, the agents also hear creaks (from the left (CL), from the right (CR), or no creaks (S)), which noisily indicate the other agent"s opening one of the doors .
12-6:When any door is opened, the tiger persists in its original location with a probability of 95% .
12-7:Agent i hears growls with a reliability of 65% and creaks with a reliability of 95% .
12-8:Agent j, on the other hand, hears growls with a reliability of 95% .
12-9:Thus, the setting is such that agent i hears agent j opening doors more reliably than the tiger"s growls .
12-10:This suggests that i could use j"s actions as an indication of the location of the tiger, as we discuss below .
12-11:Each agent"s preferences are as in the single agent game discussed in [13] .
12-12:The transition, observation, and reward functions are shown in [16] .
12-13:A good indicator of the usefulness of normative methods for decision making like I DIDs is the emergence of realistic social behaviors in their prescriptions .
12-14:In settings of the persistent multiagent tiger problem that reflect real world situations, we demonstrate followership between the agents and, as shown in [15], deception among agents who believe that they are in a follower leader type of relationship .
12-15:In particular, we analyze the situational and epistemological conditions sufficient for their emergence .
12-16:The followership behavior, for example, results from the agent knowing its own weaknesses, assessing the strengths, preferences, and possible behaviors of the other, and realizing that its best for it to follow the other"s actions in order to maximize its payoffs .
12-17:Let us consider a particular setting of the tiger problem in which agent i believes that j"s preferences are aligned with its own both of them just want to get the gold and j"s hearing is more reliable in comparison to itself .
12-18:As an example, suppose that j, on listening can discern the tiger"s location 95% of the times compared to i"s 65% accuracy .
12-19:Additionally, agent i does not have any initial information about the tiger"s location .
12-20:In other words, i"s single level nested belief, bi,1, assigns 0.5 to each of the two locations of the tiger .
12-21:In addition, i considers two models of j, which differ in j"s flat level 0 initial beliefs .
12-22:This is represented in the level 1 I ID shown in Fig .
12-23:6(a) .
12-24:According to one model, j assigns a probability of 0.9 that the tiger is behind the left door, while the other 818 The Sixth Intl .
12-25:Joint Conf .
12-26:on Autonomous Agents and Multi Agent Systems (AAMAS 07) Figure 6: (a) Level 1 I ID of agent i, (b) two level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a) .
12-27:model assigns 0.1 to that location (see Fig .
12-28:6(b)) .
12-29:Agent i is undecided on these two models of j .
12-30:If we vary i"s hearing ability, and solve the corresponding level 1 I ID expanded over three time steps, we obtain the normative behavioral policies shown in Fig 7 that exhibit followership behavior .
12-31:If i"s probability of correctly hearing the growls is 0.65, then as shown in the policy in Fig .
12-32:7(a), i begins to conditionally follow j"s actions: i opens the same door that j opened previously iff i"s own assessment of the tiger"s location confirms j"s pick .
12-33:If i loses the ability to correctly interpret the growls completely, it blindly follows j and opens the same door that j opened previously (Fig .
12-34:7(b)) .
12-35:Figure 7: Emergence of (a) conditional followership, and (b) blind followership in the tiger problem .
12-36:Behaviors of interest are in bold .
12-37:* is a wildcard, and denotes any one of the observations .
12-38:We observed that a single level of belief nesting beliefs about the other"s models was sufficient for followership to emerge in the tiger problem .
12-39:However, the epistemological requirements for the emergence of leadership are more complex .
12-40:For an agent, say j, to emerge as a leader, followership must first emerge in the other agent are identical to those of j, and believes that j has a better sense of hearing, i will follow j"s actions over time .
12-41:Agent j emerges as a leader if it believes that i will follow it, which implies that j"s belief must be nested two levels deep to enable it to recognize its leadership role .
12-42:Realizing that i will follow presents j with an opportunity to influence i"s actions in the benefit of the collective good or its self interest alone .
12-43:For example, in the tiger problem, let us consider a setting in which if both i and j open the correct door, then each gets a payoff of 20 that is double the original .
12-44:If j alone selects the correct door, it gets the payoff of 10 .
12-45:On the other hand, if both agents pick the wrong door, their penalties are cut in half .
12-46:In this setting, it is in both j"s best interest as well as the collective betterment for j to use its expertise in selecting the correct door, and thus be a good leader .
12-47:However, consider a slightly different problem in which j gains from i"s loss and is penalized if i gains .
12-48:Specifically, let i"s payoff be subtracted from j"s, indicating that j is antagonistic toward i if j picks the correct door and i the wrong one, then i"s loss of 100 becomes j"s gain .
12-49:Agent j believes that i incorrectly thinks that j"s preferences are those that promote the collective good and that it starts off by believing with 99% confidence where the tiger is .
12-50:Because i believes that its preferences are similar to those of j, and that j starts by believing almost surely that one of the two is the correct location (two level 0 models of j), i will start by following j"s actions .
12-51:We show i"s normative policy on solving its singly nested I DID over three time steps in Fig .
12-52:8(a) .
12-53:The policy demonstrates that i will blindly follow j"s actions .
12-54:Since the tiger persists in its original location with a probability of 0.95, i will select the same door again .
12-55:If j begins the game with a 99% probability that the tiger is on the right, solving j"s I DID nested two levels deep, results in the policy shown in Fig .
12-56:8(b) .
12-57:Even though j is almost certain that OL is the correct action, it will start by selecting OR, followed by OL .
12-58:Agent j"s intention is to deceive i who, it believes, will follow j"s actions, so as to gain $110 in the second time step, which is more than what j would gain if it were to be honest .
12-59:Figure 8: Emergence of deception between agents in the tiger problem .
12-60:Behaviors of interest are in bold .
12-61:* denotes as before .
12-62:(a) Agent i"s policy demonstrating that it will blindly follow j"s actions .
12-63:(b) Even though j is almost certain that the tiger is on the right, it will start by selecting OR, followed by OL, in order to deceive i .
12-64:5.2 Altruism and Reciprocity in the Public Good Problem The public good problem [7], consists of a group of M agents, each of whom must either contribute some resource to a public pot or keep it for themselves .
12-65:Since resources contributed to the public pot are shared among all the agents, they are less valuable to the agent when in the public pot .
12-66:However, if all agents choose to contribute their resources, then the payoff to each agent is more than if no one contributes .
12-67:Since an agent gets its share of the public pot irrespective of whether it has contributed or not, the dominating action is for each agent to not contribute, and instead free ride on others" contributions .
12-68:However, behaviors of human players in empirical simulations of the PG problem differ from the normative predictions .
12-69:The experiments reveal that many players initially contribute a large amount to the public pot, and continue to contribute when the PG problem is played repeatedly, though in decreasing amounts [4] .
12-70:Many of these experiments [5] report that a small core group of players persistently contributes to the public pot even when all others are defecting .
12-71:These experiments also reveal that players who persistently contribute have altruistic or reciprocal preferences matching expected cooperation of others .
12-72:For simplicity, we assume that the game is played between M = 2 agents, i and j .
12-73:Let each agent be initially endowed with XT amount of resources .
12-74:While the classical PG game formulation permits each agent to contribute any quantity of resources (≤ XT ) to the public pot, we simplify the action space by allowing two possible actions .
12-75:Each agent may choose to either contribute a fixed amount of the resources, or not contribute .
12-76:The latter action is deThe Sixth Intl .
12-77:Joint Conf .
12-78:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 819 noted as defect .
12-79:We assume that the actions are not observable to others .
12-80:The value of resources in the public pot is discounted by ci for each agent i, where ci is the marginal private return .
12-81:We assume that ci < 1 so that the agent does not benefit enough that it contributes to the public pot for private gain .
12-82:Simultaneously, ciM > 1, making collective contribution pareto optimal .
12-83:i j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Table 1: The one shot PG game with punishment .
12-84:In order to encourage contributions, the contributing agents punish free riders but incur a small cost for administering the punishment .
12-85:Let P be the punishment meted out to the defecting agent and cp the non zero cost of punishing for the contributing agent .
12-86:For simplicity, we assume that the cost of punishing is same for both the agents .
12-87:The one shot PG game with punishment is shown in Table .
12-88:1 .
12-89:Let ci = cj, cp > 0, and if P > XT − ciXT , then defection is no longer a dominating action .
12-90:If P < XT − ciXT , then defection is the dominating action for both .
12-91:If P = XT − ciXT , then the game is not dominance solvable .
12-92:Figure 9: (a) Level 1 I ID of agent i, (b) level 0 IDs of agent j with decision nodes mapped to the chance nodes, A1 j and A2 j , in (a) .
12-93:We formulate a sequential version of the PG problem with punishment from the perspective of agent i .
12-94:Though in the repeated PG game, the quantity in the public pot is revealed to all the agents after each round of actions, we assume in our formulation that it is hidden from the agents .
12-95:Each agent may contribute a fixed amount, xc, or defect .
12-96:An agent on performing an action receives an observation of plenty or meager symbolizing the state of the public pot .
12-97:Notice that the observations are also indirectly indicative of agent j"s actions because the state of the public pot is influenced by them .
12-98:The amount of resources in agent i"s private pot, is perfectly observable to i .
12-99:The payoffs are analogous to Table .
12-100:1 .
12-101:Borrowing from the empirical investigations of the PG problem [5], we construct level 0 IDs for j that model altruistic and non altruistic types (Fig .
12-102:9(b)) .
12-103:Specifically, our altruistic agent has a high marginal private return (cj is close to 1) and does not punish others who defect .
12-104:Let xc = 1 and the level 0 agent be punished half the times it defects .
12-105:With one action remaining, both types of agents choose to contribute to avoid being punished .
12-106:With two actions to go, the altruistic type chooses to contribute, while the other defects .
12-107:This is because cj for the altruistic type is close to 1, thus the expected punishment, 0.5P > (1 − cj), which the altruistic type avoids .
12-108:Because cj for the non altruistic type is less, it prefers not to contribute .
12-109:With three steps to go, the altruistic agent contributes to avoid punishment (0.5P > 2(1 − cj)), and the non altruistic type defects .
12-110:For greater than three steps, while the altruistic agent continues to contribute to the public pot depending on how close its marginal private return is to 1, the non altruistic type prescribes defection .
12-111:We analyzed the decisions of an altruistic agent i modeled using a level 1 I DID expanded over 3 time steps .
12-112:i ascribes the two level 0 models, mentioned previously, to j (see Fig .
12-113:9) .
12-114:If i believes with a probability 1 that j is altruistic, i chooses to contribute for each of the three steps .
12-115:This behavior persists when i is unaware of whether j is altruistic (Fig .
12-116:10(a)), and when i assigns a high probability to j being the non altruistic type .
12-117:However, when i believes with a probability 1 that j is non altruistic and will thus surely defect, i chooses to defect to avoid being punished and because its marginal private return is less than 1 .
12-118:These results demonstrate that the behavior of our altruistic type resembles that found experimentally .
12-119:The non altruistic level 1 agent chooses to defect regardless of how likely it believes the other agent to be altruistic .
12-120:We analyzed the behavior of a reciprocal agent type that matches expected cooperation or defection .
12-121:The reciprocal type"s marginal private return is similar to that of the non altruistic type, however, it obtains a greater payoff when its action is similar to that of the other .
12-122:We consider the case when the reciprocal agent i is unsure of whether j is altruistic and believes that the public pot is likely to be half full .
12-123:For this prior belief, i chooses to defect .
12-124:On receiving an observation of plenty, i decides to contribute, while an observation of meager makes it defect (Fig .
12-125:10(b)) .
12-126:This is because an observation of plenty signals that the pot is likely to be greater than half full, which results from j"s action to contribute .
12-127:Thus, among the two models ascribed to j, its type is likely to be altruistic making it likely that j will contribute again in the next time step .
12-128:Agent i therefore chooses to contribute to reciprocate j"s action .
12-129:An analogous reasoning leads i to defect when it observes a meager pot .
12-130:With one action to go, i believing that j contributes, will choose to contribute too to avoid punishment regardless of its observations .
12-131:Figure 10: (a) An altruistic level 1 agent always contributes .
12-132:(b) A reciprocal agent i starts off by defecting followed by choosing to contribute or defect based on its observation of plenty (indicating that j is likely altruistic) or meager (j is non altruistic) .
12-133:5.3 Strategies in Two Player Poker Poker is a popular zero sum card game that has received much attention among the AI research community as a testbed [2] .
12-134:Poker is played among M ≥ 2 players in which each player receives a hand of cards from a deck .
12-135:While several flavors of Poker with varying complexity exist, we consider a simple version in which each player has three plys during which the player may either exchange a card (E), keep the existing hand (K), fold and withdraw from the game, or call (C), requiring all players to show their hands .
12-136:To keep matters simple, let M = 2, and each player receive a hand consisting of a single card drawn from the same suit .
12-137:Thus, during a showdown, the player who has the numerically larger card (2 is the lowest, ace is the highest) wins the pot .
12-138:During an exchange of cards, the discarded card is placed either in the L pile, indicating to the other agent that it was a low numbered card less than 8, or in the 820 The Sixth Intl .
12-139:Joint Conf .
12-140:on Autonomous Agents and Multi Agent Systems (AAMAS 07) H pile, indicating that the card had a rank greater than or equal to the probability of receiving a low card in exchange is now reduced .
12-141:We show the level 1 I ID for the simplified two player Poker in Fig .
12-142:11 .
12-143:We considered two models (personality types) of agent j .
12-144:The conservative type believes that it is likely that its opponent has a high numbered card in its hand .
12-145:On the other hand, the aggressive agent j believes with a high probability that its opponent has a lower numbered card .
12-146:Thus, the two types differ in their beliefs over their opponent"s hand .
12-147:In both these level 0 models, the opponent is assumed to perform its actions following a fixed, uniform distribution .
12-148:With three actions to go, regardless of its hand (unless it is an ace), the aggressive agent chooses to exchange its card, with the intent of improving on its current hand .
12-149:This is because it believes the other to have a low card, which improves its chances of getting a high card during the exchange .
12-150:The conservative agent chooses to keep its card, no matter its hand because its chances of getting a high card are slim as it believes that its opponent has one .
12-151:Figure 11: (a) Level 1 I ID of agent i .
12-152:The observation reveals information about j"s hand of the previous time step, (b) level 0 IDs of agent j whose decision nodes are mapped to the chance nodes, A1 j , A2 j , in (a) .
12-153:The policy of a level 1 agent i who believes that each card except its own has an equal likelihood of being in j"s hand (neutral personality type) and j could be either an aggressive or conservative type, is shown in Fig .
12-154:12 .
12-155:i"s own hand contains the card numbered 8 .
12-156:The agent starts by keeping its card .
12-157:On seeing that j did not exchange a card (N), i believes with probability 1 that j is conservative and hence will keep its cards .
12-158:i responds by either keeping its card or exchanging it because j is equally likely to have a lower or higher card .
12-159:If i observes that j discarded its card into the L or H pile, i believes that j is aggressive .
12-160:On observing L, i realizes that j had a low card, and is likely to have a high card after its exchange .
12-161:Because the probability of receiving a low card is high now, i chooses to keep its card .
12-162:On observing H, believing that the probability of receiving a high numbered card is high, i chooses to exchange its card .
12-163:In the final step, i chooses to call regardless of its observation history because its belief that j has a higher card is not sufficiently high to conclude that its better to fold and relinquish the payoff .
12-164:This is partly due to the fact that an observation of, say, L resets the agent i"s previous time step beliefs over j"s hand to the low numbered cards only. .
13-1:We showed how DIDs may be extended to I DIDs that enable online sequential decision making in uncertain multiagent settings
13-2:Our graphical representation of I DIDs improves on the previous Figure 12: A level 1 agent i"s three step policy in the Poker problem
13-3:i starts by believing that j is equally likely to be aggressive or conservative and could have any card in its hand with equal probability
13-4:work significantly by being more transparent, semantically clear, and capable of being solved using standard algorithms that target DIDs
13-5:I DIDs extend NIDs to allow sequential decision making over multiple time steps in the presence of other interacting agents
13-6:I DIDs may be seen as concise graphical representations for IPOMDPs providing a way to exploit problem structure and carry out online decision making as the agent acts and observes given its prior beliefs
13-7:We are currently investigating ways to solve I DIDs approximately with provable bounds on the solution quality
13-8:Acknowledgment: We thank Piotr Gmytrasiewicz for some useful discussions related to this work
13-9:The first author would like to acknowledge the support of a UGARF grant.
14-1:R
14-2:J
14-3:Aumann
14-4:Interactive epistemology i: Knowledge
14-5:International Journal of Game Theory, 28:263 300, 1999
14-6:D
14-7:Billings, A
14-8:Davidson, J
14-9:Schaeffer, and D
14-10:Szafron
14-11:The challenge of poker
14-12:AIJ, 2001
14-13:A
14-14:Brandenburger and E
14-15:Dekel
14-16:Hierarchies of beliefs and common knowledge
14-17:Journal of Economic Theory, 59:189 198, 1993
14-18:C
14-19:Camerer
14-20:Behavioral Game Theory: Experiments in Strategic Interaction
14-21:Princeton University Press, 2003
14-22:E
14-23:Fehr and S
14-24:Gachter
14-25:Cooperation and punishment in public goods experiments
14-26:American Economic Review, 90(4):980 994, 2000
14-27:D
14-28:Fudenberg and D
14-29:K
14-30:Levine
14-31:The Theory of Learning in Games
14-32:MIT Press, 1998
14-33:D
14-34:Fudenberg and J
14-35:Tirole
14-36:Game Theory
14-37:MIT Press, 1991
14-38:Y
14-39:Gal and A
14-40:Pfeffer
14-41:A language for modeling agent"s decision making processes in games
14-42:In AAMAS, 2003
14-43:P
14-44:Gmytrasiewicz and P
14-45:Doshi
14-46:A framework for sequential planning in multiagent settings
14-47:JAIR, 24:49 79, 2005
14-48:P
14-49:Gmytrasiewicz and E
14-50:Durfee
14-51:Rational coordination in multi agent environments
14-52:JAAMAS, 3(4):319 350, 2000
14-53:J
14-54:C
14-55:Harsanyi
14-56:Games with incomplete information played by bayesian players
14-57:Management Science, 14(3):159 182, 1967
14-58:R
14-59:A
14-60:Howard and J
14-61:E
14-62:Matheson
14-63:Influence diagrams
14-64:In R
14-65:A
14-66:Howard and J
14-67:E
14-68:Matheson, editors, The Principles and Applications of Decision Analysis
14-69:Strategic Decisions Group, Menlo Park, CA 94025, 1984
14-70:L
14-71:Kaelbling, M
14-72:Littman, and A
14-73:Cassandra
14-74:Planning and acting in partially observable stochastic domains
14-75:Artificial Intelligence Journal, 2, 1998
14-76:D
14-77:Koller and B
14-78:Milch
14-79:Multi agent influence diagrams for representing and solving games
14-80:In IJCAI, pages 1027 1034, 2001
14-81:K
14-82:Polich and P
14-83:Gmytrasiewicz
14-84:Interactive dynamic influence diagrams
14-85:In GTDT Workshop, AAMAS, 2006
14-86:B
14-87:Rathnas., P
14-88:Doshi, and P
14-89:J
14-90:Gmytrasiewicz
14-91:Exact solutions to interactive pomdps using behavioral equivalence
14-92:In Autonomous Agents and Multi Agent Systems Conference (AAMAS), 2006
14-93:S
14-94:Russell and P
14-95:Norvig
14-96:Artificial Intelligence: A Modern Approach (Second Edition)
14-97:Prentice Hall, 2003
14-98:R
14-99:D
14-100:Shachter
14-101:Evaluating influence diagrams
14-102:Operations Research, 34(6):871 882, 1986
14-103:D
14-104:Suryadi and P
14-105:Gmytrasiewicz
14-106:Learning models of other agents using influence diagrams
14-107:In UM, 1999
14-108:The Sixth Intl
14-109:Joint Conf
14-110:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 821
picture:
