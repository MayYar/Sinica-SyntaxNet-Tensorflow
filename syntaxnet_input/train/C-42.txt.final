Demonstration of Grid-Enabled Ensemble Kalman Filter 
content:
1 ABSTRACT :
1-1:Ensemble Kalman filter data assimilation methodology is a popular approach for hydrocarbon reservoir simulations in energy exploration .
1-2:In this approach, an ensemble of geological models and production data of oil fields is used to forecast the dynamic response of oil wells .
1-3:The Schlumberger ECLIPSE software is used for these simulations .
1-4:Since models in the ensemble do not communicate, message passing implementation is a good choice .
1-5:Each model checks out an ECLIPSE license and therefore, parallelizability of reservoir simulations depends on the number licenses available .
1-6:We have Grid enabled the ensemble Kalman filter data assimilation methodology for the TIGRE Grid computing environment .
1-7:By pooling the licenses and computing resources across the collaborating institutions using GridWay metascheduler and TIGRE environment, the computational accuracy can be increased while reducing the simulation runtime .
1-8:In this paper, we provide an account of our efforts in Gridenabling the ensemble Kalman Filter data assimilation methodology .
1-9:Potential benefits of this approach, observations and lessons learned will be discussed .
1-10:C 2.4 [Distributed Systems]: Distributed applications .
2 INTRODUCTION :
2-1:Grid computing [1] is an emerging collaborative computing paradigm to extend institution organization specific high performance computing capabilities greatly beyond local resources .
2-2:Its importance stems from the fact that ground breaking research in strategic application areas such as bioscience and medicine, energy exploration and environmental modeling involve strong interdisciplinary components and often require intercampus collaborations and computational capabilities beyond institutional limitations .
2-3:The Texas Internet Grid for Research and Education [2,3] is a state funded cyberinfrastructure development project carried out by five (Rice, A&M, TTU, UH and UT Austin) major university systems collectively called TIGRE Institutions .
2-4:The purpose of TIGRE is to create a higher education Grid to sustain and extend research and educational opportunities across Texas .
2-5:TIGRE is a project of the High Performance Computing across Texas (HiPCAT) [4] consortium .
2-6:The goal of HiPCAT is to support advanced computational technologies to enhance research, development, and educational activities .
2-7:The primary goal of TIGRE is to design and deploy state of the art Grid middleware that enables integration of computing systems, storage systems and databases, visualization laboratories and displays, and even instruments and sensors across Texas .
2-8:The secondary goal is to demonstrate the TIGRE capabilities to enhance research and educational opportunities in strategic application areas of interest to the State of Texas .
2-9:These are bioscience and medicine, energy exploration and air quality modeling .
2-10:Vision of the TIGRE project is to foster interdisciplinary and intercampus collaborations, identify novel approaches to extend academic government private partnerships, and become a competitive model for external funding opportunities .
2-11:The overall goal of TIGRE is to support local, campus and regional user interests and offer avenues to connect with national Grid projects such as Open Science Grid [5], and TeraGrid [6] .
2-12:Within the energy exploration strategic application area, we have Grid enabled the ensemble Kalman Filter (EnKF) [7] approach for data assimilation in reservoir modeling and demonstrated the extensibility of the application using the TIGRE environment and the GridWay [8] metascheduler .
2-13:Section 2 provides an overview of the TIGRE environment and capabilities .
2-14:Application description and the need for Grid enabling EnKF methodology is provided in Section 3 .
2-15:The implementation details and merits of our approach are discussed in Section 4 .
2-16:Conclusions are provided in Section in Section 6. .
3 TIGRE ENVIRONMENT :
3-1:The TIGRE Grid middleware consists of minimal set of components derived from a subset of the Virtual Data Toolkit [9] which supports a variety of operating systems .
3-2:The purpose of choosing a minimal software stack is to support applications at hand, and to simplify installation and distribution of client server stacks across TIGRE sites .
3-3:Additional components will be added as they become necessary .
3-4:The PacMan [10] packaging and distribution mechanism is employed for TIGRE client server installation and management .
3-5:The PacMan distribution mechanism involves retrieval, installation, and often configuration of the packaged software .
3-6:This approach allows the clients to keep current, consistent versions of TIGRE software .
3-7:It also helps TIGRE sites to install the needed components on resources distributed throughout the participating sites .
3-8:The TIGRE client server stack consists of an authentication and authorization layer, Globus GRAM4 based job submission via web services (pre web services installations are available up on request) .
3-9:The tools for handling Grid proxy generation, Grid enabled file transfer and Grid enabled remote login are supported .
3-10:The pertinent details of TIGRE services and tools for job scheduling and management are provided below .
3-11:2.1 .
3-12:Certificate Authority The TIGRE security infrastructure includes a certificate authority accredited by the International Grid Trust Federation for issuing X .
3-13:509 user and resource Grid certificates [11] .
3-14:The Texas Advanced Computing Center (TACC), University of Texas at Austin is the TIGRE"s shared CA .
3-15:The TIGRE Institutions serve as Registration Authorities for their respective local user base .
3-16:For up to date information on securing user and resource certificates and their installation instructions see ref [2] .
3-17:The users and hosts on TIGRE are identified by their distinguished name in their X.509 certificate provided by the CA .
3-18:A native Grid mapfile that contains a list of authorized DNs is used to authenticate and authorize user job scheduling and management on TIGRE site resources .
3-19:At Texas Tech University, the users are dynamically allocated one of the many generic pool accounts .
3-20:This is accomplished through the Grid User Management System [12] .
3-21:2.2 .
3-22:Job Scheduling and Management The TIGRE environment supports GRAM4 based job submission via web services .
3-23:The job submission scripts are generated using XML .
3-24:The web services GRAM translates the XML scripts into target cluster specific batch schedulers such as LSF, PBS, or SGE .
3-25:The high bandwidth file transfer protocols such as GridFTP are utilized for staging files in and out of the target machine .
3-26:The login to remote hosts for compilation and debugging is only through GSISSH service which requires resource authentication through X.509 certificates .
3-27:The authentication and authorization of Grid jobs are managed by issuing Grid certificates to both users and hosts .
3-28:The certificate revocation lists are updated on a daily basis to maintain high security standards of the TIGRE Grid services .
3-29:The TIGRE portal [2] documentation area provides a quick start tutorial on running jobs on TIGRE .
3-30:2.3 .
3-31:Metascheduler The metascheduler interoperates with the cluster level batch schedulers (such as LSF, PBS) in the overall Grid workflow management .
3-32:In the present work, we have employed GridWay [8] metascheduler a Globus incubator project to schedule and manage jobs across TIGRE .
3-33:The GridWay is a light weight metascheduler that fully utilizes Globus functionalities .
3-34:It is designed to provide efficient use of dynamic Grid resources by multiple users for Grid infrastructures built on top of Globus services .
3-35:The TIGRE site administrator can control the resource sharing through a powerful built in scheduler provided by GridWay or by extending GridWay"s external scheduling module to provide their own scheduling policies .
3-36:Application users can write job descriptions using GridWay"s simple and direct job template format (see Section 4 for details) or standard Job Submission Description Language .
3-37:See section 4 for implementation details .
3-38:2.4 .
3-39:Customer Service Management System A TIGRE portal [2] was designed and deployed to interface users and resource providers .
3-40:It was designed using GridPort [13] and is maintained by TACC .
3-41:The TIGRE environment is supported by open source tools such as the Open Ticket Request System [14] for servicing trouble tickets, and MoinMoin [15] Wiki for TIGRE content and knowledge management for education, outreach and training .
3-42:The links for OTRS and Wiki are consumed by the TIGRE portal [2] the gateway for users and resource providers .
3-43:The TIGRE resource status and loads are monitored by the Grid Port Information Repository service of the GridPort toolkit [13] which interfaces with local cluster load monitoring service such as Ganglia .
3-44:The GPIR utilizes cron jobs on each resource to gather site specific resource characteristics such as jobs that are running, queued and waiting for resource allocation. .
4 ENSEMBLE KALMAN FILTER APPLICATION :
4-1:APPLICATION The main goal of hydrocarbon reservoir simulations is to forecast the production behavior of oil and gas field (denoted as field hereafter) for its development and optimal management .
4-2:In reservoir modeling, the field is divided into several geological models as shown in Figure 1 .
4-3:For accurate performance forecasting of the field, it is necessary to reconcile several geological models to the dynamic response of the field through history matching [16 20] .
4-4:Figure 1 .
4-5:Cross sectional view of the Field .
4-6:Vertical layers correspond to different geological models and the nails are oil wells whose historical information will be used for forecasting the production behavior .
4-7:(Figure Ref:http: faculty.smu.edu zchen research.html) .
4-8:The EnKF is a Monte Carlo method that works with an ensemble of reservoir models .
4-9:This method utilizes crosscovariances [21] between the field measurements and the reservoir model parameters (derived from several models) to estimate prediction uncertainties .
4-10:The geological model parameters in the ensemble are sequentially updated with a goal to minimize the prediction uncertainties .
4-11:Historical production response of the field for over 50 years is used in these simulations .
4-12:The main advantage of EnKF is that it can be readily linked to any reservoir simulator, and can assimilate latest production data without the need to re run the simulator from initial conditions .
4-13:Researchers in Texas are large subscribers of the Schlumberger ECLIPSE [22] package for reservoir simulations .
4-14:In the reservoir modeling, each geological model checks out an ECLIPSE license .
4-15:The simulation runtime of the EnKF methodology depends on the number of geological models used, number of ECLIPSE licenses available, production history of the field, and propagated uncertainties in history matching .
4-16:The overall EnKF workflow is shown Figure 2 .
4-17:Figure 2 .
4-18:Ensemble Kaman Filter Data Assimilation Workflow .
4-19:Each site has L licenses .
4-20:At START, the master control process (EnKF main program) reads the simulation configuration file for number of models, and model specific input files .
4-21:Then, N working directories are created to store the output files .
4-22:At the end of iteration, the master control process collects the output files from N models and post processes crosscovariances [21] to estimate the prediction uncertainties .
4-23:This information will be used to update models (or input files) for the next iteration .
4-24:The simulation continues until the production histories are exhausted .
4-25:Typical EnKF simulation with N=50 and field histories of 50 60 years, in time steps ranging from three months to a year, takes about three weeks on a serial computing environment .
4-26:In parallel computing environment, there is no interprocess communication between the geological models in the ensemble .
4-27:However, at the end of each simulation time step, model specific output files are to be collected for analyzing cross covariances [21] and to prepare next set of input files .
4-28:Therefore, master slave model in messagepassing environment is a suitable paradigm .
4-29:In this approach, the geological models are treated as slaves and are distributed across the available processors .
4-30:The master Cluster or (TIGRE GridWay) START Read Configuration File Create N Working Directories Create N Input files Model l Model 2 Model N .
4-31:.
4-32:.
4-33:ECLIPSE on site A ECLIPSE on Site B ECLIPSE on Site Z Collect N Model Outputs, Post process Output files END .
4-34:.
4-35:.
4-36:process collects model specific output files, analyzes and prepares next set of input files for the simulation .
4-37:Since each geological model checks out an ECLIPSE license, parallelizability of the simulation depends on the number of licenses available .
4-38:When the available number of licenses is less than the number of models in the ensemble, one or more of the nodes in the MPI group have to handle more than one model in a serial fashion and therefore, it takes longer to complete the simulation .
4-39:A Petroleum Engineering Department usually procures 10 15 ECLIPSE licenses while at least ten fold increase in the number of licenses would be necessary for industry standard simulations .
4-40:The number of licenses can be increased by involving several Petroleum Engineering Departments that support ECLIPSE package .
4-41:Since MPI does not scale very well for applications that involve remote compute clusters, and to get around the firewall issues with license servers across administrative domains, Grid enabling the EnKF workflow seems to be necessary .
4-42:With this motivation, we have implemented Grid enabled EnKF workflow for the TIGRE environment and demonstrated parallelizability of the application across TIGRE using GridWay metascheduler .
4-43:Further details are provided in the next section. .
5 IMPLEMENTATION DETAILS :
5-1:To Grid enable the EnKF approach, we have eliminated the MPI code for parallel processing and replaced with N single processor jobs (or sub jobs) where, N is the number of geological models in the ensemble .
5-2:These model specific sub jobs were distributed across TIGRE sites that support ECLIPSE package using the GridWay [8] metascheduler .
5-3:For each sub job, we have constructed a GridWay job template that specifies the executable, input and output files, and resource requirements .
5-4:Since the TIGRE compute resources are not expected to change frequently, we have used static resource discovery policy for GridWay and the sub jobs were scheduled dynamically across the TIGRE resources using GridWay .
5-5:Figure 3 represents the sub job template file for the GridWay metascheduler .
5-6:Figure 3 .
5-7:GridWay Sub Job Template In Figure 3, REQUIREMENTS flag is set to choose the resources that satisfy the application requirements .
5-8:In the case of EnKF application, for example, we need resources that support ECLIPSE package .
5-9:ARGUMENTS flag specifies the model in the ensemble that will invoke ECLIPSE at a remote site .
5-10:INPUT_FILES is prepared by the EnKF main program (or master control process) and is transferred by GridWay to the remote site where it is untared and is prepared for execution .
5-11:Finally, OUTPUT_FILES specifies the name and location where the output files are to be written .
5-12:The command line features of GridWay were used to collect and process the model specific outputs to prepare new set of input files .
5-13:This step mimics MPI process synchronization in master slave model .
5-14:At the end of each iteration, the compute resources and licenses are committed back to the pool .
5-15:Table 1 shows the sub jobs in TIGRE Grid via GridWay using gwps command and for clarity, only selected columns were shown .
5-16:USER JID DM EM NAME HOST pingluo 88 wrap pend enkf.jt antaeus.hpcc.ttu.edu LSF pingluo 89 wrap pend enkf.jt antaeus.hpcc.ttu.edu LSF pingluo 90 wrap actv enkf.jt minigar.hpcc.ttu.edu LSF pingluo 91 wrap pend enkf.jt minigar.hpcc.ttu.edu LSF pingluo 92 wrap done enkf.jt cosmos.tamu.edu PBS pingluo 93 wrap epil enkf.jt cosmos.tamu.edu PBS Table 1 .
5-17:Job scheduling across TIGRE using GridWay Metascheduler .
5-18:DM: Dispatch state, EM: Execution state, JID is the job id and HOST corresponds to site specific cluster and its local batch scheduler .
5-19:When a job is submitted to GridWay, it will go through a series of dispatch and execution states .
5-20:For DM, the states include pend(ing), prol(og), wrap(per), epil(og), and done .
5-21:DM=prol means the job has been scheduled to a resource and the remote working directory is in preparation .
5-22:DM=warp implies that GridWay is executing the wrapper which in turn executes the application .
5-23:DM=epil implies the job has finished running at the remote site and results are being transferred back to the GridWay server .
5-24:Similarly, when EM=pend implies the job is waiting in the queue for resource and the job is running when EM=actv .
5-25:For complete list of message flags and their descriptions, see the documentation in ref [8] .
5-26:We have demonstrated the Grid enabled EnKF runs using GridWay for TIGRE environment .
5-27:The jobs are so chosen that the runtime doesn"t exceed more than a half hour .
5-28:The simulation runs involved up to 20 jobs between A&M and TTU sites with TTU serving 10 licenses .
5-29:For resource information, see Table I .
5-30:One of the main advantages of Grid enabled EnKF simulation is that both the resources and licenses are released back to the pool at the end of each simulation time step unlike in the case of MPI implementation where licenses and nodes are locked until the completion of entire simulation .
5-31:However, the fact that each sub job gets scheduled independently via GridWay could possibly incur another time delay caused by waiting in queue for execution in each simulation time step .
5-32:Such delays are not expected EXECUTABLE=runFORWARD REQUIREMENTS=HOSTNAME=cosmos.tamu.edu | HOSTNAME=antaeus.hpcc.ttu.edu | HOSTNAME=minigar.hpcc.ttu.edu | ARGUMENTS=001 INPUT_FILES=001.in.tar OUTPUT_FILES=001.out.tar in MPI implementation where the node is blocked for processing sub jobs (model specific calculation) until the end of the simulation .
5-33:There are two main scenarios for comparing Grid and cluster computing approaches .
5-34:Scenario I: The cluster is heavily loaded .
5-35:The conceived average waiting time of job requesting large number of CPUs is usually longer than waiting time of jobs requesting single CPU .
5-36:Therefore, overall waiting time could be shorter in Grid approach which requests single CPU for each sub job many times compared to MPI implementation that requests large number of CPUs at a single time .
5-37:It is apparent that Grid scheduling is beneficial especially when cluster is heavily loaded and requested number of CPUs for the MPI job is not readily available .
5-38:Scenario II: The cluster is relatively less loaded or largely available .
5-39:It appears the MPI implementation is favorable compared to the Grid scheduling .
5-40:However, parallelizability of the EnKF application depends on the number of ECLIPSE licenses and ideally, the number of licenses should be equal to the number of models in the ensemble .
5-41:Therefore, if a single institution does not have sufficient number of licenses, the cluster availability doesn"t help as much as it is expected .
5-42:Since the collaborative environment such as TIGRE can address both compute and software resource requirements for the EnKF application, Grid enabled approach is still advantageous over the conventional MPI implementation in any of the above scenarios. .
6 CONCLUSIONS AND FUTURE WORK :
6-1:TIGRE is a higher education Grid development project and its purpose is to sustain and extend research and educational opportunities across Texas .
6-2:Within the energy exploration application area, we have Grid enabled the MPI implementation of the ensemble Kalman filter data assimilation methodology for reservoir characterization .
6-3:This task was accomplished by removing MPI code for parallel processing and replacing with single processor jobs one for each geological model in the ensemble .
6-4:These single processor jobs were scheduled across TIGRE via GridWay metascheduler .
6-5:We have demonstrated that by pooling licenses across TIGRE sites, more geological models can be handled in parallel and therefore conceivably better simulation accuracy .
6-6:This approach has several advantages over MPI implementation especially when a site specific cluster is heavily loaded and or the number licenses required for the simulation is more than those available at a single site .
6-7:Towards the future work, it would be interesting to compare the runtime between MPI, and Grid implementations for the EnKF application .
6-8:This effort could shed light on quality of service (QoS) of Grid environments in comparison with cluster computing .
6-9:Another aspect of interest in the near future would be managing both compute and license resources to address the job (or processor) to license ratio management. .
7 OBSERVATIONS AND LESSIONS LEARNED :
7-1:LEARNED The Grid enabling efforts for EnKF application have provided ample opportunities to gather insights on the visibility and promise of Grid computing environments for application development and support .
7-2:The main issues are industry standard data security and QoS comparable to cluster computing .
7-3:Since the reservoir modeling research involves proprietary data of the field, we had to invest substantial efforts initially in educating the application researchers on the ability of Grid services in supporting the industry standard data security through role and privilege based access using X.509 standard .
7-4:With respect to QoS, application researchers expect cluster level QoS with Grid environments .
7-5:Also, there is a steep learning curve in Grid computing compared to the conventional cluster computing .
7-6:Since Grid computing is still an emerging technology, and it spans over several administrative domains, Grid computing is still premature especially in terms of the level of QoS although, it offers better data security standards compared to commodity clusters .
7-7:It is our observation that training and outreach programs that compare and contrast the Grid and cluster computing environments would be a suitable approach for enhancing user participation in Grid computing .
7-8:This approach also helps users to match their applications and abilities Grids can offer .
7-9:In summary, our efforts through TIGRE in Grid enabling the EnKF data assimilation methodology showed substantial promise in engaging Petroleum Engineering researchers through intercampus collaborations .
7-10:Efforts are under way to involve more schools in this effort .
7-11:These efforts may result in increased collaborative research, educational opportunities, and workforce development through graduate faculty research programs across TIGRE Institutions. .
8-1:The authors acknowledge the State of Texas for supporting the TIGRE project through the Texas Enterprise Fund, and TIGRE Institutions for providing the mechanism, in which the authors (Ravi Vadapalli, Taesung Kim, and Ping Luo) are also participating
8-2:The authors thank the application researchers Prof
8-3:Akhil Datta Gupta of Texas A&M University and Prof
8-4:Lloyd Heinze of Texas Tech University for their discussions and interest to exploit the TIGRE environment to extend opportunities in research and development.
9-1:Foster, I
9-2:and Kesselman, C
9-3:(eds.) 2004
9-4:The Grid: Blueprint for a new computing infrastructure (The Elsevier series in Grid computing) TIGRE Portal: http:  tigreportal.hipcat.net Vadapalli, R
9-5:Sill, A., Dooley, R., Murray, M., Luo, P., Kim, T., Huang, M., Thyagaraja, K., and Chaffin, D
9-6:2007
9-7:Demonstration of TIGRE environment for Grid enabled suitable applications
9-8:8th IEEE ACM Int
9-9:Conf
9-10:on Grid Computing, Sept 19 21, Austin The High Performance Computing across Texas Consortium http:  www.hipcat.net Pordes, R
9-11:Petravick, D
9-12:Kramer, B
9-13:Olson, D
9-14:Livny, M
9-15:Roy, A
9-16:Avery, P
9-17:Blackburn, K
9-18:Wenaus, T
9-19:Würthwein, F
9-20:Foster, I
9-21:Gardner, R
9-22:Wilde, M
9-23:Blatecky, A
9-24:McGee, J
9-25:and Quick, R
9-26:2007
9-27:The Open Science Grid, J
9-28:Phys Conf Series http:  www.iop.org EJ abstract 1742 6596 78 1 012057 and http:  www.opensciencegrid.org Reed, D.A
9-29:2003
9-30:Grids, the TeraGrid and Beyond, Computer, vol 30, no
9-31:1 and http:  www.teragrid.org Evensen, G
9-32:2006
9-33:Data Assimilation: The Ensemble Kalman Filter, Springer Herrera, J
9-34:Huedo, E
9-35:Montero, R
9-36:S
9-37:and Llorente, I
9-38:M
9-39:2005
9-40:Scientific Programming, vol 12, No
9-41:4
9-42:pp 317 331 Avery, P
9-43:and Foster, I
9-44:2001
9-45:The GriPhyN project: Towards petascale virtual data grids, technical report GriPhyN 200115 and http:  vdt.cs.wisc.edu The PacMan documentation and installation guide http:  physics.bu.edu pacman htmls Caskey, P
9-46:Murray, M
9-47:Perez, J
9-48:and Sill, A
9-49:2007
9-50:Case studies in identify management for virtual organizations, EDUCAUSE Southwest Reg
9-51:Conf., Feb 21 23, Austin, TX
9-52:http:  www.educause.edu ir library pdf SWR07058.pdf The Grid User Management System (GUMS) https:  www.racf.bnl.gov Facility GUMS index.html Thomas, M
9-53:and Boisseau, J
9-54:2003
9-55:Building grid computing portals: The NPACI grid portal toolkit, Grid computing: making the global infrastructure a reality, Chapter 28, Berman, F
9-56:Fox, G
9-57:Thomas, M
9-58:Boisseau, J
9-59:and Hey, T
9-60:(eds), John Wiley and Sons, Ltd, Chichester Open Ticket Request System http:  otrs.org The MoinMoin Wiki Engine http:  moinmoin.wikiwikiweb.de Vasco, D.W
9-61:Yoon, S
9-62:and Datta Gupta, A
9-63:1999
9-64:Integrating dynamic data into high resolution reservoir models using streamline based analytic sensitivity coefficients, Society of Petroleum Engineers (SPE) Journal, 4 (4)
9-65:Emanuel, A
9-66:S
9-67:and Milliken, W
9-68:J
9-69:1998
9-70:History matching finite difference models with 3D streamlines, SPE 49000, Proc of the Annual Technical Conf and Exhibition, Sept 2730, New Orleans, LA
9-71:Nævdal, G
9-72:Johnsen, L.M
9-73:Aanonsen, S.I
9-74:and Vefring, E.H
9-75:2003
9-76:Reservoir monitoring and Continuous Model Updating using Ensemble Kalman Filter, SPE 84372, Proc of the Annual Technical Conf and Exhibition, Oct 5 8, Denver, CO
9-77:Jafarpour B
9-78:and McLaughlin, D.B
9-79:2007
9-80:History matching with an ensemble Kalman filter and discrete cosine parameterization, SPE 108761, Proc of the Annual Technical Conf and Exhibition, Nov 11 14, Anaheim, CA Li, G
9-81:and Reynolds, A
9-82:C
9-83:2007
9-84:An iterative ensemble Kalman filter for data assimilation, SPE 109808, Proc of the SPE Annual Technical Conf and Exhibition, Nov 11 14, Anaheim, CA Arroyo Negrete, E
9-85:Devagowda, D
9-86:Datta Gupta, A
9-87:2006
9-88:Streamline assisted ensemble Kalman filter for rapid and continuous reservoir model updating
9-89:Proc of the Int
9-90:Oil & Gas Conf and Exhibition, SPE 104255, Dec 5 7, China ECLIPSE Reservoir Engineering Software http:  www.slb.com content services software reseng index.a sp
picture:
