Dynamics Based Control 
content:
1 ABSTRACT :
1-1:In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments .
1-2:Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics .
1-3:We show that a recently developed planning and control approach, Extended Markov Tracking is an instantiation of DBC .
1-4:EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments .
1-5:We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area sweeping problems (searching for moving targets) .
1-6:We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation .
1-7:I.2.8 [Problem Solving, Control Methods, and Search]: .
2 INTRODUCTION :
2-1:Planning and control constitutes a central research area in multiagent systems and artificial intelligence .
2-2:In recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments .
2-3:In this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility .
2-4:While theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7] .
2-5:We take an alternative view of planning in stochastic environments .
2-6:We do not use a (state based) reward function, but instead optimize over a different criterion, a transition based specification of the desired system dynamics .
2-7:The idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria .
2-8:We call this general planning framework Dynamics Based Control .
2-9:In DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics .
2-10:As actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed loop controllers [16] .
2-11:Here, optimality is measured in terms of probability of deviation magnitudes .
2-12:In this paper, we present the structure of Dynamics Based Control .
2-13:We show that the recently developed Extended Markov Tracking approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC .
2-14:EMT is an efficient instantiation of DBC .
2-15:To evaluate DBC, we carried out a set of experiments applying multi target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty .
2-16:Experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agent"s position) .
2-17:The paper is organized as follows .
2-18:In Section 2 we motivate DBC using area sweeping problems, and discuss related work .
2-19:Section 3 introduces the Dynamics Based Control structure, and its specialization to Markovian environments .
2-20:This is followed by a review of the Extended Markov Tracking approach as a DBC structured control regimen in Section 4 .
2-21:That section also discusses the limitations of EMT based control relative to the general DBC framework .
2-22:Experimental settings and results are then presented in Section 5 .
2-23:Section 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work .
2-24:790 978 81 904262 7 5 c 2007 IFAAMAS .
3 MOTIVATION AND RELATED WORK :
3-1:Many real life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported .
3-2:For example, security guards perform persistent sweeps of an area to detect any sign of intrusion .
3-3:Cunning thieves will attempt to track these sweeps, and time their operation to key points of the guards" motion .
3-4:It is thus advisable to make the guards" motion dynamics appear irregular and random .
3-5:Recent work by Paruchuri et al .
3-6:[10] has addressed such randomization in the context of single agent and distributed POMDPs .
3-7:The goal in that work was to generate policies that provide a measure of action selection randomization, while maintaining rewards within some acceptable levels .
3-8:Our focus differs from this work in that DBC does not optimize expected rewards indeed we do not consider rewards at all but instead maintains desired dynamics (including, but not limited to, randomization) .
3-9:The Game of Tag is another example of the applicability of the approach .
3-10:It was introduced in the work by Pineau et al .
3-11:[11] .
3-12:There are two agents that can move about an area, which is divided into a grid .
3-13:The grid may have blocked cells (holes) into which no agent can move .
3-14:One agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co located (this is a successful tag) .
3-15:The quarry seeks to avoid the hunter agent, and is always aware of the hunter"s position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey .
3-16:The hunter knows the quarry"s probabilistic law of motion, but does not know its current location .
3-17:Tag is an instance of a family of area sweeping (pursuit evasion) problems .
3-18:In [11], the hunter modeled the problem using a POMDP .
3-19:A reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time .
3-20:Due to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation .
3-21:In this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics .
3-22:In fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics .
3-23:Dynamics Based Control provides a natural approach to solving these problems. .
4 DYNAMICS BASED CONTROL :
4-1:The specification of Dynamics Based Control can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level .
4-2:• Environment Design Level is concerned with the formal specification and modeling of the environment .
4-3:For example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant .
4-4:• User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe .
4-5:The User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation .
4-6:In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping .
4-7:• Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification .
4-8:As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time .
4-9:To this end, we use a probability threshold that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold .
4-10:Specific action selection then depends on system formalization .
4-11:One possibility would be to create a mixture of available system trends, much like that which happens in Behavior Based Robotic architectures [1] .
4-12:The other alternative would be to rely on the estimation procedure provided by the User Level to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved .
4-13:Notice that this manipulation is not direct, but via the environment .
4-14:Thus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input) .
4-15:DBC levels can also have a back flow of information (see Figure 1) .
4-16:For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior .
4-17:Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level .
4-18:UserEnv .
4-19:Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm .
4-20:For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior .
4-21:In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update .
4-22:In fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9] .
4-23:3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner .
4-24:Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: S is the set of all possible environment states; s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl .
4-25:Joint Conf .
4-26:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 791 A is the set of all possible actions applicable in the environment; T is the environment"s probabilistic transition function: T : S ×A → Π(S) .
4-27:That is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; O is the set of all possible observations .
4-28:This is what the sensor input would look like for an outside observer; Ω is the observation probability function: Ω : S × A × S → Π(O) .
4-29:That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a .
4-30:• User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {τ : S × A → Π(S)} .
4-31:Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics .
4-32:There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: Trace distance or L1 distance between two distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) Kullback Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations .
4-33:For instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p .
4-34:The User Level also defines the threshold of dynamics deviation probability θ .
4-35:• Agent Level is then faced with a problem of selecting a control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q .
4-36:Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level .
4-37:3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment .
4-38:POMDPs regard state as a stationary snap shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation .
4-39:This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism the POMDP policy .
4-40:DBC concentrates on the underlying principle of state sequencing, the system dynamics .
4-41:DBC"s target dynamics specification can use the environment"s state space as a means to describe, discern, and preserve changes that occur within the system .
4-42:As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition .
4-43:For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible .
4-44:POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion .
4-45:Alternatively, the state space could directly include the notion of speed .
4-46:For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation .
4-47:Now, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure .
4-48:On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation .
4-49:In this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. .
5 EMT BASED CONTROL AS A DBC :
5-1:Recently, a control algorithm was introduced called EMT based Control [13], which instantiates the DBC framework .
5-2:Although it provides an approximate greedy solution in the DBC sense, initial experiments using EMT based control have been encouraging [14, 15] .
5-3:EMT based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality .
5-4:• User Level of EMT based control defines a limited case target system dynamics independent of action: qEMT : S → Π(S) .
5-5:It then utilizes the Kullback Leibler divergence measure to compose a momentary system dynamics estimator the Extended Markov Tracking algorithm .
5-6:The algorithm keeps a system dynamics estimate τt EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt−1 to pt, and updates it conservatively using Kullback Leibler divergence .
5-7:Since τt EMT and pt−1,t are respectively the conditional and marginal probabilities over the system"s state space, explanation simply means that pt(s ) = s τt EMT (s |s)pt−1(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl .
5-8:Joint Conf .
5-9:on Autonomous Agents and Multi Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs .
5-10:Dynamics Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, Ω >,where S set of states A set of actions Design T : S × A → Π(S) transition O observation set Ω : S × A × S → Π(O) User r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r reward function q ideal dynamics F reward remodeling L dynamics estimator θ threshold Agent π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) minimization problem: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t .
5-11:pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • Agent Level in EMT based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMT"s reaction .
5-12:The prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environment"s transition function limited to action a, and pt−1 is the auxiliary Bayesian system state estimator, then the EMT based control choice is described by a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics .
5-13:Yet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2) .
5-14:4.1 Multi Target EMT At times, there may exist several behavioral preferences .
5-15:For example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity .
5-16:On the other hand, no corner of the museum is to be left unchecked, which demands constant motion .
5-17:Successful museum security would demand that the guards adhere to, and balance, both of these behaviors .
5-18:For EMT based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them .
5-19:A balancing mechanism can be applied to resolve this issue .
5-20:Note that EMT based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target .
5-21:If these preference vectors are normalized, they can be combined into a single unified preference .
5-22:This requires replacement of standard EMT based action selection by the algorithm below [15]: • Given: a set of target dynamics {qk}K k=1, vector of weights w(k) • Select action as follows For each action a ∈ A predict the future state distribution ¯pa t+1 = Ta ∗ pt; For each action, compute Da = H(¯pa t+1, pt, PDt) For each a ∈ A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .
5-23:Let Vk(a) = 1 Zk V (a, k), where Zk = a∈A V (a, k) is a normalization factor .
5-24:Select a∗ = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves .
5-25:This balancing method is also seamlessly integrated into the EMT based control flow of operation .
5-26:4.2 EMT based Control Limitations EMT based control is a sub optimal (in the DBC sense) representative of the DBC structure .
5-27:It limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection .
5-28:This kind of combination, however, is common for on line algorithms .
5-29:Although further development of EMT based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word .
5-30:There are two further, EMT specific, limitations to EMT based control that are evident at this point .
5-31:Both already have partial solutions and are subjects of ongoing research .
5-32:The first limitation is the problem of negative preference .
5-33:In the POMDP framework for example, this is captured simply, through The Sixth Intl .
5-34:Joint Conf .
5-35:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure .
5-36:For EMT based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT based control, however, concentrates on getting as close as possible to a distribution .
5-37:Avoidance is thus unnatural in native EMT based control .
5-38:The second limitation comes from the fact that standard environment modeling can create pure sensory actions actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received .
5-39:Since the world state does not change, EMT based control would not be able to differentiate between different sensory actions .
5-40:Notice that both of these limitations of EMT based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference. .
6 EMT PLAYING TAG :
6-1:The Game of Tag was first introduced in [11] .
6-2:It is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems .
6-3:An example domain is shown in Figure 2 .
6-4:0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent attempts to seek and capture a quarry The Game of Tag extremely limits the agent"s perception, so that the agent is able to detect the quarry only if they are co located in the same cell of the grid world .
6-5:In the classical version of the game, co location leads to a special observation, and the ‘Tag" action can be performed .
6-6:We slightly modify this setting: the moment that both agents occupy the same cell, the game ends .
6-7:As a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West .
6-8:These form a formal space of actions within a Markovian environment .
6-9:The state space of the formal Markovian environment is described by the cross product of the agent and quarry"s positions .
6-10:For Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23} .
6-11:The effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry .
6-12:With probability q0 1 it stays put, and with probability 1 − q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2 .
6-13:agent .
6-14:So for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarry"s position is not .
6-15:The only sensory information available to the agent is its own location .
6-16:We use EMT and directly specify the target dynamics .
6-17:For the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry .
6-18:This results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics .
6-19:We ran several experiments to evaluate EMT performance in the Tag Game .
6-20:Three configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability .
6-21:In each setting, a set of 1000 runs was performed with a time limit of 100 steps .
6-22:In every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarry"s initial position was uniformly distributed over the entire domain cell space .
6-23:We also used two variations of the environment observability function .
6-24:In the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation .
6-25:In the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunter"s location .
6-26:The second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends .
6-27:The results of these experiments are shown in Table 2 .
6-28:Balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains .
6-29:Although direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis `a vis the POMDP approach .
6-30:In spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments .
6-31:For the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours .
6-32:That is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11] .
6-33:The significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation .
6-34:We also tested the behavior cell frequency entropy, empirical measures from trial data .
6-35:As Figure 4 and Figure 5 show, empir794 The Sixth Intl .
6-36:Joint Conf .
6-37:on Autonomous Agents and Multi Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunter"s position Model Domain Capture% E(Steps) Time Step I Dead ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction .
6-38:For runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios .
6-39:As the agent actively seeks the quarry, the entropy never reaches its maximum .
6-40:One characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model .
6-41:Near the maximum limit of trial length (100 steps), entropy suddenly dropped .
6-42:Further analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry following behavior .
6-43:Since the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells .
6-44:It is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics .
6-45:This behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action essentially contributing to tie breaking among them. .
7 DISCUSSION :
7-1:The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other .
7-2:POMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization .
7-3:EMT discards any reward scheme, and instead measures and influences system dynamics directly .
7-4:2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains .
7-5:Thus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agent"s behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained .
7-6:Experimental data shows that these targets need not be directly achievable via the agent"s actions .
7-7:However, the ratio between EMT performance and achievability of target dynamics remains to be explored .
7-8:The tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space .
7-9:POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing .
7-10:DBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics .
7-11:The importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie breaking and commitment to directed motion .
7-12:The structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche .
7-13:For instance, POMDPs could be seen as a much more natural formulation of economic sequential decision making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied agent problems .
7-14:The complementary properties of POMDPs and EMT can be further exploited .
7-15:There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself .
7-16:DBC can be an effective partner in such a hybrid solution .
7-17:For instance, POMDPs have prohibitively large off line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on line operation. .
8 CONCLUSIONS AND FUTURE WORK :
8-1:In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control framework .
8-2:DBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment .
8-3:Optimality of DBC plans of action are measured The Sixth Intl .
8-4:Joint Conf .
8-5:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry .
8-6:Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead end, b) irregular open arena, c) circular corridor .
8-7:0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead−ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunter"s position .
8-8:Entropy development with length of Tag Game for the three experiment scenarios: a) multiple dead end, b) irregular open arena, c) circular corridor .
8-9:with respect to the deviation of actual system dynamics from the target dynamics .
8-10:We show that a recently developed technique of Extended Markov Tracking [13] is an instantiation of DBC .
8-11:In fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure .
8-12:Since EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics based descriptions, as instantiated by our experiments in the Tag Game domain .
8-13:As enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference .
8-14:This prevents direct application of EMT to such problems as Rendezvous Evasion Games (e.g., [6]) .
8-15:However, DBC in general has no such limitations, and readily enables the formulation of evasion games .
8-16:In future work, we intend to proceed with the development of dynamics based controllers for these problems. .
9-1:The work of the first two authors was partially supported by Israel Science Foundation grant #898 05, and the third author was partially supported by a grant from Israel"s Ministry of Science and Technology.
10-1:R
10-2:C
10-3:Arkin
10-4:Behavior Based Robotics
10-5:MIT Press, 1998
10-6:J
10-7:A
10-8:Bilmes
10-9:A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models
10-10:Technical Report TR 97 021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998
10-11:T
10-12:M
10-13:Cover and J
10-14:A
10-15:Thomas
10-16:Elements of information theory
10-17:Wiley, 1991
10-18:M
10-19:E
10-20:desJardins, E
10-21:H
10-22:Durfee, C
10-23:L
10-24:Ortiz, and M
10-25:J
10-26:Wolverton
10-27:A survey of research in distributed, continual planning
10-28:AI Magazine, 4:13 22, 1999
10-29:V
10-30:R
10-31:Konda and J
10-32:N
10-33:Tsitsiklis
10-34:Actor Critic algorithms
10-35:SIAM Journal on Control and Optimization, 42(4):1143 1166, 2003
10-36:W
10-37:S
10-38:Lim
10-39:A rendezvous evasion game on discrete locations with joint randomization
10-40:Advances in Applied Probability, 29(4):1004 1017, December 1997
10-41:M
10-42:L
10-43:Littman, T
10-44:L
10-45:Dean, and L
10-46:P
10-47:Kaelbling
10-48:On the complexity of solving Markov decision problems
10-49:In Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI 95), pages 394 402, 1995
10-50:O
10-51:Madani, S
10-52:Hanks, and A
10-53:Condon
10-54:On the undecidability of probabilistic planning and related stochastic optimization problems
10-55:Artificial Intelligence Journal, 147(1 2):5 34, July 2003
10-56:R
10-57:M
10-58:Neal and G
10-59:E
10-60:Hinton
10-61:A view of the EM algorithm 796 The Sixth Intl
10-62:Joint Conf
10-63:on Autonomous Agents and Multi Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants
10-64:In M
10-65:I
10-66:Jordan, editor, Learning in Graphical Models, pages 355 368
10-67:Kluwer Academic Publishers, 1998
10-68:P
10-69:Paruchuri, M
10-70:Tambe, F
10-71:Ordonez, and S
10-72:Kraus
10-73:Security in multiagent systems by policy randomization
10-74:In Proceeding of AAMAS 2006, 2006
10-75:J
10-76:Pineau, G
10-77:Gordon, and S
10-78:Thrun
10-79:Point based value iteration: An anytime algorithm for pomdps
10-80:In International Joint Conference on Artificial Intelligence (IJCAI), pages 1025 1032, August 2003
10-81:M
10-82:L
10-83:Puterman
10-84:Markov Decision Processes
10-85:Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section
10-86:Wiley Interscience Publication, New York, 1994
10-87:Z
10-88:Rabinovich and J
10-89:S
10-90:Rosenschein
10-91:Extended Markov Tracking with an application to control
10-92:In The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95 100, New York, July 2004
10-93:Z
10-94:Rabinovich and J
10-95:S
10-96:Rosenschein
10-97:Multiagent coordination by Extended Markov Tracking
10-98:In The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431 438, Utrecht, The Netherlands, July 2005
10-99:Z
10-100:Rabinovich and J
10-101:S
10-102:Rosenschein
10-103:On the response of EMT based control to interacting targets and models
10-104:In The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465 470, Hakodate, Japan, May 2006
10-105:R
10-106:F
10-107:Stengel
10-108:Optimal Control and Estimation
10-109:Dover Publications, 1994
10-110:M
10-111:Tambe, E
10-112:Bowring, H
10-113:Jung, G
10-114:Kaminka, R
10-115:Maheswaran, J
10-116:Marecki, J
10-117:Modi, R
10-118:Nair, J
10-119:Pearce, P
10-120:Paruchuri, D
10-121:Pynadath, P
10-122:Scerri, N
10-123:Schurr, and P
10-124:Varakantham
10-125:Conflicts in teamwork: Hybrids to the The Sixth Intl
10-126:Joint Conf
10-127:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 797
picture:
