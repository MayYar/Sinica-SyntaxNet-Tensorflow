A Framework for Agent-Based Distributed Machine 
content:
1 ABSTRACT :
1-1:This paper proposes a framework for agent based distributed machine learning and data mining based on (i) the exchange of meta level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents .
1-2:We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes .
1-3:This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent based computing, and enables agents to engage in meta reasoning about their own learning decisions .
1-4:We apply this architecture to a real world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms .
1-5:We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework .
1-6:General Terms Theory .
2 2. ABSTRACT ARCHITECTURE :
2-1:I.2.11 [Artificial Intelligence]: Distributed Artificial In the areas of machine learning and data mining (cf .
2-2:[14, 17] for overviews), it has long been recognised that parallelisation and distribution can be used to improve learning performance .
2-3:Various techniques have been suggested in this respect, ranging from the low level integration of independently derived learning hypotheses (e.g .
2-4:combining different classifiers to make optimal classification decisions [4, 7], model averaging of Bayesian classifiers [8], or consensusbased methods for integrating different clusterings [11]), to the high level combination of learning results obtained by heterogeneous learning agents using meta learning (e.g .
2-5:[3, 10, 21]) .
2-6:All of these approaches assume homogeneity of agent design (all agents apply the same learning algorithm) and or agent objectives (all agents are trying to cooperatively solve a single, global learning problem) .
2-7:Therefore, the techniques they suggest are not applicable in societies of autonomous learners interacting in open systems .
2-8:In such systems, learners (agents) may not be able to integrate their datasets or learning results (because of different data formats and representations, learning algorithms, or legal restrictions that prohibit such integration [11]) and cannot always be guaranteed to interact in a strictly cooperative fashion (discovered knowledge and collected data might be economic assets that should only be shared when this is deemed profitable; malicious agents might attempt to adversely influence others" learning results, etc.) .
2-9:Examples for applications of this kind abound .
2-10:Many distributed learning domains involve the use of sensitive data and prohibit the exchange of this data (e.g .
2-11:exchange of patient data in distributed brain tumour diagnosis [2]) however, they may permit the exchange of local learning hypotheses among different learners .
2-12:In other areas, training data might be commercially valuable, so that agents would only make it available to others if those agents could provide something in return (e.g .
2-13:in remote ship surveillance and tracking, where the different agencies involved are commercial service providers [1]) .
2-14:Furthermore, agents might have a vested interest in negatively affecting other agents" learning performance .
2-15:An example for this is that of fraudulent agents on eBay which may try to prevent reputationlearning agents from the construction of useful models for detecting fraud .
2-16:Viewing learners as autonomous, self directed agents is the only appropriate view one can take in modelling these distributed learning environments: the agent metaphor becomes a necessity as oppossed to preferences for scalability, dynamic data selection, interactivity [13], which can also be achieved through (non agent) distribution and parallelisation in principle .
2-17:Despite the autonomy and self directedness of learning agents, many of these systems exhibit a sufficient overlap in terms of individual learning goals so that beneficial cooperation might be possible if a model for flexible interaction between autonomous learners was available that allowed agents to own learning mechanism at different levels of detail without being forced to reveal private information that should not be disclosed, about their own learning processes and utilise information provided by other learners, and improve their own learning performance .
2-18:Our model is based on the simple idea that autonomous learners should maintain meta descriptions of their own learning processes (see also [3]) in order to be able to exchange information and reason about them in a rational way (i.e .
2-19:with the overall objective of improving their own learning results) .
2-20:Our hypothesis is a very simple one: If we can devise a sufficiently general, abstract view of describing learning processes, we will be able to utilise the whole range of methods for (i) rational reasoning and (ii) communication and coordination offered by agent technology so as to build effective autonomous learning agents .
2-21:To test this hypothesis, we introduce such an abstract architecture (section 2) and implement a simple, concrete instance of it in a real world domain (section 3) .
2-22:We report on empirical results obtained with this implemented system that demonstrate the viability of our approach (section 4) .
2-23:Finally, we review related work (section 5) and conclude with a summary, discussion of our approach and outlook to future work on the subject (section 6). .
3 ABSTRACT ARCHITECTURE :
3-1:Our framework is based on providing formal (meta level) descriptions of learning processes, i.e .
3-2:representations of all relevant components of the learning machinery used by a learning agent, together with information about the state of the learning process .
3-3:To ensure that this framework is sufficiently general, we consider the following general description of a learning problem: Given data D ⊆ D taken from an instance space D, a hypothesis space H and an (unknown) target function c ∈ H1 , derive a function h ∈ H that approximates c as well as possible according to some performance measure g : H → Q where Q is a set of possible levels of learning performance .
3-4:1 By requiring this we are ensuring that the learning problem can be solved in principle using the given hypothesis space .
3-5:This very broad definition includes a number of components of a learning problem for which more concrete specifications can be provided if we want to be more precise .
3-6:For the cases of classification and clustering, for example, we can further specify the above as follows: Learning data can be described in both cases as D = ×n i=1[Ai] where [Ai] is the domain of the ith attribute and the set of attributes is A = {1, .
3-7:.
3-8:.
3-9:, n} .
3-10:For the hypothesis space we obtain H ⊆ {h|h : D → {0, 1}} in the case of classification (i.e .
3-11:a subset of the set of all possible classifiers, the nature of which depends on the expressivity of the learning algorithm used) and H ⊆ {h|h : D → N, h is total with range {1, .
3-12:.
3-13:.
3-14:, k}} in the case of clustering (i.e .
3-15:a subset of all sets of possible cluster assignments that map data points to a finite number of clusters numbered 1 to k) .
3-16:For classification, g might be defined in terms of the numbers of false negatives and false positives with respect to some validation set V ⊆ D, and clustering might use various measures of cluster validity to evaluate the quality of a current hypothesis, so that Q = R in both cases (but other sets of learning quality levels can be imagined) .
3-17:Next, we introduce a notion of learning step, which imposes a uniform basic structure on all learning processes that are supposed to exchange information using our framework .
3-18:For this, we assume that each learner is presented with a finite set of data D = d1, .
3-19:.
3-20:.
3-21:dk in each step (this is an ordered set to express that the order in which the samples are used for training matters) and employs a training update function f : H × D∗ → H which updates h given a series of samples d1, .
3-22:.
3-23:.
3-24:, dk .
3-25:In other words, one learning step always consists of applying the update function to all samples in D exactly once .
3-26:We define a learning step as a tuple l = D, H, f, g, h where we require that H ⊆ H and h ∈ H .
3-27:The intuition behind this definition is that each learning step completely describes one learning iteration as shown in Figure 1: in step t, the learner updates the current hypothesis ht−1 with data Dt, and evaluates the resulting new hypothesis ht according to the current performance measure gt .
3-28:Such a learning step is equivalent to the following steps of computation: calculate ft(ht−1, Dt) = ht, gt(ht) .
3-29:We denote the set of all possible learning steps by L .
3-30:For ease of notation, we denote the components of any l ∈ L by D(l), H(l), f(l) and g(l), respectively .
3-31:The reason why such learning step specifications use a subset H of H instead of H itself is that learners often have explicit knowledge about which hypotheses are effectively ruled out by f given h in the future (if this is not the case, we can still set H = H) .
3-32:A learning process is a finite, non empty sequence l = l1 → l2 → .
3-33:.
3-34:.
3-35:→ ln of learning steps such that ∀1 ≤ i < n .h(li+1) = f(li)(h(li), D(li)) The Sixth Intl .
3-36:Joint Conf .
3-37:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 679 training function ht performance measure solution quality qtgtft training set Dt hypothesis hypothesis ht−1 Figure 1: A generic model of a learning step i.e .
3-38:the only requirement the transition relation →⊆ L × L makes is that the new hypothesis is the result of training the old hypothesis on all available sample data that belongs to the current step .
3-39:We denote the set of all possible learning processes by L (ignoring, for ease of notation, the fact that this set depends on H, D and the spaces of possible training and evaluation functions f and g) .
3-40:The performance trace associated with a learning process l is the sequence q1, .
3-41:.
3-42:.
3-43:, qn ∈ Qn where qi = g(li)(h(li)), i.e .
3-44:the sequence of quality values calculated by the performance measures of the individual learning steps on the respective hypotheses .
3-45:Such specifications allow agents to provide a selfdescription of their learning process .
3-46:However, in communication among learning agents, it is often useful to provide only partial information about one"s internal learning process rather than its full details, e.g .
3-47:when advertising this information in order to enter information exchange negotiations with others .
3-48:For this purpose, we will assume that learners describe their internal state in terms of sets of learning processes (in the sense of disjunctive choice) which we call learning process descriptions (LPDs) rather than by giving precise descriptions about a single, concrete learning process .
3-49:This allows us to describe properties of a learning process without specifying its details exhaustively .
3-50:As an example, the set {l ∈ L|∀l = l[i].D(l) ≤ 100} describes all processes that have a training set of at most 100 samples (where all the other elements are arbitrary) .
3-51:Likewise, {l ∈ L|∀l = l[i].D(l) = {d}} is equivalent to just providing information about a single sample {d} and no other details about the process (this can be useful to model, for example, data received from the environment) .
3-52:Therefore, we use ℘(L), that is the set of all LPDs, as the basis for designing content languages for communication in the protocols we specify below .
3-53:In practice, the actual content language chosen will of course be more restricted and allow only for a special type of subsets of L to be specified in a compact way, and its choice will be crucial for the interactions that can occur between learning agents .
3-54:For our examples below, we simply assume explicit enumeration of all possible elements of the respective sets and function spaces (D, H, etc.) extended by the use of wildcard symbols ∗ (so that our second example above would become ({d}, ∗, ∗, ∗, ∗)) .
3-55:2.1 Learning agents In our framework, a learning agent is essentially a metareasoning function that operates on information about learning processes and is situated in an environment co inhabited by other learning agents .
3-56:This means that it is not only capable of meta level control on how to learn, but in doing so it can take information into account that is provided by other agents or the environment .
3-57:Although purely cooperative or hybrid cases are possible, for the purposes of this paper we will assume that agents are purely self interested, and that while there may be a potential for cooperation considering how agents can mutually improve each others" learning performance, there is no global mechanism that can enforce such cooperative behaviour.2 Formally speaking, an agent"s learning function is a function which, given a set of histories of previous learning processes (of oneself and potentially of learning processes about which other agents have provided information) and outputs a learning step which is its next learning action .
3-58:In the most general sense, our learning agent"s internal learning process update can hence be viewed as a function λ : ℘(L) → L × ℘(L) which takes a set of learning histories of oneself and others as inputs and computes a new learning step to be executed while updating the set of known learning process histories (e.g .
3-59:by appending the new learning action to one"s own learning process and leaving all information about others" learning processes untouched) .
3-60:Note that in λ({l1, .
3-61:.
3-62:.
3-63:ln}) = (l, {l1, .
3-64:.
3-65:.
3-66:ln }) some elements li of the input learning process set may be descriptions of new learning data received from the environment .
3-67:The λ function can essentially be freely chosen by the agent as long as one requirement is met, namely that the learning data that is being used always stems from what has been previously observed .
3-68:More formally, ∀{l1, .
3-69:.
3-70:.
3-71:ln} ∈ ℘(L).λ({l1, .
3-72:.
3-73:.
3-74:ln}) = (l, {l1, .
3-75:.
3-76:.
3-77:ln }) ⇒ „ D(l) ∪ [ l =li[j] D(l ) « ⊆ [ l =li[j] D(l ) i.e .
3-78:whatever λ outputs as a new learning step and updated set of learning histories, it cannot invent new data; it has to work with the samples that have been made available to it earlier in the process through the environment or from other agents (and it can of course re train on previously used data) .
3-79:The goal of the agent is to output an optimal learning step in each iteration given the information that it has .
3-80:One possibility of specifying this is to require that ∀{l1, .
3-81:.
3-82:.
3-83:ln} ∈ ℘(L).λ({l1, .
3-84:.
3-85:.
3-86:ln}) = (l, {l1, .
3-87:.
3-88:.
3-89:ln }) ⇒ l = arg max l ∈L g(l )(h(l )) but since it will usually be unrealistic to compute the optimal next learning step in every situation, it is more useful 2 Note that our outlook is not only different from common, cooperative models of distributed machine learning and data mining, but also delineates our approach from multiagent learning systems in which agents learn about other agents [25], i.e .
3-90:the learning goal itself is not affected by agents" behaviour in the environment .
3-91:680 The Sixth Intl .
3-92:Joint Conf .
3-93:on Autonomous Agents and Multi Agent Systems (AAMAS 07) i j Dj Hj fj gj hj Di pD→D 1 (Di, Dj) .
3-94:. .
3-95:pD→D kD→D (Di, Dj) .
3-96:.
3-97:.
3-98:.
3-99:.
3-100:.
3-101:n a .
3-102:.
3-103:.
3-104:Hi .
3-105:. .
3-106:.. .
3-107:n a fi .
3-108:. .
3-109:.. .
3-110:n a gi .
3-111:3.1 Domain description As an illustration of our framework, we present an agentbased data mining system for clustering based surveillance using AIS (Automatic Identification System [1]) data .
3-112:In our application domain, different commercial and governmental agencies track the journeys of ships over time using AIS data which contains structured information automatically provided by ships equipped with shipborne mobile AIS stations to shore stations, other ships and aircrafts .
3-113:This data contains the ship"s identity, type, position, course, speed, navigational status and other safety related information .
3-114:Figure 2 shows a screenshot of our simulation system .
3-115:It is the task of AIS agencies to detect anomalous behaviour so as to alarm police coastguard units to further investigate unusual, potentially suspicious behaviour .
3-116:Such behaviour might include things such as deviation from the standard routes between the declared origin and destination of the journey, unexpected close encounters between different vessels on sea, or unusual patterns in the choice of destination over multiple journeys, taking the type of vessel and reported freight into account .
3-117:While the reasons for such unusual behaviour may range from pure coincidence or technical problems to criminal activity (such as smuggling, piracy, terrorist military attacks) it is obviously useful to pre process the huge amount of vessel (tracking) data that is available before engaging in further analysis by human experts .
3-118:To support this automated pre processing task, software used by these agencies applies clustering methods in order to identify outliers and flag those as potentially suspicious entities to the human user .
3-119:However, many agencies active in this domain are competing enterprises and use their (partially overlapping, but distinct) datasets and learning hypotheses (models) as assets and hence cannot be expected to collaborate in a fully cooperative way to improve overall learning results .
3-120:Considering that this is the reality of the domain in the real world, it is easy to see that a framework like the one we have suggested above might be useful to exploit the cooperation potential that is not exploited by current systems .
3-121:3.2 Agent based distributed learning system design To describe a concrete design for the AIS domain, we need to specify the following elements of the overall system: individual agents, descriptions of learning processes, and decisions .
3-122:Regarding 1., our agents are equipped with their own private datasets in the form of vessel descriptions .
3-123:Learning samples are represented by tuples containing data about individual vessels in terms of attributes A = {1, .
3-124:.
3-125:.
3-126:, n} including things such as width, length, etc .
3-127:with real valued domains ([Ai] = R for all i) .
3-128:In terms of learning algorithm, we consider clustering with a fixed number of k clusters using the k means and k medoids clustering algorithms [5] (fixed meaning that the learning algorithm will always output k clusters; however, we allow agents to change the value of k over different learning cycles) .
3-129:This means that the hypothesis space can be defined as H = { c1, .
3-130:.
3-131:.
3-132:, ck |ci ∈ R|A| } i.e .
3-133:the set of all possible sets of k cluster centroids in |A| dimensional Euclidean space .
3-134:For each hypothesis h = c1, .
3-135:.
3-136:.
3-137:, ck and any data point d ∈ ×n i=1[Ai] given domain [Ai] for the ith attribute of each sample, the assignment to clusters is given by C( c1, .
3-138:.
3-139:.
3-140:, ck , d) = arg min 1≤j≤k |d − cj| i.e .
3-141:d is assigned to that cluster whose centroid is closest to the data point in terms of Euclidean distance .
3-142:For evaluation purposes, each dataset pertaining to a particular agent i is initially split into a training set Di and a validation Vi .
3-143:Then, we generate a set of fake vessels Fi such that |Fi| = |Vi| .
3-144:These two sets assess the agent"s ability to detect suspicious vessels .
3-145:For this, we assign a confidence value r(h, d) to every ship d: r(h, d) = 1 |d − cC(h,d)| where C(h, d) is the index of the nearest centroid .
3-146:Based on this measure, we classify any vessel in Fi ∪ Vi as fake if its r value is below the median of all the confidences r(h, d) for d ∈ Fi ∪ Vi .
3-147:With this, we can compute the quality gi(h) ∈ R as the ratio between all correctly classified vessels and all vessels in Fi ∪ Vi .
3-148:As concerns 2., we use a simple Contract Net Protocol [20] based hypothesis trading mechanism: Before each learning iteration, agents issue (publicly broadcasted) Calls For Proposals (CfPs), advertising their own numerical model quality .
3-149:In other words, the initiator of a CNP describes its own current learning state as (∗, ∗, ∗, gi(h), ∗) where h is their current hypothesis model .
3-150:We assume that agents are sincere when advertising their model quality, but note that this quality might be of limited relevance to other agents as they may specialise on specific regions of the data space not related to the test set of the sender of the CfP .
3-151:Subsequently, (some) agents may issue bids in which they advertise, in turn, the quality of their own model .
3-152:If the 682 The Sixth Intl .
3-153:Joint Conf .
3-154:on Autonomous Agents and Multi Agent Systems (AAMAS 07) bids (if any) are accepted by the initiator of the protocol who issued the CfP, the agents exchange their hypotheses and the next learning iteration ensues .
3-155:To describe what is necessary for 3., we have to specify (i) under which conditions agents submit bids in response to a CfP, (ii) when they accept bids in the CNP negotiation process, and (iii) how they integrate the received information in their own learning process .
3-156:Concerning (i) and (ii), we employ a very simple rule that is identical in both cases: let g be one"s own model quality and g that advertised by the CfP (or highest bid, respectively) .
3-157:If g > g we respond to the CfP (accept the bid), else respond to the CfP (accept the bid) with probability P(g g) and ignore (reject) it else .
3-158:If two agents make a deal, they exchange their learning hypotheses (models) .
3-159:In our experiments, g and g are calculated by an additional agent that acts as a global validation mechanism for all agents (in a more realistic setting a comparison mechanism for different g functions would have to be provided) .
3-160:As for (iii), each agent uses a single model merging operator taken from the following two classes of operators (hj is the receiver"s own model and hi is the provider"s model): • ph→h (hi, hj) : m join: The m best clusters (in terms of coverage of Dj) from hypothesis hi are appended to hj .
3-161:m select: The set of the m best clusters (in terms of coverage of Dj) from the union hi ∪hj is chosen as a new model .
3-162:(Unlike m join this method does not prefer own clusters over others".) • ph→D (hi, Dj) : m filter: The m best clusters (as above) from hi are identified and appended to a new model formed by using those samples not covered by these clusters applying the own learning algorithm fj .
3-163:Whenever m is large enough to encompass all clusters, we simply write join or filter for them .
3-164:In section 4 we analyse the performance of each of these two classes for different choices of m .
3-165:It is noteworthy that this agent based distributed data mining system is one of the simplest conceivable instances of our abstract architecture .
3-166:While we have previously applied it also to a more complex market based architecture using Inductive Logic Programming learners in a transport logistics domain [22], we believe that the system described here is complex enough to illustrate the key design decisions involved in using our framework and provides simple example solutions for these design issues. .
4 EXPERIMENTAL RESULTS :
4-1:Figure 3 shows results obtained from simulations with three learning agents in the above system using the k means and k medoids clustering methods respectively .
4-2:We partition the total dataset of 300 ships into three disjoint sets of 100 samples each and assign each of these to one learning agent .
4-3:The Single Agent is learning from the whole dataset .
4-4:The parameter k is set to 10 as this is the optimal value for the total dataset according to the Davies Bouldin index [9] .
4-5:For m select we assume m = k which achieves a constant Figure 3: Performance results obtained for different integration operations in homogeneous learner societies using the k means (top) and k medoids (bottom) methods model size .
4-6:For m join and m filter we assume m = 3 to limit the extent to which models increase over time .
4-7:During each experiment the learning agents receive ship descriptions in batches of 10 samples .
4-8:Between these batches, there is enough time to exchange the models among the agents and recompute the models if necessary .
4-9:Each ship is described using width, length, draught and speed attributes with the goal of learning to detect which vessels have provided fake descriptions of their own properties .
4-10:The validation set contains 100 real and 100 randomly generated fake ships .
4-11:To generate sufficiently realistic properties for fake ships, their individual attribute values are taken from randomly selected ships in the validation set (so that each fake sample is a combination of attribute values of several existing ships) .
4-12:In these experiments, we are mainly interested in investigating whether a simple form of knowledge sharing between self interested learning agents could improve agent performance compared to a setting of isolated learners .
4-13:Thereby, we distinguish between homogeneous learner societies where all agents use the same clustering algorithm and heterogeneous ones where different agents use different algorithms .
4-14:As can be seen from the performance plots in Figure 3 (homogeneous case) and 4 (heterogeneous case, two agents use the same method and one agent uses the other) this is clearly the case for the (unrestricted) join and filter integration operations (m = k) in both cases .
4-15:This is quite natural, as these operations amount to sharing all available model knowledge among agents (under appropriate constraints depending on how beneficial the exchange seems to the agents) .
4-16:We can see that the quality of these operations is very close to the Single Agent that has access to all training data .
4-17:For the restricted (m < k) m join, m filter and m select methods we can also observe an interesting distinction, The Sixth Intl .
4-18:Joint Conf .
4-19:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 683 Figure 4: Performance results obtained for different integration operations in heterogeneous societies with the majority of learners using the k means (top) and k medoids (bottom) methods namely that these perform similarly to the isolated learner case in homogeneous agent groups but better than isolated learners in more heterogeneous societies .
4-20:This suggests that heterogeneous learners are able to benefit even from rather limited knowledge sharing (and this is what using a rather small m = 3 amounts to given that k = 10) while this is not always true for homogeneous agents .
4-21:This nicely illustrates how different learning or data mining algorithms can specialise on different parts of the problem space and then integrate their local results to achieve better individual performance .
4-22:Apart from these obvious performance benefits, integrating partial learning results can also have other advantages: The m filter operation, for example, decreases the number of learning samples and thus can speed up the learning process .
4-23:The relative number of filtered examples measured in our experiments is shown in the following table .
4-24:k means k medoids filtering 30 40 % 10 20 % m filtering 20 30 % 5 15 % The overall conclusion we can draw from these initial experiments with our architecture is that since a very simplistic application of its principles has proven capable of improving the performance of individual learning agents, it is worthwhile investigating more complex forms of information exchange about learning processes among autonomous learners. .
5 RELATED WORK :
5-1:We have already mentioned work on distributed (nonagent) machine learning and data mining in the introductory chapter, so in this section we shall restrict ourselves to approaches that are more closely related to our outlook on distributed learning systems .
5-2:Very often, approaches that are allegedly agent based completely disregard agent autonomy and prescribe local decision making procedures a priori .
5-3:A typical example for this type of system is the one suggested by Caragea et al .
5-4:[6] which is based on a distributed support vector machine approach where agents incrementally join their datasets together according to a fixed distributed algorithm .
5-5:A similar example is the work of Weiss [24], where groups of classifier agents learn to organise their activity so as to optimise global system behaviour .
5-6:The difference between this kind of collaborative agentbased learning systems [16] and our own framework is that these approaches assume a joint learning goal that is pursued collaboratively by all agents .
5-7:Many approaches rely heavily on a homogeneity assumption: Plaza and Ontanon [15] suggest methods for agentbased intelligent reuse of cases in case based reasoning but is only applicable to societies of homogeneous learners (and coined towards a specific learning method) .
5-8:An agentbased method for integrating distributed cluster analysis processes using density estimation is presented by Klusch et al .
5-9:[13] which is also specifically designed for a particular learning algorithm .
5-10:The same is true of [22, 23] which both present market based mechanisms for aggregating the output of multiple learning agents, even though these approaches consider more interesting interaction mechanisms among learners .
5-11:A number of approaches for sharing learning data [18] have also been proposed: Grecu and Becker [12] suggest an exchange of learning samples among agents, and Ghosh et al .
5-12:[11] is a step in the right direction in terms of revealing only partial information about one"s learning process as it deals with limited information sharing in distributed clustering .
5-13:Papyrus [3] is a system that provides a markup language for meta description of data, hypotheses and intermediate results and allows for an exchange of all this information among different nodes, however with a strictly cooperative goal of distributing the load for massively distributed data mining tasks .
5-14:The MALE system [19] was a very early multiagent learning system in which agents used a blackboard approach to communicate their hypotheses .
5-15:Agents were able to critique each others" hypotheses until agreement was reached .
5-16:However, all agents in this system were identical and the system was strictly cooperative .
5-17:The ANIMALS system [10] was used to simulate multistrategy learning by combining two or more learning techniques (represented by heterogeneous agents) in order to overcome weaknesses in the individual algorithms, yet it was also a strictly cooperative system .
5-18:As these examples show and to the best of our knowledge, there have been no previous attempts to provide a framework that can accommodate both independent and heterogeneous learning agents and this can be regarded as the main contribution of our work. .
6-1:In this paper, we outlined a generic, abstract framework for distributed machine learning and data mining
6-2:This framework constitutes, to our knowledge, the first attempt 684 The Sixth Intl
6-3:Joint Conf
6-4:on Autonomous Agents and Multi Agent Systems (AAMAS 07) to capture complex forms of interaction between heterogeneous and or self interested learners in an architecture that can be used as the foundation for implementing systems that use complex interaction and reasoning mechanisms to enable agents to inform and improve their learning abilities with information provided by other learners in the system, provided that all agents engage in a sufficiently similar learning activity
6-5:To illustrate that the abstract principles of our architecture can be turned into concrete, computational systems, we described a market based distributed clustering system which was evaluated in the domain of vessel tracking for purposes of identifying deviant or suspicious behaviour
6-6:Although our experimental results only hint at the potential of using our architecture, they underline that what we are proposing is feasible in principle and can have beneficial effects even in its most simple instantiation
6-7:Yet there is a number of issues that we have not addressed in the presentation of the architecture and its empirical evaluation: Firstly, we have not considered the cost of communication and made the implicit assumption that the required communication comes for free
6-8:This is of course inadequate if we want to evaluate our method in terms of the total effort required for producing a certain quality of learning results
6-9:Secondly, we have not experimented with agents using completely different learning algorithms (e.g
6-10:symbolic and numerical)
6-11:In systems composed of completely different agents the circumstances under which successful information exchange can be achieved might be very different from those described here, and much more complex communication and reasoning methods may be necessary to achieve a useful integration of different agents" learning processes
6-12:Finally, more sophisticated evaluation criteria for such distributed learning architectures have to be developed to shed some light on what the right measures of optimality for autonomously reasoning and communicating agents should be
6-13:These issues, together with a more systematic and thorough investigation of advanced interaction and communication mechanisms for distributed, collaborating and competing agents will be the subject of our future work on the subject
6-14:Acknowledgement: We gratefully acknowledge the support of the presented research by Army Research Laboratory project N62558 03 0819 and Office for Naval Research project N00014 06 1 0232.
7-1:http:  www.aislive.com
7-2:http:  www.healthagents.com
7-3:S
7-4:Bailey, R
7-5:Grossman, H
7-6:Sivakumar, and A
7-7:Turinsky
7-8:Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super Clusters
7-9:In Proc
7-10:of the Conference on Supercomputing
7-11:1999
7-12:E
7-13:Bauer and R
7-14:Kohavi
7-15:An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants
7-16:Machine Learning, 36, 1999
7-17:P
7-18:Berkhin
7-19:Survey of Clustering Data Mining Techniques, Technical Report, Accrue Software, 2002
7-20:D
7-21:Caragea, A
7-22:Silvescu, and V
7-23:Honavar
7-24:Agents that Learn from Distributed Dynamic Data sources
7-25:In Proc
7-26:of the Workshop on Learning Agents, 2000
7-27:N
7-28:Chawla and S
7-29:E
7-30:abd L
7-31:O
7-32:Hall
7-33:Creating ensembles of classifiers
7-34:In Proceedings of ICDM 2001, pages 580 581, San Jose, CA, USA, 2001
7-35:D
7-36:Dash and G
7-37:F
7-38:Cooper
7-39:Model Averaging for Prediction with Discrete Bayesian Networks
7-40:Journal of Machine Learning Research, 5:1177 1203, 2004
7-41:D
7-42:L
7-43:Davies and D
7-44:W
7-45:Bouldin
7-46:A Cluster Separation Measure
7-47:IEEE Transactions on Pattern Analysis and Machine Intelligence, 4:224 227, 1979
7-48:P
7-49:Edwards and W
7-50:Davies
7-51:A Heterogeneous Multi Agent Learning System
7-52:In Proceedings of the Special Interest Group on Cooperating Knowledge Based Systems, pages 163 184, 1993
7-53:J
7-54:Ghosh, A
7-55:Strehl, and S
7-56:Merugu
7-57:A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing
7-58:In NSF Workshop on Next Generation Data Mining, 99 108, 2002
7-59:D
7-60:L
7-61:Grecu and L
7-62:A
7-63:Becker
7-64:Coactive Learning for Distributed Data Mining
7-65:In Proceedings of KDD 98, pages 209 213, New York, NY, August 1998
7-66:M
7-67:Klusch, S
7-68:Lodi, and G
7-69:Moro
7-70:Agent based distributed data mining: The KDEC scheme
7-71:In AgentLink, number 2586 in LNCS
7-72:Springer, 2003
7-73:T
7-74:M
7-75:Mitchell
7-76:Machine Learning, pages 29 36
7-77:McGraw Hill, New York, 1997
7-78:S
7-79:Ontanon and E
7-80:Plaza
7-81:Recycling Data for Multi Agent Learning
7-82:In Proc
7-83:of ICML 05, 2005
7-84:L
7-85:Panait and S
7-86:Luke
7-87:Cooperative multi agent learning: The state of the art
7-88:Autonomous Agents and Multi Agent Systems, 11(3):387 434, 2005
7-89:B
7-90:Park and H
7-91:Kargupta
7-92:Distributed Data Mining: Algorithms, Systems, and Applications
7-93:In N
7-94:Ye, editor, Data Mining Handbook, pages 341 358, 2002
7-95:F
7-96:J
7-97:Provost and D
7-98:N
7-99:Hennessy
7-100:Scaling up: Distributed machine learning with cooperation
7-101:In Proc
7-102:of AAAI 96, pages 74 79
7-103:AAAI Press, 1996
7-104:S
7-105:Sian
7-106:Extending learning to multiple agents: Issues and a model for multi agent machine learning (ma ml)
7-107:In Y
7-108:Kodratoff, editor, Machine LearningEWSL 91, pages 440 456
7-109:Springer Verlag, 1991
7-110:R
7-111:Smith
7-112:The contract net protocol: High level communication and control in a distributed problem solver
7-113:IEEE Transactions on Computers, C 29(12):1104 1113, 1980
7-114:S
7-115:J
7-116:Stolfo, A
7-117:L
7-118:Prodromidis, S
7-119:Tselepis, W
7-120:Lee, D
7-121:W
7-122:Fan, and P
7-123:K
7-124:Chan
7-125:Jam: Java Agents for Meta Learning over Distributed Databases
7-126:In Proc
7-127:of the KDD 97, pages 74 81, USA, 1997
7-128:J
7-129:Toˇziˇcka, M
7-130:Jakob, and M
7-131:Pˇechouˇcek
7-132:Market Inspired Approach to Collaborative Learning
7-133:In Cooperative Information Agents X (CIA 2006), volume 4149 of LNCS, pages 213 227
7-134:Springer, 2006
7-135:Y
7-136:Z
7-137:Wei, L
7-138:Moreau, and N
7-139:R
7-140:Jennings
7-141:Recommender systems: a market based design
7-142:In Proceedings of AAMAS 03), pages 600 607, 2003
7-143:G
7-144:Weiß
7-145:A Multiagent Perspective of Parallel and Distributed Machine Learning
7-146:In Proceedings of Agents"98, pages 226 230, 1998
7-147:G
7-148:Weiss and P
7-149:Dillenbourg
7-150:What is "multi" in multi agent learning? Collaborative learning: Cognitive and Computational Approaches, 64 80, 1999
7-151:The Sixth Intl
7-152:Joint Conf
7-153:on Autonomous Agents and Multi Agent Systems (AAMAS 07) 685
picture:
