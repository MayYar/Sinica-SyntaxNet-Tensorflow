Implementation of a Dynamic Adjustment Mechanism with 
content:
1 ABSTRACT :
1-1:The co allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers .
1-2:Several co allocation strategies have been coupled and used to exploit rate differences among various client server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes .
1-3:However, a major obstacle, the idle time of faster servers having to wait for the slowest server to deliver the final block, makes it important to reduce differences in finishing time among replica servers .
1-4:In this paper, we propose a dynamic coallocation scheme, namely Recursive Adjustment Co Allocation scheme, to improve the performance of data transfer in Data Grids .
1-5:Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time .
1-6:We also provide an effective scheme for reducing the cost of reassembling data blocks .
1-7:C.2.4 [Distributed Systems]: Distributed applications. .
2 INTRODUCTION :
2-1:Data Grids aggregate distributed resources for solving large size dataset management problems .
2-2:Most Data Grid applications execute simultaneously and access large numbers of data files in the Grid environment .
2-3:Certain data intensive scientific applications, such as high energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access .
2-4:The data grid infrastructure integrates data storage devices and data management services into the grid environment, which consists of scattered computing and storage resources, perhaps located in different countries regions yet accessible to users [12] .
2-5:Replicating popular content in distributed servers is widely used in practice [14, 17, 19] .
2-6:Recently, large scale, data sharing scientific communities such as those described in [1, 5] used this technology to replicate their large datasets over several sites .
2-7:Downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity .
2-8:Bandwidth quality is the most important factor affecting transfers between clients and servers since download speeds are limited by the bandwidth traffic congestion in the links connecting the servers to the clients .
2-9:One way to improve download speeds is to determine the best replica locations using replica selection techniques [19] .
2-10:This method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet .
2-11:Another way is to use co allocation technology [17] to download data .
2-12:Co allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel .
2-13:This can improve the performance compared to the single server cases and alleviate the internet congestion problem [17] .
2-14:Several co allocation strategies were provided in previous work [17] .
2-15:An idle time drawback remains since faster servers must wait for the slowest server to deliver its final block .
2-16:Therefore, it is important to reduce the differences in finishing time among replica servers .
2-17:In this paper, we propose a dynamic co allocation scheme based on co allocation Grid data transfer architecture called RecursiveAdjustment Co Allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance [24] .
2-18:Experimental results show that our approach is superior to previous methods and achieved the best overall performance .
2-19:We also discuss combination cost and provide an effective scheme for reducing it .
2-20:The remainder of this paper is organized as follows .
2-21:Related background review and studies are presented in Section 2 and the co allocation architecture and related work are introduced in Section 3 .
2-22:In Section 4, an efficient replica selection service is proposed by us .
2-23:Our research approaches are outlined in Section 5, and experimental results and a performance evaluation of our scheme are presented in Section 6 .
2-24:Section 7 concludes this research paper. .
3 BACKGROUND :
3-1:2.1 Data Grid The Data Grids enable the sharing, selection, and connection of a wide variety of geographically distributed computational and storage resources for solving large scale data intensive scientific applications (e.g., high energy physics, bioinformatics applications, and astrophysical virtual observatory) .
3-2:The term Data Grid traditionally represents the network of distributed storage resources, from archival systems to caches and databases, which are linked using a logical name space to create global, persistent identifiers and provide uniform access mechanisms [4] .
3-3:Data Grids [1, 2, 16] federate a lot of storage resources .
3-4:Large collections of measured or computed data are emerging as important resources in many data intensive applications .
3-5:2.1.1 Replica Management Replica management involves creating or removing replicas at a data grid site [19] .
3-6:In other words, the role of a replica manager is to create or delete replicas, within specified storage systems .
3-7:Most often, these replicas are exact copies of the original files, created only to harness certain performance benefits .
3-8:A replica manager typically maintains a replica catalog containing replica site addresses and the file instances .
3-9:The replica management service is responsible for managing the replication of complete and partial copies of datasets, defined as collections of files .
3-10:The replica management service is just one component in a Data Grid environment that provides support for high performance, data intensive applications .
3-11:A replica or location is a subset of a collection that is stored on a particular physical storage system .
3-12:There may be multiple possibly overlapping subsets of a collection stored on multiple storage systems in a Data Grid .
3-13:These Grid storage systems may use a variety of underlying storage technologies and data movement protocols, which are independent of replica management .
3-14:2.1.2 Replica Catalog As mentioned above, the purpose of the replica catalog is to provide mappings between logical names for files or collections and one or more copies of the objects on physical storage systems .
3-15:The replica catalog includes optional entries that describe individual logical files .
3-16:Logical files are entities with globally unique names that may have one or more physical instances .
3-17:The catalog may optionally contain one logical file entry in the replica catalog for each logical file in a collection .
3-18:A Data Grid may contain multiple replica catalogs .
3-19:For example, a community of researchers interested in a particular research topic might maintain a replica catalog for a collection of data sets of mutual interest .
3-20:It is possible to create hierarchies of replica catalogs to impose a directory like structure on related logical collections .
3-21:In addition, the replica manager can perform access control on entire catalogs as well as on individual logical files .
3-22:2.1.3 Replica Selection The purpose of replica selection [16] is to select a replica from among the sites which constitute a Data Grid [19] .
3-23:The criteria of selection depend on characteristics of the application .
3-24:By using this mechanism, users of the Data Grid can easily manage replicas of data sets at their sites, with better performance .
3-25:Much previous effort has been devoted to the replica selection problem .
3-26:The common process of replica selection consists of three steps: data preparation, preprocessing and prediction .
3-27:Then, applications can select a replica according to its specific attributes .
3-28:Replica selection is important to data intensive applications, and it can provide location transparency .
3-29:When a user requests for accessing a data set, the system determines an appropriate way to deliver the replica to the user .
3-30:2.2 Globus Toolkit and GridFTP The Globus Project [9, 11, 16] provides software tools collectively called The Globus Toolkit that makes it easier to build computational Grids and Grid based applications .
3-31:Many organizations use the Globus Toolkit to build computational Grids to support their applications .
3-32:The composition of the Globus Toolkit can be pictured as three pillars: Resource Management, Information Services, and Data Management .
3-33:Each pillar represents a primary component of the Globus Toolkit and makes use of a common foundation of security .
3-34:GRAM implements a resource management protocol, MDS implements an information services protocol, and GridFTP implements a data transfer protocol .
3-35:They all use the GSI security protocol at the connection layer [10, 11, 16, 13] .
3-36:The Globus alliance proposed a common data transfer and access protocol called GridFTP that provides secure, efficient data movement in Grid environments [3] .
3-37:This protocol, which extends the standard FTP protocol, provides a superset of the features offered by the various Grid storage systems currently in use .
3-38:In order to solve the appearing problems, the Data Grid community tries to develop a secure, efficient data transport mechanism and replica management services .
3-39:GridFTP is a reliable, secure and efficient data transport protocol which is developed as a part of the Globus project .
3-40:There is another key technology from Globus project, called replica catalog [16] which is used to register and manage complete and partial copies of data sets .
3-41:The replica catalog contains the mapping information from a logical file or collection to one or more physical files .
3-42:2.3 Network Weather Service The Network Weather Service [22] is a generalized and distributed monitoring system for producing short term performance forecasts based on historical performance measurements .
3-43:The goal of the system is to dynamically characterize and forecast the performance deliverable at the application level from a set of network and computational resources .
3-44:A typical installation involves one nws_nameserver, one or more nws_memory (which may reside on different machines), and an nws_sensor running on each machine with resources which are to be monitored .
3-45:The system includes sensors for end to end TCP IP performance (bandwidth and latency), available CPU percentage, and available non paged memory .
3-46:798 2.4 Sysstat Utilities The Sysstat [15] utilities are a collection of performance monitoring tools for the Linux OS .
3-47:The Sysstat package incorporates the sar, mpstat, and iostat commands .
3-48:The sar command collects and reports system activity information, which can also be saved in a system activity file for future inspection .
3-49:The iostat command reports CPU statistics and I O statistics for tty devices and disks .
3-50:The statistics reported by sar concern I O transfer rates, paging activity, process related activities, interrupts, network activity, memory and swap space utilization, CPU utilization, kernel activities, and tty statistics, among others .
3-51:Uniprocessor and Symmetric multiprocessor machines are fully supported. .
4 CO ALLOCATION ARCHITECTURE AND RELATED WORK :
4-1:AND RELATED WORK The co allocation architecture proposed in [17] consists of three main components: an information service, a broker co allocator, and local storage systems .
4-2:Figure 1 shows the co allocation of Grid Data transfers, which is an extension of the basic template for resource management [7] provided by Globus Toolkit .
4-3:Applications specify the characteristics of desired data and pass the attribute description to a broker .
4-4:The broker queries available resources and gets replica locations from information services [6] and replica management services [19], and then gets a list of physical locations for the desired files .
4-5:Figure 1 .
4-6:Data Grid Co Allocation Architecture [17] The candidate replica locations are passed to a replica selection service [19], which was presented in a previous work [23] .
4-7:This replica selection service provides estimates of candidate transfer performance based on a cost model and chooses appropriate amounts to request from the better locations .
4-8:The co allocation agent then downloads the data in parallel from the selected servers .
4-9:In these researches, GridFTP [1, 11, 16] was used to enable parallel data transfers .
4-10:GridFTP is a high performance, secure, reliable data transfer protocol optimized for high bandwidth widearea networks .
4-11:Among its many features are security, parallel streams, partial file transfers, third party transfers, and reusable data channels .
4-12:Its partial file transfer ability allows files to be retrieved from data servers by specifying the start and end offsets of file sections .
4-13:Data grids consist of scattered computing and storage resources located in different countries regions yet accessible to users [8] .
4-14:In this study we used the grid middleware Globus Toolkit [16] as the data grid infrastructure .
4-15:The Globus Toolkit provides solutions for such considerations as security, resource management, data management, and information services .
4-16:One of its primary components is MDS [6, 11, 16, 25], which is designed to provide a standard mechanism for discovering and publishing resource status and configuration information .
4-17:It provides a uniform and flexible interface for data collected by lower level information providers in two modes: static (e.g., OS, CPU types, and system architectures) and dynamic data (e.g., disk availability, memory availability, and loading) .
4-18:And it uses GridFTP [1, 11, 16], a reliable, secure, and efficient data transport protocol to provide efficient management and transfer of terabytes or petabytes of data in a wide area, distributed resource environment .
4-19:As datasets are replicated within Grid environments for reliability and performance, clients require the abilities to discover existing data replicas, and create and register new replicas .
4-20:A Replica Location Service [4] provides a mechanism for discovering and registering existing replicas .
4-21:Several prediction metrics have been developed to help replica selection .
4-22:For instance, Vazhkudai and Schopf [18, 20, 21] used past data transfer histories to estimate current data transfer throughputs .
4-23:In our previous work [23, 24], we proposed a replica selection cost model and a replica selection service to perform replica selection .
4-24:In [17], the author proposes co allocation architecture for co allocating Grid data transfers across multiple connections by exploiting the partial copy feature of GridFTP .
4-25:It also provides Brute Force, History Base, and Dynamic Load Balancing for allocating data block .
4-26:Brute Force Co Allocation: Brute Force Co Allocation works by dividing the file size equally among available flows .
4-27:It does not address the bandwidth differences among the various client server links .
4-28:History based Co Allocation: The History based CoAllocation scheme keeps block sizes per flow proportional to predicted transfer rates .
4-29:Conservative Load Balancing: One of their dynamic coallocation is Conservative Load Balancing .
4-30:The Conservative Load Balancing dynamic co allocation strategy divides requested datasets into k disjoint blocks of equal size .
4-31:Available servers are assigned single blocks to deliver in parallel .
4-32:When a server finishes delivering a block, another is requested, and so on, till the entire file is downloaded .
4-33:The loadings on the co allocated flows are automatically adjusted because the faster servers will deliver more quickly providing larger portions of the file .
4-34:Aggressive Load Balancing: Another dynamic coallocation strategy, presented in [17], is the Aggressive Load Balancing .
4-35:The Aggressive Load Balancing dynamic co allocation strategy presented in [17] adds functions that change block size de liveries by: (1) progressively increasing the amounts of data requested from faster servers, and (2) reducing the amounts of data requested from slower servers or ceasing to request data from them altogether .
4-36:The co allocation strategies described above do not handle the shortcoming of faster servers having to wait for the slowest server to deliver its final block .
4-37:In most cases, this wastes much time and decreases overall performance .
4-38:Thus, we propose an efficient approach called Recursive Adjustment Co Allocation and based 799 on a co allocation architecture .
4-39:It improves dynamic co allocation and reduces waiting time, thus improving overall transfer performance. .
5 AN EFFICIENT REPLICA SELECTION SERVICE :
5-1:SERVICE We constructed a replica selection service to enable clients to select the better replica servers in Data Grid environments .
5-2:See below for a detailed description .
5-3:4.1 Replica Selection Scenario Our proposed replica selection model is illustrated in [23], which shows how a client identifies the best location for a desired replica transfer .
5-4:The client first logins in at a local site and executes the Data Grid platform application, which checks to see if the files are available at the local site .
5-5:If they are present at the local site, the application accesses them immediately; otherwise, it passes the logical file names to the replica catalog server, which returns a list of physical locations for all registered copies .
5-6:The application passes this list of replica locations to a replica selection server, which identifies the storage system destination locations for all candidate data transfer operations .
5-7:The replica selection server sends the possible destination locations to the information server, which provides performance measurements and predictions of the three system factors described below .
5-8:The replica selection server chooses better replica locations according to these estimates and returns location information to the transfer application, which receives the replica through GridFTP .
5-9:When the application finishes, it returns the results to the user .
5-10:4.2 System Factors Determining the best database from many with the same replications is a significant problem .
5-11:In our model, we consider three system factors that affect replica selection: Network bandwidth: This is one of the most significant Data Grid factors since data files in Data Grid environments are usually very large .
5-12:In other words, data file transfer times are tightly dependent on network bandwidth situations .
5-13:Because network bandwidth is an unstable dynamic factor, we must measure it frequently and predict it as accurately as possible .
5-14:The Network Weather Service is a powerful toolkit for this purpose .
5-15:CPU load: Grid platforms consist of numbers of heterogeneous systems, built with different system architectures, e.g., cluster platforms, supercomputers, PCs .
5-16:CPU loading is a dynamic system factor, and a heavy system CPU load will certainly affect data file downloads process from the site .
5-17:The measurement of it is done by the Globus Toolkit MDS .
5-18:I O state: Data Grid nodes consist of different heterogeneous storage systems .
5-19:Data files in Data Grids are huge .
5-20:If the I O state of a site that we wish to download files from is very busy, it will directly affect data transfer performance .
5-21:We measure I O states using sysstat [15] utilities .
5-22:4.3 Our Replica Selection Cost Model The target function of a cost model for distributed and replicated data storage is the information score from the information service .
5-23:We listed some influencing factors for our cost model in the preceding section .
5-24:However, we must express these factors in mathematical notation for further analysis .
5-25:We assume node i is the local site the user or application logs in on, and node j possesses the replica the user or application wants .
5-26:The seven system parameters our replica selection cost model considers are: Scorei j: the score value represents how efficiently a user or application at node i can acquire a replica from node j BW jiP : percentage of bandwidth available from node i to node j; current bandwidth divided by highest theoretical bandwidth BBW : network bandwidth weight defined by the Data Grid administrator CPU jP : percentage of node j CPU idle states WCPU : CPU load weight defined by the Data Grid administrator OI jP : percentage of node j I O idle states WI O : I O state weight defined by the Data Grid administrator We define the following general formula using these system factors .
5-27:OIOI j CPUCPU j BWBW jiji WPWPWPScore (1) The three influencing factors in this formula: WBW , WCPU , and WI O describe CPU, I O, and network bandwidth weights, which can be determined by Data Grid organization administrators according to the various attributes of the storage systems in Data Grid nodes since some storage equipment does not affect CPU loading .
5-28:After several experimental measurements, we determined that network bandwidth is the most significant factor directly influencing data transfer times .
5-29:When we performed data transfers using the GridFTP protocol we discovered that CPU and I O statuses slightly affect data transfer performance .
5-30:Their respective values in our Data Grid environment are 80%, 10%, and 10% .
5-31:4.4 Co Allocation Cost Analysis When clients download datasets using GridFTP co allocation technology, three time costs are incurred: the time required for client authentication to the GridFTP server, actual data transmission time, and data block reassembly time .
5-32:Authentication Time: Before a transfer, the client must load a Globus proxy and authenticate itself to the GridFTP server with specified user credentials .
5-33:The client then establishes a control channel, sets up transfer parameters, and requests data channel creation .
5-34:When the channel has been established, the data begins flowing .
5-35:Transmission Time: Transmission time is measured from the time when the client starts transferring to the time when all transmission jobs are finished, and it includes the time 800 required for resetting data channels between transfer requests .
5-36:Data pathways need be opened only once and may handle many transfers before being closed .
5-37:This allows the same data pathways to be used for multiple file transfers .
5-38:However, data channels must be explicitly reset between transfer requests .
5-39:This is less time costly .
5-40:Combination Time: Co allocation architecture exploits the partial copy feature of the GridFTP data movement tool to enable data transfers across multiple connections .
5-41:With partial file transfer, file sections can be retrieved from data servers by specifying only the section start and end offsets .
5-42:When these file sections are delivered, they may need to be reassembled; the reassembly operation incurs an additional time cost. .
6 DYNAMIC CO ALLOCATION STRATEGY :
6-1:STRATEGY Dynamic co allocation, described above, is the most efficient approach to reducing the influence of network variations between clients and servers .
6-2:However, the idle time of faster servers awaiting the slowest server to deliver the last block is still a major factor affecting overall efficiency, which Conservative Load Balancing and Aggressive Load Balancing [17] cannot effectively avoid .
6-3:The approach proposed in the present paper, a dynamic allocation mechanism called Recursive Adjustment CoAllocation can overcome this, and thus, improve data transfer performance .
6-4:5.1 Recursive Adjustment Co Allocation Recursive Adjustment Co Allocation works by continuously adjusting each replica server"s workload to correspond to its realtime bandwidth during file transfers .
6-5:The goal is to make the expected finish time of all servers the same .
6-6:As Figure 2 shows, when an appropriate file section is first selected, it is divided into proper block sizes according to the respective server bandwidths .
6-7:The co allocator then assigns the blocks to servers for transfer .
6-8:At this moment, it is expected that the transfer finish time will be consistent at E(T1) .
6-9:However, since server bandwidths may fluctuate during segment deliveries, actual completion time may be dissimilar (solid line, in Figure 2) .
6-10:Once the quickest server finishes its work at time T1, the next section is assigned to the servers again .
6-11:This allows each server to finish its assigned workload by the expected time at E(T2) .
6-12:These adjustments are repeated until the entire file transfer is finished .
6-13:Server 1 Server 2 Server 3 Round 1 Round 2 E(T1) E(T2)T1 File A Section 1 Section 2 .. .
6-14:.. .
6-15:.. .
6-16:Figure 2 .
6-17:The adjustment process The Recursive Adjustment Co Allocation process is illustrated in Figure 3 .
6-18:When a user requests file A, the replica selection service responds with the subset of all available servers defined by the maximum performance matrix .
6-19:The co allocation service gets this list of selected replica servers .
6-20:Assuming n replica servers are selected, Si denotes server i such that 1 i n .
6-21:A connection for file downloading is then built to each server .
6-22:The RecursiveAdjustment Co Allocation process is as follows .
6-23:A new section of a file to be allocated is first defined .
6-24:The section size, SEj, is: SEj = UnassignedFileSize , (0 < < 1) (2) where SEj denotes the section j such that 1 j k, assuming we allocate k times for the download process .
6-25:And thus, there are k sections, while Tj denotes the time section j allocated .
6-26:UnassignedFileSize is the portion of file A not yet distributed for downloading; initially, UnassignedFileSize is equal to the total size of file A .
6-27:is the rate that determines how much of the section remains to be assigned .
6-28:Figure 3 .
6-29:The Recursive Adjustment Co Allocation process .
6-30:In the next step, SEj is divided into several blocks and assigned to n servers .
6-31:Each server has a real time transfer rate to the client of Bi, which is measured by the Network Weather Service [18] .
6-32:The block size per flow from SEj for each server i at time Tj is: i n i ii n i iji zeUnFinishSiBBzeUnFinishSiSES )( 11 (3) where UnFinishSizei denotes the size of unfinished transfer blocks that is assigned in previous rounds at server i .
6-33:UnFinishSizei is equal to zero in first round .
6-34:Ideally, depending to the real time bandwidth at time Tj, every flow is expected to finish its workload in future .
6-35:This fulfills our requirement to minimize the time faster servers must wait for the slowest server to finish .
6-36:If, in some cases, network variations greatly degrade transfer rates, UnFinishSizei may exceed n i ii n i ij BBzeUnFinishSiSE 11 *)( , which is the total block size expected to be transferred after Tj .
6-37:In such cases, the co allocator eliminates the servers in advance and assigns SEj to other servers .
6-38:After allocation, all channels continue transferring data blocks .
6-39:When a faster channel finishes its assigned data blocks, the co allocator begins allocating an unassigned section of file A again .
6-40:The process of allocating data 801 blocks to adjust expected flow finish time continues until the entire file has been allocated .
6-41:5.2 Determining When to Stop Continuous Adjustment Our approach gets new sections from whole files by dividing unassigned file ranges in each round of allocation .
6-42:These unassigned portions of the file ranges become smaller after each allocation .
6-43:Since adjustment is continuous, it would run as an endless loop if not limited by a stop condition .
6-44:However, when is it appropriate to stop continuous adjustment? We provide two monitoring criteria, LeastSize and ExpectFinishedTime, to enable users to define stop thresholds .
6-45:When a threshold is reached, the co allocation server stopped dividing the remainder of the file and assigns that remainder as the final section .
6-46:The LeastSize criterion specifies the smallest file we want to process, and when the unassigned portion of UnassignedFileSize drops below the LeastSize specification, division stops .
6-47:ExpectFinishedTime criterion specifies the remaining time transfer is expected to take .
6-48:When the expected transfer time of the unassigned portion of a file drops below the time specified by ExpectFinishedTime, file division stops .
6-49:The expected rest time value is determined by: 1 n i iBFileSizeUnAssigned (4) These two criteria determine the final section size allocated .
6-50:Higher threshold values will induce fewer divisions and yield lower co allocation costs, which include establishing connections, negotiation, reassembly, etc .
6-51:However, although the total coallocation adjustment time may be lower, bandwidth variations may also exert more influence .
6-52:By contrast, lower threshold values will induce more frequent dynamic server workload adjustments and, in the case of greater network fluctuations, result in fewer differences in server transfer finish time .
6-53:However, lower values will also increase co allocation times, and hence, increase co allocation costs .
6-54:Therefore, the internet environment, transferred file sizes, and co allocation costs should all be considered in determining optimum thresholds .
6-55:5.3 Reducing the Reassembly Overhead The process of reassembling blocks after data transfers using coallocation technology results in additional overhead and decreases overall performance .
6-56:The reassembly overhead is related to total block size, and could be reduced by upgrading hardware capabilities or using better software algorithms .
6-57:We propose an efficient alternative reassembly mechanism to reduce the added combination overhead after all block transmissions are finished .
6-58:It differs from the conventional method in which the software starts assembly after all blocks have been delivered by starting to assemble blocks once the first deliveries finish .
6-59:Of course, this makes it necessary to maintain the original splitting order .
6-60:Co allocation strategies such as Conservative Load Balancing and Recursive Adjustment Co Allocation produce additional blocks during file transfers and can benefit from enabling reassembly during data transfers .
6-61:If some blocks are assembled in advance, the time cost for assembling the blocks remaining after all transfers finish can be reduced. .
7 EXPERIMENTAL RESULTS AND ANALYSIS :
7-1:ANALYSIS In this section, we discuss the performance of our RecursiveAdjustment Co Allocation strategy .
7-2:We evaluate four coallocation schemes: (1) Brute Force (Brute), (2) History based (History), (3) Conservative Load Balancing (Conservative) and (4) Recursive Adjustment Co Allocation (Recursive) .
7-3:We analyze the performance of each scheme by comparing their transfer finish time, and the total idle time faster servers spent waiting for the slowest server to finish delivering the last block .
7-4:We also analyze the overall performances in the various cases .
7-5:We performed wide area data transfer experiments using our GridFTP GUI client tool .
7-6:We executed our co allocation client tool on our testbed at Tunghai University (THU), Taichung City, Taiwan, and fetched files from four selected replica servers: one at Providence University (PU), one at Li Zen High School (LZ), one at Hsiuping Institute of Technology School (HIT), and one at Da Li High School .
7-7:All these institutions are in Taiwan, and each is at least 10 Km from THU .
7-8:Figure 4 shows our Data Grid testbed .
7-9:Our servers have Globus 3.0.2 or above installed .
7-10:Internet THU Li Zen High School HITCeleron 900 MHz 256 MB RAM 60 GB HD AMD Athlon(tm) XP 2400+ 1024 MB RAM 120 GB HD Pentium 4 2.8 GHz 512 MB RAM 80 GB HD PU Da Li High School Athlon MP 2000 MHz *2 1 GB RAM 60 GB HD Pentium 4 1.8 GHZ 128 MB RAM 40 GB HD Pentium 4 2.5 GHZ 512 MB RAM 80 GB HD Figure 4 .
7-11:Our Data Grid testbed In the following experiments, we set = 0.5, the LeastSize threshold to 10MB, and experimented with file sizes of 10 MB, 50MB, 100MB, 500MB, 1000MB, 2000MB, and 4000MB .
7-12:For comparison, we measured the performance of Conservative Load Balancing on each size using the same block numbers .
7-13:Figure 5 shows a snapshot of our GridFTP client tool .
7-14:This client tool is developed by using Java CoG .
7-15:It allows easier and more rapid application development by encouraging collaborative code reuse and avoiding duplication of effort among problem solving environments, science portals, Grid middleware, and collaborative pilots .
7-16:Table 1 shows average transmission rates between THU and each replica server .
7-17:These numbers were obtained by transferring files of 500MB, 1000MB, and 2000MB from a single replica server using our GridFTP client tool, and each number is an average over several runs .
7-18:Table 1 .
7-19:GridFTP end to end transmission rate from THU to various servers Server Average transmission rate HIT 61.5 Mbps LZ 59.5 Mbps DL 32.1 Mbps PU 26.7 Mbps 802 Figure 5 .
7-20:Our GridFTP client tool We analyzed the effect of faster servers waiting for the slowest server to deliver the last block for each scheme .
7-21:Figure 6(a) shows total idle time for various file sizes .
7-22:Note that our RecursiveAdjustment Co Allocation scheme achieved significant performance improvements over other schemes for every file size .
7-23:These results demonstrate that our approach efficiently reduces the differences in servers finish times .
7-24:The experimental results shown in Figure 6(b) indicate that our scheme beginning block reassembly as soon as the first blocks have been completely delivered reduces combination time, thus aiding co allocation strategies like Conservative Load Balancing and RecursiveAdjustment Co Allocation that produce more blocks during data transfers .
7-25:Figure 7 shows total completion time experimental results in a detailed cost structure view .
7-26:Servers were at PU, DL, and HIT, with the client at THU .
7-27:The first three bars for each file size denote the time to download the entire file from single server, while the other bars show co allocated downloads using all three servers .
7-28:Our co allocation scheme finished the job faster than the other co allocation strategies .
7-29:Thus, we may infer that the main gains our technology offers are lower transmission and combination times than other co allocation strategies .
7-30:0 20 40 60 80 100 120 140 160 180 200 100 500 1000 1500 2000 File Size WaitTime(Sec) Brute3 History3 Conservative3 Recursive3 0 10 20 30 40 50 60 70 80 90 100 500 1000 1500 2000 File Size CombinationTime(Sec) Brute3 History3 Conservative3 Recursive3 Figure 6 .
7-31:(a) Idle times for various methods; servers are at PU, DL, and HIT .
7-32:(b) Combination times for various methods; servers are at PU, DL, and HIT .
7-33:In the next experiment, we used the Recursive Adjustment CoAllocation strategy with various sets of replica servers and measured overall performances, where overall performance is: Total Performance = File size Total Completion Time (5) Table 2 lists all experiments we performed and the sets of replica servers used .
7-34:The results in Figure 8(a) show that using coallocation technologies yielded no improvement for smaller file sizes such as 10MB .
7-35:They also show that in most cases, overall performance increased as the number of co allocated flows increased .
7-36:We observed that for our testbed and our co allocation technology, overall performance reached its highest value in the REC3_2 case .
7-37:However, in the REC4 case, when we added one flow to the set of replica servers, the performance did not increase .
7-38:On the contrary, it decreased .
7-39:We can infer that the co allocation efficiency reached saturation in the REC3_2 case, and that additional flows caused additional overhead and reduced overall performance .
7-40:This means that more download flows do not necessarily result in higher performance .
7-41:We must choose appropriate numbers of flows to achieve optimum performance .
7-42:We show the detailed cost structure view for the case of REC3_2 and the case of REC4 in Figure 8(b) .
7-43:The detailed cost consists of authentication time, transfer time and combination time .
7-44:0 100 200 300 400 500 600 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 PU1 DL1 HIT1 BRU3 HIS3 CON3 REC3 500 1000 1500 2000 File Size CompletionTime(Sec) Authentication Time Transmission Time Combination Time Figure 7 .
7-45:Completion times for various methods; servers are at PU, DL, and HIT .
7-46:Table 2 .
7-47:The sets of replica servers for all cases Case Servers PU1 PU DL1 DL REC2 PU, DL REC3_1 PU, DL, LZ REC3_2 PU, DL, HIT REC4 PU, DL, HIT, LZ 0 10 20 30 40 50 60 70 10 50 100 500 1000 1500 2000 File Size OverallPerformance(Mbits) PU1 DL1 REC2 REC3_1 REC3_2 REC4 0 10 20 30 40 50 60 70 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 REC3_2 REC4 10 50 100 500 1000 1500 2000 File Size OverallPerformance(Mbits) Authentication Time Transmission Time Combination Time Figure 8 .
7-48:(a) Overall performances for various sets of servers .
7-49:(b) Detailed cost structure view for the case of REC3_2 and the case of REC4. .
8-1:The co allocation architecture provides a coordinated agent for assigning data blocks
8-2:A previous work showed that the dynamic co allocation scheme leads to performance improvements
8-3:However, it cannot handle the idle time of faster servers, which must wait for the slowest server to deliver its final block
8-4:We proposed the Recursive Adjustment Co Allocation scheme to improve data transfer performances using the co allocation architecture in [17]
8-5:In this approach, the workloads of selected replica servers are continuously adjusted during data transfers, and we provide a function that enables users to define a final 803 block threshold, according to their data grid environment
8-6:Experimental results show the effectiveness of our proposed technique in improving transfer time and reducing overall idle time spent waiting for the slowest server
8-7:We also discussed the re combination cost and provided an effective scheme for reducing it.
9-1:B
9-2:Allcock, J
9-3:Bester, J
9-4:Bresnahan, A
9-5:Chervenak, I
9-6:Foster, C
9-7:Kesselman, S
9-8:Meder, V
9-9:Nefedova, D
9-10:Quesnel, and S
9-11:Tuecke, Data Management and Transfer in HighPerformance Computational Grid Environments, Parallel Computing, 28(5):749 771, May 2002
9-12:B
9-13:Allcock, J
9-14:Bester, J
9-15:Bresnahan, A
9-16:Chervenak, I
9-17:Foster, C
9-18:Kesselman, S
9-19:Meder, V
9-20:Nefedova, D
9-21:Quesnel, and S
9-22:Tuecke, Secure, Efficient Data Transport and Replica Management for High Performance Data Intensive Computing, Proc
9-23:of the Eighteenth IEEE Symposium on Mass Storage Systems and Technologies, pp
9-24:13 28, 2001
9-25:B
9-26:Allcock, S
9-27:Tuecke, I
9-28:Foster, A
9-29:Chervenak, and C
9-30:Kesselman
9-31:Protocols and Services for Distributed DataIntensive Science
9-32:ACAT2000 Proceedings, pp
9-33:161 163, 2000
9-34:A
9-35:Chervenak, E
9-36:Deelman, I
9-37:Foster, L
9-38:Guy, W
9-39:Hoschek, A
9-40:Iamnitchi, C
9-41:Kesselman, P
9-42:Kunszt, and M
9-43:Ripeanu, Giggle: A Framework for Constructing Scalable Replica Location Services, Proc
9-44:of SC 2002, Baltimore, MD, 2002
9-45:A
9-46:Chervenak, I
9-47:Foster, C
9-48:Kesselman, C
9-49:Salisbury, and S
9-50:Tuecke, The Data Grid: Towards an Architecture for the Distributed Management and Analysis of Large Scientific Datasets, Journal of Network and Computer Applications, 23:187 200, 2001
9-51:K
9-52:Czajkowski, S
9-53:Fitzgerald, I
9-54:Foster, and C
9-55:Kesselman, Grid Information Services for Distributed Resource Sharing, Proc
9-56:of the Tenth IEEE International Symposium on High Performance Distributed Computing (HPDC 10"01), 181 194, August 2001
9-57:K
9-58:Czajkowski, I
9-59:Foster, and C
9-60:Kesselman
9-61:Resource CoAllocation in Computational Grids, Proc
9-62:of the Eighth IEEE International Symposium on High Performance Distributed Computing (HPDC 8"99), August 1999
9-63:F
9-64:Donno, L
9-65:Gaido, A
9-66:Ghiselli, F
9-67:Prelz, and M
9-68:Sgaravatto, DataGrid Prototype 1, TERENA Networking Conference, http:  www.terena.nl conferences tnc2002 Papers p5a2ghiselli.pdf, June 2002, I
9-69:Foster, C
9-70:Kesselman, and S
9-71:Tuecke
9-72:The Anatomy of the Grid: Enabling Scalable Virtual Organizations
9-73:Int
9-74:J
9-75:of Supercomputer Applications and High Performance Computing, 15(3), pp
9-76:200 222, 2001
9-77:I
9-78:Foster and C
9-79:Kesselman, Globus: A Metacomputing Infrastructure Toolkit, Intl J
9-80:Supercomputer Applications, 11(2), pp
9-81:115 128, 1997
9-82:Global Grid Forum, http:  www.ggf.org  W
9-83:Hoschek, J
9-84:Jaen Martinez, A
9-85:Samar, H
9-86:Stockinger, and K
9-87:Stockinger, Data Management in an International Data Grid Project, Proc
9-88:of First IEEE ACM International Workshop on Grid Computing  Grid 2000, Bangalore, India, December 2000
9-89:IBM Red Books, Introduction to Grid Computing with Globus, IBM Press, www.redbooks.ibm.com redbooks pdfs sg246895.pdf H
9-90:Stockinger, A
9-91:Samar, B
9-92:Allcock, I
9-93:Foster, K
9-94:Holtman, and B
9-95:Tierney, File and Object Replication in Data Grids, Journal of Cluster Computing, 5(3):305 314, 2002
9-96:SYSSTAT utilities home page, http:  perso.wanadoo.fr sebastien.godard  The Globus Alliance, http:  www.globus.org  S
9-97:Vazhkudai, Enabling the Co Allocation of Grid Data Transfers, Proc
9-98:of Fourth International Workshop on Grid Computing, pp
9-99:41 51, November 2003
9-100:S
9-101:Vazhkudai and J
9-102:Schopf, Using Regression Techniques to Predict Large Data Transfers, International Journal of High Performance Computing Applications (IJHPCA), 17:249 268, August 2003
9-103:S
9-104:Vazhkudai, S
9-105:Tuecke, and I
9-106:Foster, Replica Selection in the Globus Data Grid, Proc
9-107:of the 1st International Symposium on Cluster Computing and the Grid (CCGRID 2001), pp
9-108:106 113, May 2001
9-109:S
9-110:Vazhkudai, J
9-111:Schopf, Predicting Sporadic Grid Data Transfers, Proc
9-112:of 11th IEEE International Symposium on High Performance Distributed Computing (HPDC 11 ‘02), pp
9-113:188 196, July 2002
9-114:S
9-115:Vazhkudai, J
9-116:Schopf, and I
9-117:Foster, Predicting the Performance of Wide Area Data Transfers, Proc
9-118:of the 16th International Parallel and Distributed Processing Symposium (IPDPS 2002), pp.34 43, April 2002, pp
9-119:34  43
9-120:R
9-121:Wolski, N
9-122:Spring, and J
9-123:Hayes, The Network Weather Service: A Distributed Resource Performance Forecasting Service for Metacomputing, Future Generation Computer Systems, 15(5 6):757 768, 1999
9-124:Chao Tung Yang, Chun Hsiang Chen, Kuan Ching Li, and Ching Hsien Hsu, Performance Analysis of Applying Replica Selection Technology for Data Grid Environments, PaCT 2005, Lecture Notes in Computer Science, vol
9-125:3603, pp
9-126:278 287, Springer Verlag, September 2005
9-127:Chao Tung Yang, I Hsien Yang, Kuan Ching Li, and ChingHsien Hsu A Recursive Adjustment Co Allocation Scheme in Data Grid Environments, ICA3PP 2005 Algorithm and Architecture for Parallel Processing, Lecture Notes in Computer Science, vol
9-128:3719, pp
9-129:40 49, Springer Verlag, October 2005
9-130:X
9-131:Zhang, J
9-132:Freschl, and J
9-133:Schopf, A Performance Study of Monitoring and Information Services for Distributed Systems, Proc
9-134:of 12th IEEE International Symposium on High Performance Distributed Computing (HPDC 12 ‘03), pp
9-135:270 282, August 2003
9-136:804
picture:
