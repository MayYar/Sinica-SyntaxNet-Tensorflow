A Study of Poisson Query Generation Model for 
content:
1 ABSTRACT :
1-1:Many variants of language models have been proposed for information retrieval .
1-2:Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model .
1-3:In this paper, we propose and study a new family of query generation models based on Poisson distribution .
1-4:We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods .
1-5:We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per term smoothing and allowing for more accurate background modeling .
1-6:We present several variants of the new model corresponding to different smoothing methods, and evaluate them on four representative TREC test collections .
1-7:The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per term smoothing .
1-8:The performance can be further improved with two stage smoothing .
1-9:Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval Models As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4] .
1-10:Among many variants of language models proposed, the most popular and fundamental one is the query generation language model [21, 13], which leads to the query likelihood scoring method for ranking documents .
1-11:In such a model, given a query q and a document d, we compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional probability p(q|d) .
1-12:We can then rank documents based on the likelihood of generating the query .
1-13:Virtually all the existing query generation language models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18] .
1-14:The multinomial distribution is especially popular and also shown to be quite effective .
1-15:The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text .
1-16:Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms .
1-17:However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per term smoothing and weighting .
1-18:Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1 .
1-19:In this paper, we propose and study a new family of query generation models based on the Poisson distribution .
1-20:In this new family of models, we model the frequency of each term independently with a Poisson distribution .
1-21:To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model .
1-22:In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per term smoothing .
1-23:Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per term smoothing .
1-24:As in the existing work on multinomial language models, smoothing is critical for this new family of models .
1-25:We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions .
1-26:We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing .
1-27:In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model .
1-28:We exploit this potential advantage to develop a new term dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term independent smoothing algorithms using either Poisson or multinomial model .
1-29:This advantage is seen for both one stage and two stage smoothing .
1-30:Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula .
1-31:This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter .
1-32:The rest of the paper is organized as follows .
1-33:In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions .
1-34:In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval .
1-35:We then design empirical experiments to compare the two families of language models in Section 4 .
1-36:We discuss the related work in 5 and conclude in 6. .
2 QUERY GENERATION WITH POISSON PROCESS :
2-1:PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document .
2-2:In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution .
2-3:Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20] .
2-4:2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set .
2-5:Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w .
2-6:In retrieval, w could be either a query or a document .
2-7:We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively .
2-8:Suppose t is the time period during which the author composed the text .
2-9:With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time .
2-10:The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k! Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w| .
2-11:With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w) .
2-12:We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model .
2-13:Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above .
2-14:The maximum likelihood estimate of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in [22, 24] .
2-15:Given a document d, we may estimate a Poisson language model Λd using d as a sample .
2-16:The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term .
2-17:In practice, we have the flexibility to choose the vocabulary V .
2-18:In one extreme, we can use the vocabulary of the whole collection .
2-19:However, this may bring in noise and considerable computational cost .
2-20:In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms .
2-21:As a compromise, we may conflate all the non query terms as one single pseudo term .
2-22:In other words, we may assume that there is exactly one non query term in the vocabulary for each query .
2-23:In our experiments, we adopt this pseudo non query term strategy .
2-24:A document can be scored with the likelihood in Equation 1 .
2-25:However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero .
2-26:As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d) .
2-27:2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non zero rates for the query terms that are not seen in document d .
2-28:Many smoothing methods have been proposed for multinomial language models[2, 28, 29] .
2-29:In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words .
2-30:In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1) .
2-31:Thus we do not have to discount the probability of seen words in order to give a non zero rate to an unseen word .
2-32:Instead, we only need to guarantee that k=0,1,2,.. .
2-33:p(c(w, d) = k|d) = 1 .
2-34:In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions .
2-35:2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .
2-36:A document is assumed to be generated from a potentially different model .
2-37:Given a particular document d, we want to estimate Λd .
2-38:The rate of a term is estimated independently of other terms .
2-39:We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model .
2-40:The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w .
2-41:Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28] .
2-42:2.2.2 Interpolation (Jelinek Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models .
2-43:One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non zero rate to w .
2-44:For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]) .
2-45:With this simple interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency based models .
2-46:In the retrieval formula above, the first summation can be computed efficiently .
2-47:The second summation can be actually treated as a document prior, which penalizes long documents .
2-48:As the second summation is difficult to compute efficiently, we conflate all non query terms as one pseudo non queryterm, denoted as N .
2-49:Using the pseudo term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| .
2-50:2.2.3 Two Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query .
2-51:In order to distinguish the content and non discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U) .
2-52:p(·|U) models the typical term frequencies in the user"s queries .
2-53:We may then score each document with the query likelihood computed using the following two stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q .
2-54:This looks similar to the interpolation smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE .
2-55:With no prior knowledge on p(·|U), we could set it to p(·|C) .
2-56:Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1 .
2-57:The empirical study of the smoothing methods is presented in Section 4. .
3 ANALYSIS OF POISSON LANGUAGE MODEL :
3-1:MODEL From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model .
3-2:This is expected since they both belong to the exponential family [26] .
3-3:However, there are many differences when these two families of models are applied with different smoothing methods .
3-4:From the perspective of retrieval, will these two language models perform equivalently? If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits? In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models .
3-5:3.1 The Equivalence of Basic Models Let us begin with the assumption that all the query terms appear in every document .
3-6:Under this assumption, no smoothing is needed .
3-7:A document can be scored by the log likelihood of the query with the maximum likelihood estimate: Score(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Using the MLE, we have λd,w = c(w,d) w∈V c(w,d) .
3-8:Thus Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate .
3-9:Indeed, even with Gamma smoothing, when plugging λd,w = c(w,d)+µλC,w |d|+µ and λC,w = c(w,C) |C| into Equation 5, it is easy to show that Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6) which is exactly the Dirichlet retrieval formula in [28] .
3-10:Note that this equivalence holds only when the document length variation is modeled with Poisson process .
3-11:This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval .
3-12:With other smoothing strategies, however, the two models would be different .
3-13:Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored .
3-14:Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model .
3-15:In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models .
3-16:3.2 Term Dependent Smoothing One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per term) smoothing .
3-17:Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are .
3-18:[7] also predicted that different terms should have a different smoothing weights .
3-19:With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model [28, 29] .
3-20:This parameter can be made specific for different queries, but always has to be a constant for all the terms .
3-21:This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1 .
3-22:However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query .
3-23:For example, a non discriminative term (e.g., the, is) is expected to be explained more with the background model, while a content term (e.g., retrieval, bush) in the query should be explained with the document model .
3-24:Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term .
3-25:Since the Poisson language model does not have the sum to one constraint across terms, it can easily accommodate per term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models .
3-26:Below we present a possible way to explore term dependent smoothing with Poisson language models .
3-27:Essentially, we want to use a term specific smoothing coefficient δ in the linear combination, denoted as δw .
3-28:This coefficient should intuitively be larger if w is a common word and smaller if it is a content word .
3-29:The key problem is to find a method to assign reasonable values to δw .
3-30:Empirical tuning is infeasible for so many parameters .
3-31:We may instead estimate the parameters ∆ = {δ1, ..., δ|V |} by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U), where ΛQ is the true query model to generate the query and p(q|U) is a query background model as discussed in Section 2.2.3 .
3-32:With the model p(q|ΛQ) hidden, the query likelihood is p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents .
3-33:Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection .
3-34:Setting p(·|U) as p(·|C), the query likelihood becomes p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) where πd = p(ˆΛd|U) .
3-35:p(·|ˆΛd) is an estimated Poisson language model for document d .
3-36:If we have prior knowledge on p(ˆΛd|U), such as which documents are relevant to the query, we can set πd accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents .
3-37:Without this prior knowledge, we can leave πd as free parameters, and use the EM algorithm to estimate πd and ∆ .
3-38:The updating functions are given as π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) As discussed in [29], we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low .
3-39:We again assume our vocabulary containing all query terms plus a pseudo non query term .
3-40:Note that the function does not give an explicit way of estimating the coefficient for the unseen non query term .
3-41:In our experiments, we set it to the average over δw of all query terms .
3-42:With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values .
3-43:In Section 4, we use empirical experiments to prove this hypothesis .
3-44:3.3 Mixture Background Models Another flexibility is to explore different background (collection) models (i.e., p(·|U), or p(·|C)) .
3-45:One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models [28, 29] .
3-46:Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C| .
3-47:However, this assumption usually does not hold, since the collection is far more complex than a single document .
3-48:Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc .
3-49:Treating the collection model as a mixture of document models, instead of a single pseudo document model is more reasonable .
3-50:Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters [15, 10], neighbor documents [25], and aspects [8, 27] .
3-51:All the approaches can be easily adopted using Poisson language models .
3-52:However, a common problem of these approaches is that they all require heavy computation to construct the background model .
3-53:With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost .
3-54:Poisson Mixture [3] has been proposed to model a collection of documents, which can fit the data much better than a single Poisson .
3-55:The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) is a single Poisson model and p(λ) is an arbitrary probability density function .
3-56:There are three well known Poisson mixtures [3]: 2 Poisson, Negative Binomial, and the Katz"s K Mixture [9] .
3-57:Note that the 2 Poisson model has actually been explored in probabilistic retrieval models, which led to the well known BM25 formula [22] .
3-58:All these mixtures have closed forms, and can be estimated from the collection of documents efficiently .
3-59:This is an advantage over the multinomial mixture models, such as PLSI [8] and LDA [1], for retrieval .
3-60:For example, the probability density function of Katz"s K Mixture is given as p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k where ηk,0 = 1 when k = 0, and 0 otherwise .
3-61:With the observation of a collection of documents, αw and βw can be estimated as βw = cf(w) − df(w) df(w) and αw = cf(w) Nβw where cf(w) and df(w) are the collection frequency and document frequency of w, and N is the number of documents in the collection .
3-62:To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query .
3-63:This Poisson mixture model can be easily used to replace P(·|C) in the retrieval functions 3 and 4 .
3-64:3.4 Other Possible Flexibilities In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages .
3-65:For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization .
3-66:Intuitively, when the document has more unique words, it will be penalized more .
3-67:On the other hand, if a document is exactly n copies of another document, it would not get over penalized .
3-68:This feature is desirable and not achieved with the Dirichlet model [5] .
3-69:Potentially, this component could penalize a document according to what types of terms it contains .
3-70:With term specific settings of δ, we could get even more flexibility for document length normalization .
3-71:Pseudo feedback is yet another interesting direction where the Poission model might be able to show its advantage .
3-72:With model based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model .
3-73:We could also utilize the relevant documents to learn better per term smoothing coefficients. .
4 EVALUATION :
4-1:In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval .
4-2:In this section, we compare these two families of models empirically .
4-3:Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two stage smoothing .
4-4:Using Poisson mixture as background model also improves the retrieval performance .
4-5:4.1 Datasets Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web) .
4-6:To cover different types of queries, we follow [28, 5], and construct short keyword (SK, keyword title), short verbose (SV, one sentence description), and long verbose (LV, multiple sentences) queries .
4-7:The documents are stemmed with the Porter"s stemmer, and we do not remove any stop word .
4-8:For each parameter, we vary its value to cover a reasonably wide range .
4-9:4.2 Comparison to Multinomial We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors .
4-10:Table 1 shows that the two JM smoothed models perform similarly on all data sets .
4-11:Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented .
4-12:We see that Dirichlet Gamma smoothing methods outperform both Jelinek Mercer smoothing methods .
4-13:The parameter sensitivity curves for two Jelinek Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek Mercer smoothing smoothing methods are shown in Figure 1 .
4-14:Clearly, these two methods perform similarly either in terms of optimality Data Query JM Multinomial JM Poisson Dirichlet Gamma Per term 2 Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88 89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Table 1: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two stage smoothing significantly improves Poisson An asterisk (*) indicates that the difference between the performance of the term dependent two stage smoothing and that of the Dirichlet Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05 .
4-15:or sensitivity .
4-16:This similarity of performance is expected as we discussed in Section 3.1 .
4-17:Although the Poisson model and multinomial model are similar in terms of the basic model and or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved .
4-18:As shown in the rightmost column of Table 1, term dependent two stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries .
4-19:This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent .
4-20:The parameter µ of the first stage Gamma smoothing is empirically tuned .
4-21:The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2 .
4-22:The parameter sensitivity curves for Dirichlet Gamma and the per term two stage smoothing model are plotted in Figure 2 .
4-23:The per term two stage smoothing method is less sensitive to the parameter µ than Dirichlet Gamma, and yields better optimal performance .
4-24:0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Dataset: AP; Query Type: SV Parameter: µ AveragePrecision Dirichlet Gamma Smoothing Term Dependent 2−Stage Figure 2: Term dependent two stage smoothing of Poisson outperforms Dirichlet Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models .
4-25:4.3 Term Dependent Smoothing To test the effectiveness of the term dependent smoothing, we conduct the following two experiments .
4-26:In the first experiment, we relax the constant coefficient in the simple Jelinek Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term .
4-27:Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero .
4-28:We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations .
4-29:The documents are then still scored with Formula 3, but using learnt δw .
4-30:The results are labeled with JM+L .
4-31:in Table 2 .
4-32:Data Q JM JM JM+L .
4-33:2 Stage 2 Stage PT: No Yes Yes No Yes AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Table 2: Term dependent smoothing improves retrieval performance An asterisk (*) in Column 3 indicates that the difference between the JM+L .
4-34:method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two stage method and query dependent two stage method is statistically significant; PT stands for per term .
4-35:With term dependent coefficients, the performance of the Jelinek Mercer Poisson model is improved in most cases .
4-36:However, in some cases (e.g., Trec7 SV), it performs poorly .
4-37:This might be caused by the problem of EM estimation with unsmoothed document models .
4-38:Once non zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly .
4-39:This indicates that there is still room to find better methods to estimate δw .
4-40:Please note that neither the perterm JM method nor the JM+L .
4-41:method has a parameter to tune .
4-42:As shown in Table 1, the term dependent two stage smoothing can significantly improve retrieval performance .
4-43:To understand whether the improvement is contributed by the term dependent smoothing or the two stage smoothing framework, we design another experiment to compare the perterm two stage smoothing with the two stage smoothing method proposed in [29] .
4-44:Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ .
4-45:However, since their model is based on multinomial language modeling, they could not get per term coefficients .
4-46:We adopt their method to the Poisson two stage smoothing, and also estimate a per query coefficient for all the terms .
4-47:We compare the performance of such a model with the per term two stage smoothing model, and present the results in the right two columns in Table 2 .
4-48:Again, we see that the per term two stage smoothing outperforms the per query two stage smoothing, especially for verbose queries .
4-49:The improvement is not as large as how the perterm smoothing method improves over Dirichlet Gamma .
4-50:This is expected, since the per query smoothing has already addressed the query discrimination problem to some extent .
4-51:This experiment shows that even if the smoothing is already per query, making it per term is still beneficial .
4-52:In brief, the per term smoothing improved the retrieval performance of both one stage and two stage smoothing method .
4-53:4.4 Mixture Background Model In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models .
4-54:Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katz"s K Mixture model, which is essentially a mixture of Poisson distributions .
4-55:p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3 .
4-56:Data Query JM .
4-57:Poisson JM .
4-58:K Mixture AP SK 0.203 0.204 SV 0.183 0.188* Trec 7 SK 0.168 0.169 SV 0.176 0.178* Trec 8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Table 3: K Mixture background model improves retrieval performance The performance of the JM retrieval model with single Poisson background and with Katz"s K Mixture background model is compared in Table 3 .
4-59:Clearly, using K Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant .
4-60:Figure 3 shows that the performance changes over different parameters for short verbose queries .
4-61:The model using K Mixture background is less sensitive than the one using single Poisson background .
4-62:Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2 Poisson and negative Binomial, could help the performance. .
5 RELATED WORK :
5-1:To the best of our knowledge, there has been no study of query generation models based on Poisson distribution .
5-2:Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4] .
5-3:The most popular and fundamental one is the query generation language model [21, 13] .
5-4:All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or multivariate Bernoulli distribution [21, 17, 18] .
5-5:We introduce a new family of language models, based on Poisson distribution .
5-6:Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23] .
5-7:[24] studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial .
5-8:However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation .
5-9:[26] introduces a way to empirically search for an exponential model for the documents .
5-10:Poisson mixtures [3] such as 2 Poisson [22], Negative multinomial, and Katz"s KMixture [9] has shown to be effective to model and retrieve documents .
5-11:Once again, none of this work explores Poisson distribution in the query generation framework .
5-12:Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with multinomial language models .
5-13:[7] analytically shows that term specific smoothing could be useful .
5-14:We show that Poisson language model is natural to accommodate the per term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically. .
6 CONCLUSIONS :
6-1:We present a new family of query generation language models for retrieval based on Poisson distribution .
6-2:We derive several smoothing methods for this family of models, including single stage smoothing and two stage smoothing .
6-3:We compare the new models with the popular multinomial retrieval models both analytically and experimentally .
6-4:Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences .
6-5:In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per term smoothing .
6-6:We exploit this property to develop a new per term smoothing algorithm for Poisson language models, which is shown to outperform term independent smoothing for both Poisson and multinomial models .
6-7:Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model .
6-8:Our work opens up many interesting directions for further exploration in this new family of models .
6-9:Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo feedback could be good future work .
6-10:It is also appealing to find robust methods to learn the per term smoothing coefficients without additional computation cost. .
7-1:We thank the anonymous SIGIR 07 reviewers for their useful comments
7-2:This material is based in part upon work supported by the National Science Foundation under award numbers IIS 0347933 and 0425852.
8-1:D
8-2:Blei, A
8-3:Ng, and M
8-4:Jordan
8-5:Latent dirichlet allocation
8-6:Journal of Machine Learning Research, 3:993 1022, 2003
8-7:S
8-8:F
8-9:Chen and J
8-10:Goodman
8-11:An empirical study of smoothing techniques for language modeling
8-12:Technical Report TR 10 98, Harvard University, 1998
8-13:K
8-14:Church and W
8-15:Gale
8-16:Poisson mixtures
8-17:Nat
8-18:Lang
8-19:Eng., 1(2):163 190, 1995
8-20:W
8-21:B
8-22:Croft and J
8-23:Lafferty, editors
8-24:Language Modeling and Information Retrieval
8-25:Kluwer Academic Publishers, 2003
8-26:H
8-27:Fang, T
8-28:Tao, and C
8-29:Zhai
8-30:A formal study of information retrieval heuristics
8-31:In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 49 56, 2004
8-32:D
8-33:Hiemstra
8-34:Using Language Models for Information Retrieval
8-35:PhD thesis, University of Twente, Enschede, Netherlands, 2001
8-36:D
8-37:Hiemstra
8-38:Term specific smoothing for the language modeling approach to information retrieval: the importance of a query term
8-39:In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 35 41, 2002
8-40:T
8-41:Hofmann
8-42:Probabilistic latent semantic indexing
8-43:In Proceedings of ACM SIGIR"99, pages 50 57, 1999
8-44:S
8-45:M
8-46:Katz
8-47:Distribution of content words and phrases in text and language modelling
8-48:Nat
8-49:Lang
8-50:Eng., 2(1):15 59, 1996
8-51:O
8-52:Kurland and L
8-53:Lee
8-54:Corpus structure, language models, and ad hoc information retrieval
8-55:In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 194 201, 2004
8-56:J
8-57:Lafferty and C
8-58:Zhai
8-59:Document language models, query models, and risk minimization for information retrieval
8-60:In Proceedings of SIGIR"01, pages 111 119, Sept 2001
8-61:J
8-62:Lafferty and C
8-63:Zhai
8-64:Probabilistic IR models based on query and document generation
8-65:In Proceedings of the Language Modeling and IR workshop, pages 1 5, May 31  June 1 2001
8-66:J
8-67:Lafferty and C
8-68:Zhai
8-69:Probabilistic relevance models based on document and query generation
8-70:In W
8-71:B
8-72:Croft and J
8-73:Lafferty, editors, Language Modeling and Information Retrieval
8-74:Kluwer Academic Publishers, 2003
8-75:V
8-76:Lavrenko and B
8-77:Croft
8-78:Relevance based language models
8-79:In Proceedings of SIGIR"01, pages 120 127, Sept 2001
8-80:X
8-81:Liu and W
8-82:B
8-83:Croft
8-84:Cluster based retrieval using language models
8-85:In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 186 193, 2004
8-86:E
8-87:L
8-88:Margulis
8-89:Modelling documents with multiple poisson distributions
8-90:Inf
8-91:Process
8-92:Manage., 29(2):215 227, 1993
8-93:A
8-94:McCallum and K
8-95:Nigam
8-96:A comparison of event models for naive bayes text classification
8-97:In Proceedings of AAAI 98 Workshop on Learning for Text Categorization, 1998
8-98:D
8-99:Metzler, V
8-100:Lavrenko, and W
8-101:B
8-102:Croft
8-103:Formal multiple bernoulli models for language modeling
8-104:In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 540 541, 2004
8-105:D
8-106:H
8-107:Miller, T
8-108:Leek, and R
8-109:Schwartz
8-110:A hidden Markov model information retrieval system
8-111:In Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval, pages 214 221, 1999
8-112:A
8-113:Papoulis
8-114:Probability, random variables and stochastic processes
8-115:New York: McGraw Hill, 1984, 2nd ed., 1984
8-116:J
8-117:M
8-118:Ponte and W
8-119:B
8-120:Croft
8-121:A language modeling approach to information retrieval
8-122:In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275 281, 1998
8-123:S
8-124:Robertson and S
8-125:Walker
8-126:Some simple effective approximations to the 2 poisson model for probabilistic weighted retrieval
8-127:In Proceedings of SIGIR"94, pages 232 241, 1994
8-128:S
8-129:E
8-130:Robertson, S
8-131:Walker, S
8-132:Jones, M
8-133:M.Hancock Beaulieu, and M
8-134:Gatford
8-135:Okapi at TREC 3
8-136:In D
8-137:K
8-138:Harman, editor, The Third Text REtrieval Conference (TREC 3), pages 109 126, 1995
8-139:T
8-140:Roelleke and J
8-141:Wang
8-142:A parallel derivation of probabilistic information retrieval models
8-143:In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 107 114, 2006
8-144:T
8-145:Tao, X
8-146:Wang, Q
8-147:Mei, and C
8-148:Zhai
8-149:Language model information retrieval with document expansion
8-150:In Proceedings of HLT NAACL 2006, pages 407 414, 2006
8-151:J
8-152:Teevan and D
8-153:R
8-154:Karger
8-155:Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model
8-156:In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 18 25, 2003
8-157:X
8-158:Wei and W
8-159:B
8-160:Croft
8-161:Lda based document models for ad hoc retrieval
8-162:In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 178 185, 2006
8-163:C
8-164:Zhai and J
8-165:Lafferty
8-166:A study of smoothing methods for language models applied to ad hoc information retrieval
8-167:In Proceedings of ACM SIGIR"01, pages 334 342, Sept 2001
8-168:C
8-169:Zhai and J
8-170:Lafferty
8-171:Two stage language models for information retrieval
8-172:In Proceedings of ACM SIGIR"02, pages 49 56, Aug 2002
picture:
