Efficient Bayesian Hierarchical User Modeling for 
content:
1 ABSTRACT :
1-1:A content based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user"s interest .
1-2:A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model .
1-3:Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive .
1-4:The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications .
1-5:This paper proposes a new fast learning technique to learn a large number of individual user profiles .
1-6:The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens .
1-7:Categories and Subject Descriptors: B.3.3 [Information Search and Retrieval]: Information .
2 INTRODUCTION :
2-1:Personalization is the future of the Web, and it has achieved great success in industrial applications .
2-2:For example, online stores, such as Amazon and Netflix, provide customized recommendations for additional products or services based on a user"s history .
2-3:Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a user"s interests from his her history .
2-4:One major personalization topic studied in the information retrieval community is content based personal recommendation systems1 .
2-5:These systems learn user specific profiles from user feedback so that they can recommend information tailored to each individual user"s interest without requiring the user to make an explicit query .
2-6:Learning the user profiles is the core problem for these systems .
2-7:A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user .
2-8:One major challenge of building a recommendation or personalization system is that the profile learned for a particular user is usually of low quality when the amount of data from that particular user is small .
2-9:This is known as the cold start problem .
2-10:This means that any new user must endure poor initial performance until sufficient feedback from that user is provided to learn a reliable user profile .
2-11:There has been much research on improving classification accuracy when the amount of labeled training data is small .
2-12:The semi supervised learning approach combines unlabeled and labeled data together to achieve this goal [26] .
2-13:Another approach is using domain knowledge .
2-14:Researchers have modified different learning algorithms, such as Na¨ıveBayes [17], logistic regression [7], and SVMs [22], to integrate domain knowledge into a text classifier .
2-15:The third approach is borrowing training data from other resources [5][7] .
2-16:The effectiveness of these different approaches is mixed, due to how well the underlying model assumption fits the data .
2-17:One well received approach to improve recommendation system performance for a particular user is borrowing information from other users through a Bayesian hierarchical modeling approach .
2-18:Several researchers have demonstrated that this approach effectively trades off between shared and user specific information, thus alleviating poor initial performance for each user[27][25] .
2-19:In order to learn a Bayesian hierarchical model, the system usually tries to find the most likely model parameters for the given data .
2-20:A mature recommendation system usually works for millions of users .
2-21:It is well known that learning the optimal parameters of a Bayesian hierarchical model is computationally expensive when there are thousands or millions of users .
2-22:The EM algorithm is a commonly used technique for parameter learning due to its simplicity and convergence guarantee .
2-23:However, a content based recommendation system often handles documents in a very high dimensional space, in which each document is represented by a very sparse vector .
2-24:With careful analysis of the EM algorithm in this scenario (Section 4), we find that the EM tering, or item based collaborative filtering .
2-25:In this paper, the words filtering and recommendation are used interchangeably .
2-26:algorithm converges very slowly due to the sparseness of the input variables .
2-27:We also find that updating the model parameter at each EM iteration is also expensive with computational complexity of O(MK), where M is the number of users and K is the number of dimensions .
2-28:This paper modifies the standard EM algorithm to create an improved learning algorithm, which we call the Modified EM algorithm .
2-29:The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters .
2-30:This greatly reduces the computation at a single EM iteration, and also has the benefit of increasing the convergence speed of the learning algorithm .
2-31:The proposed technique is not only well supported by theory, but also by experimental results .
2-32:The organization of the remaining parts of this paper is as follows: Section 3 describes the Bayesian hierarchical linear regression modeling framework used for content based recommendations .
2-33:Section 4 describes how to learn the model parameters using the standard EM algorithm, along with using the new technique proposed in this paper .
2-34:The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6 .
2-35:Section 7 summarizes and offers concluding remarks. .
3 RELATED WORK :
3-1:Providing personalized recommendations to users has been identified as a very important problem in the IR community since the 1970"s .
3-2:The approaches that have been used to solve this problem can be roughly classified into two major categories: content based filtering versus collaborative filtering .
3-3:Content based filtering studies the scenario where a recommendation system monitors a document stream and pushes documents that match a user profile to the corresponding user .
3-4:The user may read the delivered documents and provide explicit relevance feedback, which the filtering system then uses to update the user"s profile using relevance feedback retrieval models (e.g .
3-5:Boolean models, vector space models, traditional probabilistic models [20] , inference networks [3] and language models [6]) or machine learning algorithms (e.g .
3-6:Support Vector Machines (SVM), K nearest neighbors (K NN) clustering, neural networks, logistic regression, or Winnow [16] [4] [23]) .
3-7:Collaborative filtering goes beyond merely using document content to recommend items to a user by leveraging information from other users with similar tastes and preferences in the past .
3-8:Memorybased heuristics and model based approaches have been used in collaborative filtering task [15] [8] [2] [14] [12] [11] .
3-9:This paper contributes to the content based recommendation research by improving the efficiency and effectiveness of Bayesian hierarchical linear models, which have a strong theoretical basis and good empirical performance on recommendation tasks[27][25] .
3-10:This paper does not intend to compare content based filtering with collaborative filtering or claim which one is a better .
3-11:We think each complements the other, and that content based filtering is extremely useful for handling new documents items with little or no user feedback .
3-12:Similar to some other researchers[18][1][21], we found that a recommendation system will be more effective when both techniques are combined .
3-13:However, this is beyond the scope of this paper and thus not discussed here. .
4 BAYESIAN HIERARCHICAL LINEAR REGRESSION :
4-1:REGRESSION Assume there are M users in the system .
4-2:The task of the system is to recommend documents that are relevant to each user .
4-3:For each user, the system learns a user model from the user"s history .
4-4:In the rest of this paper, we will use the following notations to represent the variables in the system .
4-5:m = 1, 2, ..., M: The index for each individual user .
4-6:M is the total number of users .
4-7:wm: The user model parameter associated with user m .
4-8:wm is a K dimensional vector .
4-9:j = 1, 2, ..., Jm: The index for a set of data for user m .
4-10:Jm is the number of training data for user m .
4-11:Dm = {(xm,j, ym,j)}: A set of data associated with user m .
4-12:xm,j is a K dimensional vector that represents the mth user"s jth training document.2 ym,j is a scalar that represents the label of document xm,j .
4-13:k = 1, 2, ..., K: The dimensional index of input variable x .
4-14:The Bayesian hierarchical modeling approach has been widely used in real world information retrieval applications .
4-15:Generalized Bayesian hierarchical linear models, one of the simplest Bayesian hierarchical models, are commonly used and have achieved good performance on collaborative filtering [25] and content based adaptive filtering [27] tasks .
4-16:Figure 1 shows the graphical representation of a Bayesian hierarchical model .
4-17:In this graph, each user model is represented by a random vector wm .
4-18:We assume a user model is sampled randomly from a prior distribution P(w|Φ) .
4-19:The system can predict the user label y of a document x given an estimation of wm (or wm"s distribution) using a function y = f(x, w) .
4-20:The model is called generalized Bayesian hierarchical linear model when y = f(wT x) is any generalized linear model such as logistic regression, SVM, and linear regression .
4-21:To reliably estimate the user model wm, the system can borrow information from other users through the prior Φ = (µ, Σ) .
4-22:Now we look at one commonly used model where y = wT x + , where ∼ N(0, σ2 ) is a random noise [25][27] .
4-23:Assume that each user model wm is an independent draw from a population distribution P(w|Φ), which is governed by some unknown hyperparameter Φ .
4-24:Let the prior distribution of user model w be a Gaussian distribution with parameter Φ = (µ, Σ), which is the commonly used prior for linear models .
4-25:µ = (µ1, µ2, ..., µK ) is a K dimensional vector that represents the mean of the Gaussian distribution, and Σ is the covariance matrix of the Gaussian .
4-26:Usually, a Normal distribution N(0, aI) and an Inverse Wishart distribution P(Σ) ∝ |Σ|− 1 2 b exp(−1 2 ctr(Σ−1 )) are used as hyperprior to model the prior distribution of µ and Σ respectively .
4-27:I is the K dimensional identity matrix, and a, b, and c are real numbers .
4-28:With these settings, we have the following model for the system: respectively .
4-29:2 The first dimension of x is a dummy variable that always equals to 1 .
4-30:Figure 1: Illustration of dependencies of variables in the hierarchical model .
4-31:The rating, y, for a document, x, is conditioned on the document and the user model, wm, associated with the user m .
4-32:Users share information about their models through the prior, Φ = (µ, Σ) .
4-33:Normal distribution: wm ∼ N(µ, Σ2 ) Normal distribution: ym,j ∼ N(wT mxm,j, σ2 ) .
4-34:Let θ = (Φ, w1, w2, ..., wM ) represent the parameters of this system that needs to be estimated .
4-35:The joint likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: P(D, θ) = P(Φ) m P(wm|Φ) j P(ym,j|xm,j, wm) (1) For simplicity, we assume a, b, c, and σ are provided to the system. .
5 MODEL PARAMETER LEARNING :
5-1:If the prior Φ is known, finding the optimal wm is straightforward: it is a simple linear regression .
5-2:Therefore, we will focus on estimating Φ .
5-3:The maximum a priori solution of Φ is given by ΦMAP = arg max Φ P(Φ|D) (2) = arg max Φ P(Φ, D) P(D) (3) = arg max Φ P(D|Φ)P(Φ) (4) = arg max Φ w P(D|w, Φ)P(w|Φ)P(Φ)dw (5) Finding the optimal solution for the above problem is challenging, since we need to integrate over all w = (w1, w2, ..., wM ), which are unobserved hidden variables .
5-4:4.1 EM Algorithm for Bayesian Hierarchical Linear Models In Equation 5, Φ is the parameter needs to be estimated, and the result depends on unobserved latent variables w .
5-5:This kind of optimization problem is usually solved by the EM algorithm .
5-6:Applying EM to the above problem, the set of user models w are the unobservable hidden variables and we have: Q = w P(w|µ, Σ2 , Dm) log P(µ, Σ2 , w, D)dw Based on the derivation of the EM formulas presented in [24], we have the following Expectation Maximization steps for finding the optimal hyperparameters .
5-7:For space considerations, we omit the derivation in this paper since it is not the focus of our work .
5-8:E step: For each user m, estimate the user model distribution P(wm|Dm, Φ) = N(wm; ¯wm, Σ2 m) based on the current estimation of the prior Φ = (µ, Σ2 ) .
5-9:¯wm = ((Σ2 )−1 + Sxx,m σ2 )−1 ( Sxy,m σ2 + (Σ2 )−1 µ) (6) Σ2 m = ((Σ2 )−1 + Sxx,m σ2 )−1 (7) where Sxx,m = j xm,jxT m,j Sxy,m = j xm,jym,j M step: Optimize the prior Φ = (µ, Σ2 ) based on the estimation from the last E step .
5-10:µ = 1 M m ¯wm (8) Σ2 = 1 M m Σ2 m + ( ¯wm − µ)( ¯wm − µ)T (9) Many machine learning driven IR systems use a point estimate of the parameters at different stages in the system .
5-11:However, we are estimating the posterior distribution of the variables at the E step .
5-12:This avoids overfitting wm to a particular user"s data, which may be small and noisy .
5-13:A detailed discussion about this subject appears in [10] .
5-14:4.2 New Algorithm: Modified EM Although the EM algorithm is widely studied and used in machine learning applications, using the above EM process to solve Bayesian hierarchical linear models in large scale information retrieval systems is still too computationally expensive .
5-15:In this section, we describe why the learning rate of the EM algorithm is slow in our application and introduce a new technique to make the learning of the Bayesian hierarchical linear model scalable .
5-16:The derivation of the new learning algorithm will be based on the EM algorithm described in the previous section .
5-17:First, the covariance matrices Σ2 , Σ2 m are usually too large to be computationally feasible .
5-18:For simplicity, and as a common practice in IR, we do not model the correlation between features .
5-19:Thus we approximate these matrices with K dimensional diagonal matrices .
5-20:In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 . .
5-21:0 0 σ2 2 . .
5-22:0 5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database .
5-23:MovieLens allows users to rank how much he she enjoyed a specific movie on a scale from 1 to measurement of how relevant the document representing the corresponding movie is to the user .
5-24:We considered documents with likeability scores of 4 or 5 as relevant, and documents with a score of 1 to 3 as irrelevant to the user .
5-25:MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users .
5-26:On average, each user rated 151 movies, of these 87 were judged to be relevant .
5-27:The average score for a document was 3.58 .
5-28:Documents representing each movie were constructed from the portion of the IMDB database that is available for public download[13] .
5-29:Based on this database, we created one document per movie that contained the relevant information about it (e.g .
5-30:directors, actors, etc.) .
5-31:Table 1: Data Set Statistics .
5-32:On Reuters, the number of rating for a simulated user is the number of documents relevant to the corresponding topic .
5-33:Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix all 480,189 17,770 208 Netflix 1000 1000 17,770 127 Reuters C 34 100,000 3949 Reuters E 26 100,000 1632 Reuters G 33 100,000 2222 Reuters M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix[19] .
5-34:Netflix publicly provides the relevance judgments of 480,189 anonymous customers .
5-35:There are around 100 million rating on a scale of 1 to 5 for 17,770 documents .
5-36:Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant .
5-37:This number was reduced to 1000 customers through random sampling .
5-38:The average customer on the reduced data set provided 127 judgments, with 70 being deemed relevant .
5-39:The average score for documents is 3.55 .
5-40:Reuters Data: This is the Reuters Corpus, Volume 1 .
5-41:It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997 .
5-42:Only the first 100,000 news were used in our experiments .
5-43:The Reuters corpus comes with a topic hierarchy .
5-44:Each document is assigned to one of several locations on the hierarchical tree .
5-45:The first level of the tree contains four topics, denoted as C, E, M, and G .
5-46:For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters E Reuters C, ReutersM and Reuters G .
5-47:For each small data set, we created several profiles, one profile for each node in a sub tree, to simulate multiple users, each with a related, yet separate definition of relevance .
5-48:All the user profiles on a sub tree are supposed to share the same prior model distribution .
5-49:Since this corpus explicitly indicates only the relevant documents for a topic(user), all other documents are considered irrelevant .
5-50:5.2 Evaluation We designed the experiments to answer the following three questions: approach and learn a prior from other users? EM algorithm for learning the Bayesian hierarchical linear model? models? To answer the first question, we compared the Bayesian hierarchical models with commonly used Norm 2 regularized linear regression models .
5-51:In fact, the commonly used approach is equivalent to the model learned at the end of the first EM iteration .
5-52:To answer the second question, we compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is better .
5-53:To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together .
5-54:For the MovieLens and Netflix data sets, algorithm effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative .
5-55:We first evaluated the performance on each individual user, and then estimated the macro average over all users .
5-56:Statistical tests (t tests) were carried out to see whether the results are significant .
5-57:For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for training, and the rest for testing .
5-58:On Reuters" data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing .
5-59:For all runs, we set (a, b, c, Σ ) = (0.1, 10, 0.1, 1) manually. .
6 EXPERIMENTAL RESULTS :
6-1:Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian hierarchical models learned at the first iteration .
6-2:Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets .
6-3:This suggests that the borrowing information from other users has more significant improvements for users with less training data, which is as expected .
6-4:However, the strength of the correlation differs over data sets, and the amount of training data is not the only characteristics that will influence the final performance .
6-5:Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets .
6-6:This is not surprising since the number of related feature users pairs is much smaller than the number of unrelated feature user pairs on these two data sets, and thus the proposed new algorithm is expected to work better .
6-7:Figure 4 shows that the two algorithms work similarly on the Reuters E data set .
6-8:The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration .
6-9:The general patterns are very similar on other Reuters" subsets .
6-10:Further analysis shows that only 58% of the user feature pairs are unrelated on this data set .
6-11:Since the number of unrelated user feature pairs is not extremely large, the sparseness is not a serious problem on the Reuters data set .
6-12:Thus the two learning algorithms perform similarly .
6-13:The results suggest that only on a corpus where the number of unrelated user feature pairs is much larger than the number of related pairs, such as on the Netflix data set, the proposed technique will get a significant improvement over standard EM .
6-14:However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance .
6-15:Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users .
6-16:The new algorithm is statistical significantly better than EM algorithm at iterations 2 10 .
6-17:Norm 2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models .
6-18:0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users .
6-19:The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error) .
6-20:1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters E subset with 26 profiles .
6-21:Performances on Reuters C, Reuters M, Reuters G are similar .
6-22:1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly? Our results show that the modified EM algorithm converges quickly, and 2 3 modified EM iterations would result in a reliable estimation .
6-23:We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB memory, P4 3GHz) .
6-24:The system finished one modified EM iteration in about 4 hours .
6-25:This demonstrates that the proposed technique can efficiently handle large scale system like Netflix. .
7 CONCLUSION :
7-1:Content based user profile learning is an important problem and is the key to providing personal recommendations to a user, especially for recommending new items with a small number of ratings .
7-2:The Bayesian hierarchical modeling approach is becoming an important user profile learning approach due to its theoretically justified ability to help one user through information transfer from the other users by way of hyperpriors .
7-3:This paper examined the weakness of the popular EM based learning approach for Bayesian hierarchical linear models and proposed a better learning technique called Modified EM .
7-4:We showed that the new technique is theoretically more computationally efficient than the standard EM algorithm .
7-5:Evaluation on the MovieLens and Netflix data sets demonstrated the effectiveness of the new technique when the data is sparse, by which we mean the ratio of related user feature pairs to unrelated pairs is small .
7-6:Evaluation on the Reuters data set showed that the new technique performed similar to the standard EM algorithm when the sparseness condition does not hold .
7-7:In general, it is better to use the new algorithm since it is as simple as standard EM, the performance is either better or similar to EM, and the computation complexity is lower at each iteration .
7-8:It is worth mentioning that even if the original problem space is not sparse, sparseness can be created artificially when a recommendation system uses user specific feature selection techniques to reduce the noise and user model complexity .
7-9:The proposed technique can also be adapted to improve the learning in such a scenario .
7-10:We also demonstrated that the proposed technique can learn half a million user profiles from 100 million ratings in a few hours with a single CPU .
7-11:The research is important because scalability is a major concern for researchers when using the Bayesian hierarchical linear modeling approach to build a practical large scale system, even though the literature have demonstrated the effectiveness of the models in many applications .
7-12:Our work is one major step on the road to make Bayesian hierarchical linear models more practical .
7-13:The proposed new technique can be easily adapted to run on a cluster of machines, and thus further speed up the learning process to handle a larger scale system with hundreds of millions of users .
7-14:The research has much potential to benefit people using EM algorithm on many other IR problems as well as machine learning problems .
7-15:EM algorithm is a commonly used machine learning technique .
7-16:It is used to find model parameters in many IR problems where the training data is very sparse .
7-17:Although we are focusing on the Bayesian hierarchical linear models for recommendation and filtering, the new idea of using analytical solution instead of numerical solution for unrelated user feature pairs at the M step could be adapted to many other problems. .
8-1:We thank Wei Xu, David Lewis and anonymous reviewers for valuable feedback on the work described in this paper
8-2:Part of the work was supported by Yahoo, Google, the Petascale Data Storage Institute and the Institute for Scalable Scientific Data Management
8-3:Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect those of the sponsors.
9-1:C
9-2:Basu, H
9-3:Hirsh, and W
9-4:Cohen
9-5:Recommendation as classification: Using social and content based information in recommendation
9-6:In Proceedings of the Fifteenth National Conference on Artificial Intelligence, 1998
9-7:J
9-8:S
9-9:Breese, D
9-10:Heckerman, and C
9-11:Kadie
9-12:Empirical analysis of predictive algorithms for collaborative filtering
9-13:Technical report, Microsoft Research, One Microsoft Way, Redmond, WA 98052, 1998
9-14:J
9-15:Callan
9-16:Document filtering with inference networks
9-17:In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 262 269, 1996
9-18:N
9-19:Cancedda, N
9-20:Cesa Bianchi, A
9-21:Conconi, C
9-22:Gentile, C
9-23:Goutte, T
9-24:Graepel, Y
9-25:Li, J
9-26:M
9-27:Renders, J
9-28:S
9-29:Taylor, and A
9-30:Vinokourov
9-31:Kernel method for document filtering
9-32:In The Eleventh Text REtrieval Conference (TREC11)
9-33:National Institute of Standards and Technology, special publication 500 249, 2003
9-34:C
9-35:Chelba and A
9-36:Acero
9-37:Adaptation of maximum entropy capitalizer: Little data can help a lot
9-38:In D
9-39:Lin and D
9-40:Wu, editors, Proceedings of EMNLP 2004, pages 285 292, Barcelona, Spain, July 2004
9-41:Association for Computational Linguistics
9-42:B
9-43:Croft and J
9-44:Lafferty, editors
9-45:Language Modeling for Information Retrieval
9-46:Kluwer, 2002
9-47:A
9-48:Dayanik, D
9-49:D
9-50:Lewis, D
9-51:Madigan, V
9-52:Menkov, and A
9-53:Genkin
9-54:Constructing informative prior distributions from domain knowledge in text classification
9-55:In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 493 500, New York, NY, USA, 2006
9-56:ACM Press
9-57:J
9-58:Delgado and N
9-59:Ishii
9-60:Memory based weightedmajority prediction for recommender systems
9-61:In ACM SIGIR"99 Workshop on Recommender Systems, 1999
9-62:GroupLens
9-63:Movielens
9-64:http:  www.grouplens.org taxonomy term 14, 2006
9-65:D
9-66:Heckerman
9-67:A tutorial on learning with bayesian networks
9-68:In M
9-69:Jordan, editor, Learning in Graphical Models
9-70:Kluwer Academic, 1998
9-71:J
9-72:L
9-73:Herlocker, J
9-74:A
9-75:Konstan, A
9-76:Borchers, and J
9-77:Riedl
9-78:An algorithmic framework for performing collaborative filtering
9-79:In SIGIR "99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 230 237, New York, NY, USA, 1999
9-80:ACM Press
9-81:T
9-82:Hofmann and J
9-83:Puzicha
9-84:Latent class models for collaborative filtering
9-85:In IJCAI "99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 688 693, San Francisco, CA, USA, 1999
9-86:Morgan Kaufmann Publishers Inc
9-87:I
9-88:M
9-89:D
9-90:(IMDB)
9-91:Internet movie database
9-92:http:  www.imdb.com interfaces , 2006
9-93:R
9-94:Jin, J
9-95:Y
9-96:Chai, and L
9-97:Si
9-98:An automatic weighting scheme for collaborative filtering
9-99:In SIGIR "04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 337 344, New York, NY, USA, 2004
9-100:ACM Press
9-101:J
9-102:A
9-103:Konstan, B
9-104:N
9-105:Miller, D
9-106:Maltz, J
9-107:L
9-108:Herlocker, L
9-109:R
9-110:Gordon, and J
9-111:Riedl
9-112:GroupLens: Applying collaborative filtering to Usenet news
9-113:Communications of the ACM, 40(3):77 87, 1997
9-114:D
9-115:Lewis
9-116:Applying support vector machines to the TREC 2001 batch filtering and routing tasks
9-117:In Proceedings of the Eleventh Text REtrieval Conference (TREC 11), 2002
9-118:B
9-119:Liu, X
9-120:Li, W
9-121:S
9-122:Lee, , and P
9-123:Yu
9-124:Text classification by labeling words
9-125:In Proceedings of The Nineteenth National Conference on Artificial Intelligence (AAAI 2004), July 25 29, 2004
9-126:P
9-127:Melville, R
9-128:J
9-129:Mooney, and R
9-130:Nagarajan
9-131:Content boosted collaborative filtering for improved recommendations
9-132:In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI 2002), Edmonton, Canada, 2002
9-133:Netflix
9-134:Netflix prize
9-135:http:  www.netflixprize.com (visited on Nov
9-136:30, 2006), 2006
9-137:S
9-138:Robertson and K
9-139:Sparck Jones
9-140:Relevance weighting of search terms
9-141:In Journal of the American Society for Information Science, volume 27, pages 129 146, 1976
9-142:J
9-143:Wang, A
9-144:P
9-145:de Vries, and M
9-146:J
9-147:T
9-148:Reinders
9-149:Unifying user based and item based collaborative filtering approaches by similarity fusion
9-150:In SIGIR "06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501 508, New York, NY, USA, 2006
9-151:ACM Press
9-152:X
9-153:Wu and R
9-154:K
9-155:Srihari
9-156:Incorporating prior knowledge with weighted margin support vector machines
9-157:In Proc
9-158:ACM Knowledge Discovery Data Mining Conf.(ACM SIGKDD 2004), Aug
9-159:2004
9-160:Y
9-161:Yang, S
9-162:Yoo, J
9-163:Zhang, and B
9-164:Kisiel
9-165:Robustness of adaptive filtering methods in a cross benchmark evaluation
9-166:In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2005
9-167:K
9-168:Yu, V
9-169:Tresp, and A
9-170:Schwaighofer
9-171:Learning gaussian processes from multiple tasks
9-172:In ICML "05: Proceedings of the 22nd international conference on Machine learning, pages 1012 1019, New York, NY, USA, 2005
9-173:ACM Press
9-174:K
9-175:Yu, V
9-176:Tresp, and S
9-177:Yu
9-178:A nonparametric hierarchical bayesian framework for information filtering
9-179:In SIGIR "04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353 360
9-180:ACM Press, 2004
9-181:X
9-182:Zhu
9-183:Semi supervised learning literature survey
9-184:Technical report, University of Wisconsin  Madison, December 9, 2006
9-185:P
9-186:Zigoris and Y
9-187:Zhang
9-188:Bayesian adaptive user profiling with explicit & implicit feedback
9-189:In Conference on Information and Knowledge Mangement 2006, 2006
picture:
