Personalized Query Expansion for the Web 
content:
1 ABSTRACT :
1-1:The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval .
1-2:In this paper we propose to improve such Web queries by expanding them with terms collected from each user"s Personal Information Repository, thus implicitly personalizing the search output .
1-3:We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co occurrence statistics, as well as to using external thesauri .
1-4:Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings .
1-5:Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query .
1-6:A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach .
1-7:H.3.3 [Information Storage and Retrieval]: Information Search .
2 INTRODUCTION :
2-1:The booming popularity of search engines has determined simple keyword search to become the only widely accepted user interface for seeking information over the Web .
2-2:Yet keyword queries are inherently ambiguous .
2-3:The query canon book for example covers several different areas of interest: religion, photography, literature, and music .
2-4:Clearly, one would prefer search output to be aligned with user"s topic(s) of interest, rather than displaying a selection of popular URLs from each category .
2-5:Studies have shown that more than 80% of the users would prefer to receive such personalized search results [33] instead of the currently generic ones .
2-6:Query expansion assists the user in formulating a better query, by appending additional keywords to the initial search request in order to encapsulate her interests therein, as well as to focus the Web search output accordingly .
2-7:It has been shown to perform very well over large data sets, especially with short input queries (see for example [19, 3]) .
2-8:This is exactly the Web search scenario! In this paper we propose to enhance Web query reformulation by exploiting the user"s Personal Information Repository (PIR), i.e., the personal collection of text documents, emails, cached Web pages, etc .
2-9:Several advantages arise when moving Web search personalization down to the Desktop level (note that by Desktop we refer to PIR, and we use the two terms interchangeably) .
2-10:First is of course the quality of personalization: The local Desktop is a rich repository of information, accurately describing most, if not all interests of the user .
2-11:Second, as all profile information is stored and exploited locally, on the personal machine, another very important benefit is privacy .
2-12:Search engines should not be able to know about a person"s interests, i.e., they should not be able to connect a specific person with the queries she issued, or worse, with the output URLs she clicked within the search interface1 (see Volokh [35] for a discussion on privacy issues related to personalized Web search) .
2-13:Our algorithms expand Web queries with keywords extracted from user"s PIR, thus implicitly personalizing the search output .
2-14:After a discussion of previous works in Section 2, we first investigate the analysis of local Desktop query context in Section 3.1.1 .
2-15:We propose several keyword, expression, and summary based techniques for determining expansion terms from those personal documents matching the Web query best .
2-16:In Section 3.1.2 we move our analysis to the global Desktop collection and investigate expansions based on co occurrence metrics and external thesauri .
2-17:The experiments presented in Section 3.2 show many of these approaches to perform very well, especially on ambiguous queries, producing NDCG [15] improvements of up to 51.28% .
2-18:In Section 4 we move this algorithmic framework further and propose to make the expansion process adaptive to the clarity level of the query .
2-19:This yields an additional improvement of 8.47% over the previously identified best algorithm .
2-20:We conclude and discuss further work in Section 5 .
2-21:1 Search engines can map queries at least to IP addresses, for example by using cookies and mining the query logs .
2-22:However, by moving the user profile at the Desktop level we ensure such information is not explicitly associated to a particular user and stored on the search engine side. .
3 PREVIOUS WORK :
3-1:This paper brings together two IR areas: Search Personalization and Automatic Query Expansion .
3-2:There exists a vast amount of algorithms for both domains .
3-3:However, not much has been done specifically aimed at combining them .
3-4:In this section we thus present a separate analysis, first introducing some approaches to personalize search, as this represents the main goal of our research, and then discussing several query expansion techniques and their relationship to our algorithms .
3-5:2.1 Personalized Search Personalized search comprises two major components: (1) User profiles, and (2) The actual search algorithm .
3-6:This section splits the relevant background according to the focus of each article into either one of these elements .
3-7:Approaches focused on the User Profile .
3-8:Sugiyama et al .
3-9:[32] analyzed surfing behavior and generated user profiles as features (terms) of the visited pages .
3-10:Upon issuing a new query, the search results were ranked based on the similarity between each URL and the user profile .
3-11:Qiu and Cho [26] used Machine Learning on the past click history of the user in order to determine topic preference vectors and then apply Topic Sensitive PageRank [13] .
3-12:User profiling based on browsing history has the advantage of being rather easy to obtain and process .
3-13:This is probably why it is also employed by several industrial search engines (e.g., Yahoo! MyWeb2 DESKTOP DATA Desktop data represents a very rich repository of profiling information .
3-14:However, this information comes in a very unstructured way, covering documents which are highly diverse in format, content, and even language characteristics .
3-15:In this section we first tackle this problem by proposing several lexical analysis algorithms which exploit user"s PIR to extract keyword expansion terms at various granularities, ranging from term frequency within Desktop documents up to utilizing global co occurrence statistics over the personal information repository .
3-16:Then, in the second part of the section we empirically analyze the performance of each approach .
3-17:3.1 Algorithms This section presents the five generic approaches for analyzing user"s Desktop data in order to provide expansion terms for Web search .
3-18:In the proposed algorithms we gradually increase the amount of personal information utilized .
3-19:Thus, in the first part we investigate three local analysis techniques focused only on those Desktop documents matching user"s query best .
3-20:We append to the Web query the most relevant terms, compounds, and sentence summaries from these documents .
3-21:In the second part of the section we move towards a global Desktop analysis, proposing to investigate term co occurrences, as well as thesauri, in the expansion process .
3-22:3.1.1 Expanding with Local Desktop Analysis Local Desktop Analysis is related to enhancing Pseudo Relevance Feedback to generate query expansion keywords from the PIR best hits for user"s Web query, rather than from the top ranked Web search results .
3-23:We distinguish three granularity levels for this process and we investigate each of them separately .
3-24:Term and Document Frequency .
3-25:As the simplest possible measures, TF and DF have the advantage of being very fast to compute .
3-26:Previous experiments with small data sets have showed them to yield very good results [11] .
3-27:We thus independently associate a score with each term, based on each of the two statistics .
3-28:The TF based one is obtained by multiplying the actual frequency of a term with a position score descending as the term first appears closer to the end of the document .
3-29:This is necessary especially for longer documents, because more informative terms tend to appear towards their beginning [10] .
3-30:The complete TF based keyword extraction formula is as follows: TermScore = 1 2 + 1 2 · nrWords − pos nrWords ! · log(1 + TF) (1) where nrWords is the total number of terms in the document and pos is the position of the first appearance of the term; TF represents the frequency of each term in the Desktop document matching user"s Web query .
3-31:The identification of suitable expansion terms is even simpler when using DF: Given the set of Top K relevant Desktop documents, generate their snippets as focused on the original search request .
3-32:This query orientation is necessary, since the DF scores are computed at the level of the entire PIR and would produce too noisy suggestions otherwise .
3-33:Once the set of candidate terms has been identified, the selection proceeds by ordering them according to the DF scores they are associated with .
3-34:Ties are resolved using the corresponding TF scores .
3-35:Note that a hybrid TFxIDF approach is not necessarily efficient, since one Desktop term might have a high DF on the Desktop, while being quite rare in the Web .
3-36:For example, the term PageRank would be quite frequent on the Desktop of an IR scientist, thus achieving a low score with TFxIDF .
3-37:However, as it is rather rare in the Web, it would make a good resolution of the query towards the correct topic .
3-38:Lexical Compounds .
3-39:Anick and Tipirneni [2] defined the lexical dispersion hypothesis, according to which an expression"s lexical dispersion (i.e., the number of different compounds it appears in within a document or group of documents) can be used to automatically identify key concepts over the input document set .
3-40:Although several possible compound expressions are available, it has been shown that simple approaches based on noun analysis are almost as good as highly complex part of speech pattern identification algorithms [1] .
3-41:We thus inspect the matching Desktop documents for all their lexical compounds of the following form: { adjective? noun+ } All such compounds could be easily generated off line, at indexing time, for all the documents in the local repository .
3-42:Moreover, once identified, they can be further sorted depending on their dispersion within each document in order to facilitate fast retrieval of the most frequent compounds at run time .
3-43:Sentence Selection .
3-44:This technique builds upon sentence oriented document summarization: First, the set of relevant Desktop documents is identified; then, a summary containing their most important sentences is generated as output .
3-45:Sentence selection is the most comprehensive local analysis approach, as it produces the most detailed expansions (i.e., sentences) .
3-46:Its downside is that, unlike with the first two algorithms, its output cannot be stored efficiently, and consequently it cannot be computed off line .
3-47:We generate sentence based summaries by ranking the document sentences according to their salience score, as follows [21]: SentenceScore = SW2 TW + PS + TQ2 NQ The first term is the ratio between the square amount of significant words within the sentence and the total number of words therein .
3-48:A word is significant in a document if its frequency is above a threshold as follows: TF > ms = V ` X 7 − 0.1 ∗ (25 − NS) , if NS < 25 7 , if NS ∈ [25, 40] 7 + 0.1 ∗ (NS − 40) , if NS > 40 with NS being the total number of sentences in the document (see [21] for details) .
3-49:The second term is a position score set to (Avg(NS) − SentenceIndex) Avg2 for the first ten sentences, and to 0 otherwise, Avg(NS) being the average number of sentences over all Desktop items .
3-50:This way, short documents such as emails are not affected, which is correct, since they usually do not contain a summary in the very beginning .
3-51:However, as longer documents usually do include overall descriptive sentences in the beginning [10], these sentences are more likely to be relevant .
3-52:The final term biases the summary towards the query .
3-53:It is the ratio between the square number of query terms present in the sentence and the total number of terms from the query .
3-54:It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query .
3-55:3.1.2 Expanding with Global Desktop Analysis In contrast to the previously presented approach, global analysis relies on information from across the entire personal Desktop to infer the new relevant query terms .
3-56:In this section we propose two such techniques, namely term co occurrence statistics, and filtering the output of an external thesaurus .
3-57:Term Co occurrence Statistics .
3-58:For each term, we can easily compute off line those terms co occurring with it most frequently in a given collection (i.e., PIR in our case), and then exploit this information at run time in order to infer keywords highly correlated with the user query .
3-59:Our generic co occurrence based query expansion algorithm is as follows: Algorithm 3.1.2.1 .
3-60:Co occurrence based keyword similarity search .
3-61:Off line computation: 1: Filter potential keywords k with DF ∈ [10, .
3-62:.
3-63:.
3-64:, 20% · N] 2: For each keyword ki 3: For each keyword kj 4: Compute SCki,kj , the similarity coefficient of (ki, kj) On line computation: 1: Let S be the set of keywords, potentially similar to an input expression E .
3-65:2: For each keyword k of E: 3: S ← S ∪ TSC(k), where TSC(k) contains the Top K terms most similar to k 4: For each term t of S: 5a: Let Score(t) ← Q k∈E(0.01 + SCt,k) 5b: Let Score(t) ← #DesktopHits(E|t) 6: Select Top K terms of S with the highest scores .
3-66:The off line computation needs an initial trimming phase (step 1) for optimization purposes .
3-67:In addition, we also restricted the algorithm to computing co occurrence levels across nouns only, as they contain by far the largest amount of conceptual information, and as this approach reduces the size of the co occurrence matrix considerably .
3-68:During the run time phase, having the terms most correlated with each particular query keyword already identified, one more operation is necessary, namely calculating the correlation of every output term with the entire query .
3-69:Two approaches are possible: (1) using a product of the correlation between the term and all keywords in the original expression (step 5a), or (2) simply counting the number of documents in which the proposed term co occurs with the entire user query (step 5b) .
3-70:We considered the following formulas for Similarity Coefficients [17]: • Cosine Similarity, defined as: CS = DFx,y pDFx · DFy (2) • Mutual Information, defined as: MI = log N · DFx,y DFx · DFy (3) • Likelihood Ratio, defined in the paragraphs below .
3-71:DFx is the Document Frequency of term x, and DFx,y is the number of documents containing both x and y .
3-72:To further increase the quality of the generated scores we limited the latter indicator to cooccurrences within a window of W terms .
3-73:We set W to be the same as the maximum amount of expansion keywords desired .
3-74:Dunning"s Likelihood Ratio λ [9] is a co occurrence based metric similar to χ2 .
3-75:It starts by attempting to reject the null hypothesis, according to which two terms A and B would appear in text independently from each other .
3-76:This means that P(A B) = P(A¬B) = P(A), where P(A¬B) is the probability that term A is not followed by term B .
3-77:Consequently, the test for independence of A and B can be performed by looking if the distribution of A given that B is present is the same as the distribution of A given that B is not present .
3-78:Of course, in reality we know these terms are not independent in text, and we only use the statistical metrics to highlight terms which are frequently appearing together .
3-79:We compare the two binomial processes by using likelihood ratios of their associated hypotheses .
3-80:First, let us define the likelihood ratio for one hypothesis: λ = maxω∈Ω0 H(ω; k) maxω∈Ω H(ω; k) (4) where ω is a point in the parameter space Ω, Ω0 is the particular hypothesis being tested, and k is a point in the space of observations underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp H(p, p; k1, k2, n1, n2) maxp1,p2 H(p1, p2; k1, k2, n1, n2) (5) where H(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ .
3-81:Since the maxima are obtained with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp L(p, k1, n1)L(p, k2, n2) maxp1,p2 L(p1, k1, n1)L(p2, k2, n2) (6) where L(p, k, n) = pk · (1 − p)n−k .
3-82:Taking the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) − log L(p, k1, n1) − log L(p, k2, n2)] where log L(p, k, n) = k · log p + (n − k) · log(1 − p) .
3-83:Finally, if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B), and O22 = P(¬A¬B), then the co occurrence likelihood of terms A and B becomes: −2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) + O21 · log p2 + O22 · log (1 − p2) − (O11 + O21) · log p − (O12 + O22) · log (1 − p)] where p1 = k1 n1 = O11 O11+O12 , p2 = k2 n2 = O21 O21+O22 , and p = k1+k2 n1+n2 Thesaurus Based Expansion .
3-84:Large scale thesauri encapsulate global knowledge about term relationships .
3-85:Thus, we first identify the set of terms closely related to each query keyword, and then we calculate the Desktop co occurrence level of each of these possible expansion terms with the entire initial search request .
3-86:In the end, those suggestions with the highest frequencies are kept .
3-87:The algorithm is as follows: Algorithm 3.1.2.2 .
3-88:Filtered thesaurus based query expansion .
3-89:1: For each keyword k of an input query Q: 2: Select the following sets of related terms using WordNet: 2a: Syn: All Synonyms 2b: Sub: All sub concepts residing one level below k 2c: Super: All super concepts residing one level above k 3: For each set Si of the above mentioned sets: 4: For each term t of Si: 5: Search the PIR with (Q|t), i.e., the original query, as expanded with t 6: Let H be the number of hits of the above search (i.e., the co occurence level of t with Q) 7: Return Top K terms as ordered by their H values .
3-90:We observe three types of term relationships (steps 2a 2c): (1) synonyms, (2) sub concepts, namely hyponyms (i.e., sub classes) and meronyms (i.e., sub parts), and (3) super concepts, namely hypernyms (i.e., super classes) and holonyms (i.e., super parts) .
3-91:As they represent quite different types of association, we investigated them separately .
3-92:We limited the output expansion set (step 7) to contain only terms appearing at least T times on the Desktop, in order to avoid noisy suggestions, with T = min( N DocsPerTopic , MinDocs) .
3-93:We set DocsPerTopic = 2, 500, and MinDocs = 5, the latter one coping with the case of small PIRs .
3-94:3.2 Experiments 3.2.1 Experimental Setup We evaluated our algorithms with 18 subjects (Ph.D .
3-95:and PostDoc .
3-96:students in different areas of computer science and education) .
3-97:First, they installed our Lucene based search engine3 and 3 Clearly, if one had already installed a Desktop search application, then this overhead would not be present .
3-98:indexed all their locally stored content: Files within user selected paths, Emails, and Web Cache .
3-99:Without loss of generality, we focused the experiments on single user machines .
3-100:Then, they chose 4 queries related to their everyday activities, as follows: • One very frequent AltaVista query, as extracted from the top 2% queries most issued to the search engine within a 7.2 million entries log from October 2001 .
3-101:In order to connect such a query to each user"s interests, we added an off line preprocessing phase: We generated the most frequent search requests and then randomly selected a query with at least 10 hits on each subject"s Desktop .
3-102:To further ensure a real life scenario, users were allowed to reject the proposed query and ask for a new one, if they considered it totally outside their interest areas .
3-103:• One randomly selected log query, filtered using the same procedure as above .
3-104:• One self selected specific query, which they thought to have only one meaning .
3-105:• One self selected ambiguous query, which they thought to have at least three meanings .
3-106:The average query lengths were 2.0 and 2.3 terms for the log queries, as well as 2.9 and 1.8 for the self selected ones .
3-107:Even though our algorithms are mainly intended to enhance search when using ambiguous query keywords, we chose to investigate their performance on a wide span of query types, in order to see how they perform in all situations .
3-108:The log queries evaluate real life requests, in contrast to the self selected ones, which target rather the identification of top and bottom performances .
3-109:Note that the former ones were somewhat farther away from each subject"s interest, thus being also more difficult to personalize on .
3-110:To gain an insight into the relationship between each query type and user interests, we asked each person to rate the query itself with a score of 1 to 5, having the following interpretations: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partially, (4) know it well, (5) major interest .
3-111:The obtained grades were 3.11 for the top log queries, 3.72 for the randomly selected ones, 4.45 for the self selected specific ones, and 4.39 for the self selected ambiguous ones .
3-112:For each query, we collected the Top 5 URLs generated by 20 versions of the algorithms4 presented in Section 3.1 .
3-113:These results were then shuffled into one set containing usually between 70 and 90 URLs .
3-114:Thus, each subject had to assess about 325 documents for all four queries, being neither aware of the algorithm, nor of the ranking of each assessed URL .
3-115:Overall, 72 queries were issued and over 6,000 URLs were evaluated during the experiment .
3-116:For each of these URLs, the testers had to give a rating ranging from 0 to 2, dividing the relevant results in two categories, (1) relevant and (2) highly relevant .
3-117:Finally, the quality of each ranking was assessed using the normalized version of Discounted Cumulative Gain [15] .
3-118:DCG is a rich measure, as it gives more weight to highly ranked documents, while also incorporating different relevance levels by giving them different gain values: DCG(i) = & G(1) , if i = 1 DCG(i − 1) + G(i) log(i) , otherwise .
3-119:We used G(i) = 1 for relevant results, and G(i) = 2 for highly relevant ones .
3-120:As queries having more relevant output documents will have a higher DCG, we also normalized its value to a score between 0 (the worst possible DCG given the ratings) and 1 (the best possible DCG given the ratings) to facilitate averaging over queries .
3-121:All results were tested for statistical significance using T tests .
3-122:4 Note that all Desktop level parts of our algorithms were performed with Lucene using its predefined searching and ranking functions .
3-123:Algorithmic specific aspects .
3-124:The main parameter of our algorithms is the number of generated expansion keywords .
3-125:For this experiment we set it to 4 terms for all techniques, leaving an analysis at this level for a subsequent investigation .
3-126:In order to optimize the run time computation speed, we chose to limit the number of output keywords per Desktop document to the number of expansion keywords desired (i.e., four) .
3-127:For all algorithms we also investigated bigger limitations .
3-128:This allowed us to observe that the Lexical Compounds method would perform better if only at most one compound per document were selected .
3-129:We therefore chose to experiment with this new approach as well .
3-130:For all other techniques, considering less than four terms per document did not seem to consistently yield any additional qualitative gain .
3-131:We labeled the algorithms we evaluated as follows: Google API; .
4 TF, DF: Term and Document Frequency; :
4-1:one top compound per document) Lexical Compounds; .
5 SS: Sentence Selection; :
5-1:using respectively Cosine Similarity, Mutual Information, and Likelihood Ratio as similarity coefficients; expansion with synonyms, sub concepts, and super concepts, respectively .
5-2:Except for the thesaurus based expansion, in all cases we also investigated the performance of our algorithms when exploiting only the Web browser cache to represent user"s personal information .
5-3:This is motivated by the fact that other personal documents such as for example emails are known to have a somewhat different language than that residing on the world wide Web [34] .
5-4:However, as this approach performed visibly poorer than using the entire Desktop data, we omitted it from the subsequent analysis .
5-5:3.2.2 Results Log Queries .
5-6:We evaluated all variants of our algorithms using NDCG .
5-7:For log queries, the best performance was achieved with TF, LC[O], and TC[LR] .
5-8:The improvements they brought were up to 5.2% for top queries (p = 0.14) and 13.8% for randomly selected queries (p = 0.01, statistically significant), both obtained with LC[O] .
5-9:A summary of all results is depicted in Table 1 .
5-10:Both TF and LC[O] yielded very good results, indicating that simple keyword and expression oriented approaches might be sufficient for the Desktop based query expansion task .
5-11:LC[O] was much better than LC, ameliorating its quality with up to 25.8% in the case of randomly selected log queries, improvement which was also significant with p = 0.04 .
5-12:Thus, a selection of compounds spanning over several Desktop documents is more informative about user"s interests than the general approach, in which there is no restriction on the number of compounds produced from every personal item .
5-13:The more complex Desktop oriented approaches, namely sentence selection and all term co occurrence based algorithms, showed a rather average performance, with no visible improvements, except for TC[LR] .
5-14:Also, the thesaurus based expansion usually produced very few suggestions, possibly because of the many technical queries employed by our subjects .
5-15:We observed however that expanding with sub concepts is very good for everyday life terms (e.g., car), whereas the use of super concepts is valuable for compounds having at least one term with low technicality (e.g., document clustering) .
5-16:As expected, the synonym based expansion performed generally well, though in some very Algorithm NDCG Signific .
5-17:NDCG Signific .
5-18:Top vs .
5-19:Google Random vs .
5-20:Google Google 0.42 0.40TF 0.43 p = 0.32 0.43 p = 0.04 DF 0.17 0.23LC 0.39 0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01 SS 0.33 0.36TC[CS] 0.37 0.35TC[MI] 0.40 0.36TC[LR] 0.41 0.42 p = 0.06 WN[SYN] 0.42 0.38WN[SUB] 0.28 0.33WN[SUP] 0.26 0.26Table 1: Normalized Discounted Cumulative Gain at the first 5 results when searching for top (left) and random (right) log queries .
5-21:Algorithm NDCG Signific .
5-22:NDCG Signific .
5-23:Clear vs .
5-24:Google Ambiguous vs .
5-25:Google Google 0.71 0.39TF 0.66 0.52 p 0.01 DF 0.37 0.31LC 0.65 0.54 p 0.01 LC[O] 0.69 0.59 p 0.01 SS 0.56 0.52 p 0.01 TC[CS] 0.60 0.50 p = 0.01 TC[MI] 0.60 0.47 p = 0.02 TC[LR] 0.56 0.47 p = 0.03 WN[SYN] 0.70 0.36WN[SUB] 0.46 0.32WN[SUP] 0.51 0.29Table 2: Normalized Discounted Cumulative Gain at the first 5 results when searching for user selected clear (left) and ambiguous (right) queries .
5-26:technical cases it yielded rather general suggestions .
5-27:Finally, we noticed Google to be very optimized for some top frequent queries .
5-28:However, even within this harder scenario, some of our personalization algorithms produced statistically significant improvements over regular search (i.e., TF and LC[O]) .
5-29:Self selected Queries .
5-30:The NDCG values obtained with selfselected queries are depicted in Table 2 .
5-31:While our algorithms did not enhance Google for the clear search tasks, they did produce strong improvements of up to 52.9% (which were of course also highly significant with p 0.01) when utilized with ambiguous queries .
5-32:In fact, almost all our algorithms resulted in statistically significant improvements over Google for this query type .
5-33:In general, the relative differences between our algorithms were similar to those observed for the log based queries .
5-34:As in the previous analysis, the simple Desktop based Term Frequency and Lexical Compounds metrics performed best .
5-35:Nevertheless, a very good outcome was also obtained for Desktop based sentence selection and all term co occurrence metrics .
5-36:There were no visible differences between the behavior of the three different approaches to cooccurrence calculation .
5-37:Finally, for the case of clear queries, we noticed that fewer expansion terms than 4 might be less noisy and thus helpful in bringing further improvements .
5-38:We thus pursued this idea with the adaptive algorithms presented in the next section. .
6 INTRODUCING ADAPTIVITY :
6-1:In the previous section we have investigated the behavior of each technique when adding a fixed number of keywords to the user query .
6-2:However, an optimal personalized query expansion algorithm should automatically adapt itself to various aspects of each query, as well as to the particularities of the person using it .
6-3:In this section we discuss the factors influencing the behavior of our expansion algorithms, which might be used as input for the adaptivity process .
6-4:Then, in the second part we present some initial experiments with one of them, namely query clarity .
6-5:4.1 Adaptivity Factors Several indicators could assist the algorithm to automatically tune the number of expansion terms .
6-6:We start by discussing adaptation by analyzing the query clarity level .
6-7:Then, we briefly introduce an approach to model the generic query formulation process in order to tailor the search algorithm automatically, and discuss some other possible factors that might be of use for this task .
6-8:Query Clarity .
6-9:The interest for analyzing query difficulty has increased only recently, and there are not many papers addressing this topic .
6-10:Yet it has been long known that query disambiguation has a high potential of improving retrieval effectiveness for low recall searches with very short queries [20], which is exactly our targeted scenario .
6-11:Also, the success of IR systems clearly varies across different topics .
6-12:We thus propose to use an estimate number expressing the calculated level of query clarity in order to automatically tweak the amount of personalization fed into the algorithm .
6-13:The following metrics are available: • The Query Length is expressed simply by the number of words in the user query .
6-14:The solution is rather inefficient, as reported by He and Ounis [14] .
6-15:• The Query Scope relates to the IDF of the entire query, as in: C1 = log( #DocumentsInCollection #Hits(Query) ) (7) This metric performs well when used with document collections covering a single topic, but poor otherwise [7, 14] .
6-16:• The Query Clarity [7] seems to be the best, as well as the most applied technique so far .
6-17:It measures the divergence between the language model associated to the user query and the language model associated to the collection .
6-18:In a simplified version (i.e., without smoothing over the terms which are not present in the query), it can be expressed as follows: C2 =  w∈Query Pml(w|Query) · log Pml(w|Query) Pcoll(w) (8) where Pml(w|Query) is the probability of the word w within the submitted query, and Pcoll(w) is the probability of w within the entire collection of documents .
6-19:Other solutions exist, but we think they are too computationally expensive for the huge amount of data that needs to be processed within Web applications .
6-20:We thus decided to investigate only C1 and C2 .
6-21:First, we analyzed their performance over a large set of queries and split their clarity predictions in three categories: • Small Scope Clear Query: C1 ∈ [0, 12], C2 ∈ [4, ∞) .
6-22:• Medium Scope Semi Ambiguous Query: C1 ∈ [12, 17), C2 ∈ [2.5, 4) .
6-23:• Large Scope Ambiguous Query: C1 ∈ [17, ∞), C2 ∈ [0, 2.5] .
6-24:In order to limit the amount of experiments, we analyzed only the results produced when employing C1 for the PIR and C2 for the Web .
6-25:As algorithmic basis we used LC[O], i.e., optimized lexical compounds, which was clearly the winning method in the previous analysis .
6-26:As manual investigation showed it to slightly overfit the expansion terms for clear queries, we utilized a substitute for this particular case .
6-27:Two candidates were considered: (1) TF, i.e., the second best approach, and (2) WN[SYN], as we observed that its first and second expansion terms were often very good .
6-28:Desktop Scope Web Clarity No .
6-29:of Terms Algorithm Large Ambiguous 4 LC[O] Large Semi Ambig .
6-30:3 LC[O] Large Clear 2 LC[O] Medium Ambiguous 3 LC[O] Medium Semi Ambig .
6-31:2 LC[O] Medium Clear 1 TF WN[SYN] Small Ambiguous 2 TF WN[SYN] Small Semi Ambig .
6-32:1 TF WN[SYN] Small Clear 0Table 3: Adaptive Personalized Query Expansion .
6-33:Given the algorithms and clarity measures, we implemented the adaptivity procedure by tailoring the amount of expansion terms added to the original query, as a function of its ambiguity in the Web, as well as within user"s PIR .
6-34:Note that the ambiguity level is related to the number of documents covering a certain query .
6-35:Thus, to some extent, it has different meanings on the Web and within PIRs .
6-36:While a query deemed ambiguous on a large collection such as the Web will very likely indeed have a large number of meanings, this may not be the case for the Desktop .
6-37:Take for example the query PageRank .
6-38:If the user is a link analysis expert, many of her documents might match this term, and thus the query would be classified as ambiguous .
6-39:However, when analyzed against the Web, this is definitely a clear query .
6-40:Consequently, we employed more additional terms, when the query was more ambiguous in the Web, but also on the Desktop .
6-41:Put another way, queries deemed clear on the Desktop were inherently not well covered within user"s PIR, and thus had fewer keywords appended to them .
6-42:The number of expansion terms we utilized for each combination of scope and clarity levels is depicted in Table 3 .
6-43:Query Formulation Process .
6-44:Interactive query expansion has a high potential for enhancing search [29] .
6-45:We believe that modeling its underlying process would be very helpful in producing qualitative adaptive Web search algorithms .
6-46:For example, when the user is adding a new term to her previously issued query, she is basically reformulating her original request .
6-47:Thus, the newly added terms are more likely to convey information about her search goals .
6-48:For a general, non personalized retrieval engine, this could correspond to giving more weight to these new keywords .
6-49:Within our personalized scenario, the generated expansions can similarly be biased towards these terms .
6-50:Nevertheless, more investigations are necessary in order to solve the challenges posed by this approach .
6-51:Other Features .
6-52:The idea of adapting the retrieval process to various aspects of the query, of the user itself, and even of the employed algorithm has received only little attention in the literature .
6-53:Only some approaches have been investigated, usually indirectly .
6-54:There exist studies of query behaviors at different times of day, or of the topics spanned by the queries of various classes of users, etc .
6-55:However, they generally do not discuss how these features can be actually incorporated in the search process itself and they have almost never been related to the task of Web personalization .
6-56:4.2 Experiments We used exactly the same experimental setup as for our previous analysis, with two log based queries and two self selected ones (all different from before, in order to make sure there is no bias on the new approaches), evaluated with NDCG over the Top 5 results output by each algorithm .
6-57:The newly proposed adaptive personalized query expansion algorithms are denoted as A[LCO TF] for the approach using TF with the clear Desktop queries, and as A[LCO WN] when WN[SYN] was utilized instead of TF .
6-58:The overall results were at least similar, or better than Google for all kinds of log queries (see Table 4) .
6-59:For top frequent queries, Algorithm NDCG Signific .
6-60:NDCG Signific .
6-61:Top vs .
6-62:Google Random vs .
6-63:Google Google 0.51 0.45TF 0.51 0.48 p = 0.04 LC[O] 0.53 p = 0.09 0.52 p < 0.01 WN[SYN] 0.51 0.45A[LCO TF] 0.56 p < 0.01 0.49 p = 0.04 A[LCO WN] 0.55 p = 0.01 0.44Table 4: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on top (left) and random (right) log queries .
6-64:Algorithm NDCG Signific .
6-65:NDCG Signific .
6-66:Clear vs .
6-67:Google Ambiguous vs .
6-68:Google Google 0.81 0.46TF 0.76 0.54 p = 0.03 LC[O] 0.77 0.59 p 0.01 WN[SYN] 0.79 0.44A[LCO TF] 0.81 0.64 p 0.01 A[LCO WN] 0.81 0.63 p 0.01 Table 5: Normalized Discounted Cumulative Gain at the first 5 results when using our adaptive personalized search algorithms on user selected clear (left) and ambiguous (right) queries .
6-69:both adaptive algorithms, A[LCO TF] and A[LCO WN], improve with 10.8% and 7.9% respectively, both differences being also statistically significant with p ≤ 0.01 .
6-70:They also achieve an improvement of up to 6.62% over the best performing static algorithm, LC[O] (p = 0.07) .
6-71:For randomly selected queries, even though A[LCO TF] yields significantly better results than Google (p = 0.04), both adaptive approaches fall behind the static algorithms .
6-72:The major reason seems to be the imperfect selection of the number of expansion terms, as a function of query clarity .
6-73:Thus, more experiments are needed in order to determine the optimal number of generated expansion keywords, as a function of the query ambiguity level .
6-74:The analysis of the self selected queries shows that adaptivity can bring even further improvements into Web search personalization (see Table 5) .
6-75:For ambiguous queries, the scores given to Google search are enhanced by 40.6% through A[LCO TF] and by 35.2% through A[LCO WN], both strongly significant with p 0.01 .
6-76:Adaptivity also brings another 8.9% improvement over the static personalization of LC[O] (p = 0.05) .
6-77:Even for clear queries, the newly proposed flexible algorithms perform slightly better, improving with 0.4% and 1.0% respectively .
6-78:All results are depicted graphically in Figure 1 .
6-79:We notice that A[LCO TF] is the overall best algorithm, performing better than Google for all types of queries, either extracted from the search engine log, or self selected .
6-80:The experiments presented in this section confirm clearly that adaptivity is a necessary further step to take in Web search personalization. .
7 CONCLUSIONS AND FURTHER WORK :
7-1:In this paper we proposed to expand Web search queries by exploiting the user"s Personal Information Repository in order to automatically extract additional keywords related both to the query itself and to user"s interests, personalizing the search output .
7-2:In this context, the paper includes the following contributions: • We proposed five techniques for determining expansion terms from personal documents .
7-3:Each of them produces additional query keywords by analyzing user"s Desktop at increasing granularity levels, ranging from term and expression level analysis up to global co occurrence statistics and external thesauri .
7-4:Figure 1: Relative NDCG gain (in %) for each algorithm overall, as well as separated per query category .
7-5:• We provided a thorough empirical analysis of several variants of our approaches, under four different scenarios .
7-6:We showed some of these approaches to perform very well, producing NDCG improvements of up to 51.28% .
7-7:• We moved this personalized search framework further and proposed to make the expansion process adaptive to features of each query, a strong focus being put on its clarity level .
7-8:• Within a separate set of experiments, we showed our adaptive algorithms to provide an additional improvement of 8.47% over the previously identified best approach .
7-9:We are currently performing investigations on the dependency between various query features and the optimal number of expansion terms .
7-10:We are also analyzing other types of approaches to identify query expansion suggestions, such as applying Latent Semantic Analysis on the Desktop data .
7-11:Finally, we are designing a set of more complex combinations of these metrics in order to provide enhanced adaptivity to our algorithms. .
8-1:We thank Ricardo Baeza Yates, Vassilis Plachouras, Carlos Castillo and Vanessa Murdock from Yahoo! for the interesting discussions about the experimental setup and the algorithms we presented
8-2:We are grateful to Fabrizio Silvestri from CNR and to Ronny Lempel from IBM for providing us the AltaVista query log
8-3:Finally, we thank our colleagues from L3S for participating in the time consuming experiments we performed, as well as to the European Commission for the funding support (project Nepomuk, 6th Framework Programme, IST contract no
8-4:027705).
9-1:J
9-2:Allan and H
9-3:Raghavan
9-4:Using part of speech patterns to reduce query ambiguity
9-5:In Proc
9-6:of the 25th Intl
9-7:ACM SIGIR Conf
9-8:on Research and development in information retrieval, 2002
9-9:P
9-10:G
9-11:Anick and S
9-12:Tipirneni
9-13:The paraphrase search assistant: Terminological feedback for iterative information seeking
9-14:In Proc
9-15:of the 22nd Intl
9-16:ACM SIGIR Conf
9-17:on Research and Development in Information Retrieval, 1999
9-18:D
9-19:Carmel, E
9-20:Farchi, Y
9-21:Petruschka, and A
9-22:Soffer
9-23:Automatic query wefinement using lexical affinities with maximal information gain
9-24:In Proc
9-25:of the 25th Intl
9-26:ACM SIGIR Conf
9-27:on Research and development in information retrieval, pages 283 290, 2002
9-28:C
9-29:Carpineto, R
9-30:de Mori, G
9-31:Romano, and B
9-32:Bigi
9-33:An information theoretic approach to automatic query expansion
9-34:ACM TOIS, 19(1):1 27, 2001
9-35:C. H
9-36:Chang and C. C
9-37:Hsu
9-38:Integrating query expansion and conceptual relevance feedback for personalized web information retrieval
9-39:In Proc
9-40:of the 7th Intl
9-41:Conf
9-42:on World Wide Web, 1998
9-43:P
9-44:A
9-45:Chirita, C
9-46:Firan, and W
9-47:Nejdl
9-48:Summarizing local context to personalize global web search
9-49:In Proc
9-50:of the 15th Intl
9-51:CIKM Conf
9-52:on Information and Knowledge Management, 2006
9-53:S
9-54:Cronen Townsend, Y
9-55:Zhou, and W
9-56:B
9-57:Croft
9-58:Predicting query performance
9-59:In Proc
9-60:of the 25th Intl
9-61:ACM SIGIR Conf
9-62:on Research and development in information retrieval, 2002
9-63:H
9-64:Cui, J. R
9-65:Wen, J. Y
9-66:Nie, and W. Y
9-67:Ma
9-68:Probabilistic query expansion using query logs
9-69:In Proc
9-70:of the 11th Intl
9-71:Conf
9-72:on World Wide Web, 2002
9-73:T
9-74:Dunning
9-75:Accurate methods for the statistics of surprise and coincidence
9-76:Computational Linguistics, 19:61 74, 1993
9-77:H
9-78:P
9-79:Edmundson
9-80:New methods in automatic extracting
9-81:Journal of the ACM, 16(2):264 285, 1969
9-82:E
9-83:N
9-84:Efthimiadis
9-85:User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion
9-86:Information Processing and Management, 31(4):605 620, 1995
9-87:D
9-88:Fogaras and B
9-89:Racz
9-90:Scaling link based similarity search
9-91:In Proc
9-92:of the 14th Intl
9-93:World Wide Web Conf., 2005
9-94:T
9-95:Haveliwala
9-96:Topic sensitive pagerank
9-97:In Proc
9-98:of the 11th Intl
9-99:World Wide Web Conf., Honolulu, Hawaii, May 2002
9-100:B
9-101:He and I
9-102:Ounis
9-103:Inferring query performance using pre retrieval predictors
9-104:In Proc
9-105:of the 11th Intl
9-106:SPIRE Conf
9-107:on String Processing and Information Retrieval, 2004
9-108:K
9-109:J¨arvelin and J
9-110:Keklinen
9-111:Ir evaluation methods for retrieving highly relevant documents
9-112:In Proc
9-113:of the 23th Intl
9-114:ACM SIGIR Conf
9-115:on Research and development in information retrieval, 2000
9-116:G
9-117:Jeh and J
9-118:Widom
9-119:Scaling personalized web search
9-120:In Proc
9-121:of the 12th Intl
9-122:World Wide Web Conference, 2003
9-123:M. C
9-124:Kim and K. S
9-125:Choi
9-126:A comparison of collocation based similarity measures in query expansion
9-127:Inf
9-128:Proc
9-129:and Mgmt., 35(1):19 30, 1999
9-130:S. B
9-131:Kim, H. C
9-132:Seo, and H. C
9-133:Rim
9-134:Information retrieval using word senses: root sense tagging approach
9-135:In Proc
9-136:of the 27th Intl
9-137:ACM SIGIR Conf
9-138:on Research and development in information retrieval, 2004
9-139:R
9-140:Kraft and J
9-141:Zien
9-142:Mining anchor text for query refinement
9-143:In Proc
9-144:of the 13th Intl
9-145:Conf
9-146:on World Wide Web, 2004
9-147:R
9-148:Krovetz and W
9-149:B
9-150:Croft
9-151:Lexical ambiguity and information retrieval
9-152:ACM Trans
9-153:Inf
9-154:Syst., 10(2), 1992
9-155:A
9-156:M
9-157:Lam Adesina and G
9-158:J
9-159:F
9-160:Jones
9-161:Applying summarization techniques for term selection in relevance feedback
9-162:In Proc
9-163:of the 24th Intl
9-164:ACM SIGIR Conf
9-165:on Research and Development in Information Retrieval, 2001
9-166:S
9-167:Liu, F
9-168:Liu, C
9-169:Yu, and W
9-170:Meng
9-171:An effective approach to document retrieval via utilizing wordnet and recognizing phrases
9-172:In Proc
9-173:of the 27th Intl
9-174:ACM SIGIR Conf
9-175:on Research and development in information retrieval, 2004
9-176:G
9-177:Miller
9-178:Wordnet: An electronic lexical database
9-179:Communications of the ACM, 38(11):39 41, 1995
9-180:L
9-181:Nie, B
9-182:Davison, and X
9-183:Qi
9-184:Topical link analysis for web search
9-185:In Proc
9-186:of the 29th Intl
9-187:ACM SIGIR Conf
9-188:on Res
9-189:and Development in Inf
9-190:Retr., 2006
9-191:L
9-192:Page, S
9-193:Brin, R
9-194:Motwani, and T
9-195:Winograd
9-196:The PageRank citation ranking: Bringing order to the web
9-197:Technical report, Stanford Univ., 1998
9-198:F
9-199:Qiu and J
9-200:Cho
9-201:Automatic indentification of user interest for personalized search
9-202:In Proc
9-203:of the 15th Intl
9-204:WWW Conf., 2006
9-205:Y
9-206:Qiu and H. P
9-207:Frei
9-208:Concept based query expansion
9-209:In Proc
9-210:of the 16th Intl
9-211:ACM SIGIR Conf
9-212:on Research and Development in Inf
9-213:Retr., 1993
9-214:J
9-215:Rocchio
9-216:Relevance feedback in information retrieval
9-217:The Smart Retrieval System: Experiments in Automatic Document Processing, pages 313 323, 1971
9-218:I
9-219:Ruthven
9-220:Re examining the potential effectiveness of interactive query expansion
9-221:In Proc
9-222:of the 26th Intl
9-223:ACM SIGIR Conf., 2003
9-224:T
9-225:Sarlos, A
9-226:A
9-227:Benczur, K
9-228:Csalogany, D
9-229:Fogaras, and B
9-230:Racz
9-231:To randomize or not to randomize: Space optimal summaries for hyperlink analysis
9-232:In Proc
9-233:of the 15th Intl
9-234:WWW Conf., 2006
9-235:C
9-236:Shah and W
9-237:B
9-238:Croft
9-239:Evaluating high accuracy retrieval techniques
9-240:In Proc
9-241:of the 27th Intl
9-242:ACM SIGIR Conf
9-243:on Research and development in information retrieval, pages 2 9, 2004
9-244:K
9-245:Sugiyama, K
9-246:Hatano, and M
9-247:Yoshikawa
9-248:Adaptive web search based on user profile constructed without any effort from users
9-249:In Proc
9-250:of the 13th Intl
9-251:World Wide Web Conf., 2004
9-252:D
9-253:Sullivan
9-254:The older you are, the more you want personalized search, 2004
9-255:http:  searchenginewatch.com searchday article.php 3385131
9-256:J
9-257:Teevan, S
9-258:Dumais, and E
9-259:Horvitz
9-260:Personalizing search via automated analysis of interests and activities
9-261:In Proc
9-262:of the 28th Intl
9-263:ACM SIGIR Conf
9-264:on Research and Development in Information Retrieval, 2005
9-265:E
9-266:Volokh
9-267:Personalization and privacy
9-268:Commun
9-269:ACM, 43(8), 2000
9-270:E
9-271:M
9-272:Voorhees
9-273:Query expansion using lexical semantic relations
9-274:In Proc
9-275:of the 17th Intl
9-276:ACM SIGIR Conf
9-277:on Res
9-278:and development in Inf
9-279:Retr., 1994
9-280:J
9-281:Xu and W
9-282:B
9-283:Croft
9-284:Query expansion using local and global document analysis
9-285:In Proc
9-286:of the 19th Intl
9-287:ACM SIGIR Conf
9-288:on Research and Development in Information Retrieval, 1996
9-289:S
9-290:Yu, D
9-291:Cai, J. R
9-292:Wen, and W. Y
9-293:Ma
9-294:Improving pseudo relevance feedback in web information retrieval using web page segmentation
9-295:In Proc
9-296:of the 12th Intl
9-297:Conf
9-298:on World Wide Web, 2003
picture:
