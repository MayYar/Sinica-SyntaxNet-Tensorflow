Self-Adaptive Applications on the Grid 
content:
1 Abstract :
1-1:Grids are inherently heterogeneous and dynamic .
1-2:One important problem in grid computing is resource selection, that is, finding an appropriate resource set for the application .
1-3:Another problem is adaptation to the changing characteristics of the grid environment .
1-4:Existing solutions to these two problems require that a performance model for an application is known .
1-5:However, constructing such models is a complex task .
1-6:In this paper, we investigate an approach that does not require performance models .
1-7:We start an application on any set of resources .
1-8:During the application run, we periodically collect the statistics about the application run and deduce application requirements from these statistics .
1-9:Then, we adjust the resource set to better fit the application needs .
1-10:This approach allows us to avoid performance bottlenecks, such as overloaded WAN links or very slow processors, and therefore can yield significant performance improvements .
1-11:We evaluate our approach in a number of scenarios typical for the Grid .
1-12:Categories and Subject Descriptors C.2.4 [COMPUTER COMMUNICATION NETWORKS]: Distributed Systems Distributed .
2 Introduction :
2-1:In recent years, grid computing has become a real alternative to traditional parallel computing .
2-2:A grid provides much computational power, and thus offers the possibility to solve very large problems, especially if applications can run on multiple sites at the same time (7; 15; 20) .
2-3:However, the complexity of Grid environments also is many times larger than that of traditional parallel machines like clusters and supercomputers .
2-4:One important problem is resource selection selecting a set of compute nodes such that the application achieves good performance .
2-5:Even in traditional, homogeneous parallel environments, finding the optimal number of nodes is a hard problem and is often solved in a trial and error fashion .
2-6:In a grid environment this problem is even more difficult, because of the heterogeneity of resources: the compute nodes have various speeds and the quality of network connections between them varies from low latency and high bandwidth local area networks (LANs) to high latency and possibly low bandwidth wide area networks (WANs) .
2-7:Another important problem is that the performance and availability of grid resources varies over time: the network links or compute nodes may become overloaded, or the compute nodes may become unavailable because of crashes or because they have been claimed by a higher priority application .
2-8:Also, new, better resources may become available .
2-9:To maintain a reasonable performance level, the application therefore needs to adapt to the changing conditions .
2-10:The adaptation problem can be reduced to the resource selection problem: the resource selection phase can be repeated during application execution, either at regular intervals, or when a performance problem is detected, or when new resources become available .
2-11:This approach has been adopted by a number of systems (5; 14; 18) .
2-12:For resource selection, the application runtime is estimated for some resource sets and the set that yields the shortest runtime is selected for execution .
2-13:Predicting the application runtime on a given set of resources, however, requires knowledge about the application .
2-14:Typically, an analytical performance model is used, but constructing such a model is inherently difficult and requires an expertise which application programmers may not have .
2-15:In this paper, we introduce and evaluate an alternative approach to application adaptation and resource selection which does not need a performance model .
2-16:We start an application on any set of resources .
2-17:During the application run, we periodically collect information about the communication times and idle times of the processors .
2-18:We use these statistics to automatically estimate the resource requirements of the application .
2-19:Next, we adjust the resource set the application is running on by adding or removing compute nodes or even entire clusters .
2-20:Our adaptation strategy uses the work by Eager et al .
2-21:(10) to determine the efficiency and tries to keep the efficiency of the application between a lower and upper threshold derived from their theory .
2-22:Processors are added or deleted to stay between the thresholds, thus adapting automatically to the changing environment .
2-23:A major advantage of our approach is that it improves application performance in many different situations that are typical for grid computing .
2-24:It handles all of the following cases: • automatically adapting the number of processors to the degree of parallelism in the application, even when this degree changes dynamically during the computation • migrating (part of) a computation away from overloaded resources • removing resources with poor communication links that slow down the computation • adding new resources to replace resources that have crashed Our work assumes the application is malleable and can run (efficiently) on multiple sites of a grid (i.e., using co allocation (15)) .
2-25:It should not use static load balancing or be very sensitive to wide121 area latencies .
2-26:We have applied our ideas to divide and conquer applications, which satisfy these requirements .
2-27:Divide and conquer has been shown to be an attractive paradigm for programming grid applications (4; 20) .
2-28:We believe that our approach can be extended to other classes of applications with the given assumptions .
2-29:We implemented our strategy in Satin, which is a Java centric framework for writing grid enabled divide and conquer applications (20) .
2-30:We evaluate the performance of our approach on the DAS 2 wide area system and we will show that our approach yields major performance improvements (roughly 10 60 %) in the above scenarios .
2-31:The rest of this paper is structured as follows .
2-32:In Section 2, we explain what assumptions we are making about the applications and grid resources .
2-33:In Section 3, we present our resource selection and adaptation strategy .
2-34:In Section 4, we describe its implementation in the Satin framework .
2-35:In Section 5, we evaluate our approach in a number of grid scenarios .
2-36:In Section 6, we compare our approach with the related work .
2-37:Finally, in Section 7, we conclude and describe future work. .
3 Background and assumptions :
3-1:In this section, we describe our assumptions about the applications and their resources .
3-2:We assume the following resource model .
3-3:The applications are running on multiple sites at the same time, where sites are clusters or supercomputers .
3-4:We also assume that the processors of the sites are accessible using a grid scheduling system, such as Koala (15), Zorilla (9) or GRMS (3) .
3-5:Processors belonging to one site are connected by a fast LAN with a low latency and high bandwidth .
3-6:The different sites are connected by a WAN .
3-7:Communication between sites suffers from high latencies .
3-8:We assume that the links connecting the sites with the Internet backbone might become bottlenecks causing the inter site communication to suffer from low bandwidths .
3-9:We studied the adaptation problem in the context of divide andconquer applications .
3-10:However, we believe that our methodology can be used for other types of applications as well .
3-11:In this section we summarize the assumptions about applications that are important to our approach .
3-12:The first assumption we make is that the application is malleable, i.e., it is able to handle processors joining and leaving the on going computation .
3-13:In (23), we showed how divide andconquer applications can be made fault tolerant and malleable .
3-14:Processors can be added or removed at any point in the computation with little overhead .
3-15:The second assumption is that the application can efficiently run on processors with different speeds .
3-16:This can be achieved by using a dynamic load balancing strategy, such as work stealing used by divide and conquer applications (19) .
3-17:Also, master worker applications typically use dynamic load balancing strategies (e.g., MW a framework for writing gridenabled master worker applications (12)) .
3-18:We find it a reasonable assumption for a grid application, since applications for which the slowest processor becomes a bottleneck will not be able to efficiently utilize grid resources .
3-19:Finally, the application should be insensitive to wide area latencies, so it can run efficiently on a widearea grid (16; 17). .
4 Self adaptation :
4-1:In this section we will explain how we use application malleability to find a suitable set of resources for a given application and to adapt to changing conditions in the grid environment .
4-2:In order to monitor the application performance and guide the adaptation, we added an extra process to the computation which we call adaptation coordinator .
4-3:The adaptation coordinator periodically collects performance statistics from the application processors .
4-4:We introduce a new application performance metric: weighted average efficiency which describes the application performance on a heterogeneous set of resources .
4-5:The coordinator uses statistics from application processors to compute the weighted average efficiency .
4-6:If the efficiency falls above or below certain thresholds, the coordinator decides on adding or removing processors .
4-7:A heuristic formula is used to decide which processors have to be removed .
4-8:During this process the coordinator learns the application requirements by remembering the characteristics of the removed processors .
4-9:These requirements are then used to guide the adding of new processors .
4-10:3.1 Weighted average efficiency In traditional parallel computing, a standard metric describing the performance of a parallel application is efficiency .
4-11:Efficiency is defined as the average utilization of the processors, that is, the fraction of time the processors spend doing useful work rather than being idle or communicating with other processors (10) .
4-12:efficiency = 1 n ∗ n i=0 (1 − overheadi ) where n is the number of processors and overheadi is the fraction of time the ith processor spends being idle or communicating .
4-13:Efficiency indicates the benefit of using multiple processors .
4-14:Typically, the efficiency drops as new processors are added to the computation .
4-15:Therefore, achieving a high speedup (and thus a low execution time) and achieving a high system utilization are conflicting goals (10) .
4-16:The optimal number of processors is the number for which the ratio of efficiency to execution time is maximized .
4-17:Adding processors beyond this number yields little benefit .
4-18:This number is typically hard to find, but in (10) it was theoretically proven that if the optimal number of processors is used, the efficiency is at least 50% .
4-19:Therefore, adding processors when efficiency is smaller or equal to 50% will only decrease the system utilization without significant performance gains .
4-20:For heterogeneous environments with different processor speeds, we extended the notion of efficiency and introduced weighted average efficiency .
4-21:wa efficiency = 1 n ∗ n i=0 speedi ∗ (1 − overheadi ) The useful work done by a processor (1 − overheadi) is weighted by multiplying it by the speed of this processor relative to the fastest processor .
4-22:The fastest processor has speed = 1, for others holds: 0 < speed ≤ 1 .
4-23:Therefore, slower processors are modeled as fast ones that spend a large fraction of the time being idle .
4-24:Weighted average efficiency reflects the fact that adding slow processors yields less benefit than adding fast processors .
4-25:In the heterogeneous world, it is hardly beneficial to add processors if the efficiency is lower than 50% unless the added processor is faster than some of the currently used processors .
4-26:Adding faster processors might be beneficial regardless of the efficiency .
4-27:3.2 Application monitoring Each processor measures the time it spends communicating or being idle .
4-28:The computation is divided into monitoring periods .
4-29:After each monitoring period, the processors compute their overhead over this period as the percentage of the time they spent being idle or communicating in this period .
4-30:Apart from total overhead, each processor also computes the overhead of inter cluster and intracluster communication .
4-31:To calculate weighted average efficiency, we need to know the relative speeds of the processors, which depend on the application and the problem size used .
4-32:Since it is impractical to run the 122 whole application on each processor separately, we use applicationspecific benchmarks .
4-33:Currently we use the same application with a small problem size as a benchmark and we require the application programmer to specify this problem size .
4-34:This approach requires extra effort from the programmer to find the right problem size and possibly to produce input files for this problem size, which may be hard .
4-35:In the future, we are planning to generate benchmarks automatically by choosing a random subset of the task graph of the original application .
4-36:Benchmarks have to be re run periodically because the speed of a processor might change if it becomes overloaded by another application (for time shared machines) .
4-37:There is a trade off between the accuracy of speed measurements and the overhead it incurs .
4-38:The longer the benchmark, the greater the accuracy of the measurement .
4-39:The more often it is run, the faster changes in processor speed are detected .
4-40:In our current implementation, the application programmer specifies the length of the benchmark (by specifying its problem size) and the maximal overhead it is allowed to cause .
4-41:Processors run the benchmark at such frequency so as not to exceed the specified overhead .
4-42:In the future, we plan to combine benchmarking with monitoring the load of the processor which would allow us to avoid running the benchmark if no change in processor load is detected .
4-43:This optimization will further reduce the benchmarking overhead .
4-44:Note that the benchmarking overhead could be avoided completely for more regular applications, for example, for masterworker applications with tasks of equal or similar size .
4-45:The processor speed could then be measured by counting the tasks processed by this processor within one monitoring period .
4-46:Unfortunately, divide and conquer applications typically exhibit a very irregular structure .
4-47:The sizes of tasks can vary by many orders of magnitude .
4-48:At the end of each monitoring period, the processors send the overhead statistics and processor speeds to the coordinator .
4-49:Periodically, the coordinator computes the weighted average efficiency and other statistics, such as average inter cluster overhead or overheads in each cluster .
4-50:The clocks of the processors are not synchronized with each other or with the clock of the coordinator .
4-51:Each processor decides separately when it is time to send data .
4-52:Occasionally, the coordinator may miss data at the end of a monitoring period, so it has to use data from the previous monitoring period for these processors .
4-53:This causes small inaccuracies in the calculations of the coordinator, but does not influence the performance of adaptation .
4-54:3.3 Adaptation strategy The adaptation coordinator tries to keep the weighted average efficiency between Emin and Emax .
4-55:When it exceeds Emax, the coordinator requests new processors from the scheduler .
4-56:The number of requested processors depends on the current efficiency: the higher the efficiency, the more processors are requested .
4-57:The coordinator starts removing processors when the weighted average efficiency drops below Emin .
4-58:The number of nodes that are removed again depends on the weighted average efficiency .
4-59:The lower the efficiency, the more nodes are removed .
4-60:The thresholds we use are Emax = 50%, because we know that adding processors when efficiency is lower does not make sense, and Emin = 30% .
4-61:Efficiency of 30% or lower might indicate performance problems such as low bandwidth or overloaded processors .
4-62:In that case, removing bad processors will be beneficial for the application .
4-63:Such low efficiency might also indicate that we simply have too many processors .
4-64:In that case, removing some processors may not be beneficial but it will not harm the application .
4-65:The coordinator always tries to remove the worst processors .
4-66:The badness of a processor is determined by the following formula: proc badnessi = α ∗ 1 speedi + β ∗ ic overheadi + γ ∗ inW orstCluster(i) The processor is considered bad if it has low speed ( 1 speed is big) and high inter cluster overhead (ic overhead) .
4-67:High intercluster overhead indicates that the bandwidth to this processor"s cluster is insufficient .
4-68:Removing processors located in a single cluster is desirable since it decreases the amount of wide area communication .
4-69:Therefore, processors belonging to the worst cluster are preferred .
4-70:Function inW orstCluster(i) returns 1 for processors belonging to the worst cluster and 0 otherwise .
4-71:The badness of clusters is computed similarly to the badness of processors: cluster badnessi = α ∗ 1 speedi + β ∗ ic overheadi The speed of a cluster is the sum of processor speeds normalized to the speed of the fastest cluster .
4-72:The ic overhead of a cluster is an average of processor inter cluster overheads .
4-73:The α, β and γ coefficients determine the relative importance of the terms .
4-74:Those coefficients are established empirically .
4-75:Currently we are using the following values: α = 1, β = 100 and γ = 10, based on the observation that ic overhead > 0.2 indicates bandwidth problems and processors with speed < 0.05 do not contribute to the computation .
4-76:Additionally, when one of the clusters has an exceptionally high inter cluster overhead (larger than 0.25), we conclude that the bandwidth on the link between this cluster and the Internet backbone is insufficient for the application .
4-77:In that case, we simply remove the whole cluster instead of computing node badness and removing the worst nodes .
4-78:After deciding which nodes are removed, the coordinator sends a message to these nodes and the nodes leave the computation .
4-79:Figure 1 shows a schematic view of the adaptation strategy .
4-80:Dashed lines indicate a part that is not supported yet, as will be explained below .
4-81:This simple adaptation strategy allows us to improve application performance in several situations typical for the Grid: • If an application is started on fewer processors than its degree of parallelism allows, it will automatically expand to more processors (as soon as there are extra resources available) .
4-82:Conversely, if an application is started on more processors than it can efficiently use, a part of the processors will be released .
4-83:• If an application is running on an appropriate set of resources but after a while some of the resources (processors and or network links) become overloaded and slow down the computation, the overloaded resources will be removed .
4-84:After removing the overloaded resources, the weighted average efficiency will increase to above the Emax threshold and the adaptation coordinator will try to add new resources .
4-85:Therefore, the application will be migrated from overloaded resources .
4-86:• If some of the original resources chosen by the user are inappropriate for the application, for example the bandwidth to one of the clusters is too small, the inappropriate resources will be removed .
4-87:If necessary, the adaptation component will try to add other resources .
4-88:• If during the computation a substantial part of the processors will crash, the adaptation component will try to add new resources to replace the crashed processors .
4-89:123 0 2000 4000 6000 runtime(secs.) Scenario 0 a b c Scenario 1 Scenario 2 Scenario 3 Scenario 4 Scenario 5 without monitoring and adaptation (runtime 1) with monitoring and adaptation (runtime 2) with monitoring but no adaptation (runtime 3) Figure 2 .
4-90:The runtimes of the Barnes Hut application, scenarios 0 5 add nodes faster nodes available if compute weighted average efficiency E wa wait & collect statistics rank nodes remove worst nodes waE Ewa Y N N Y above if below if Emin maxE Figure 1 .
4-91:Adaptation strategy • If the application degree of parallelism is changing during the computation, the number of nodes the application is running on will be automatically adjusted .
4-92:Further improvements are possible, but require extra functionality from the grid scheduler and or integration with monitoring services such as NWS (22) .
4-93:For example, adding nodes to a computation can be improved .
4-94:Currently, we add any nodes the scheduler gives us .
4-95:However, it would be more efficient to ask for the fastest processors among the available ones .
4-96:This could be done, for example, by passing a benchmark to the grid scheduler, so that it can measure processor speeds in an application specific way .
4-97:Typically, it would be enough to measure the speed of one processor per site, since clusters and supercomputers are usually homogeneous .
4-98:An alternative approach would be ranking the processors based on parameters such as clock speed and cache size .
4-99:This approach is sometimes used for resource selection for sequential applications (14) .
4-100:However, it is less accurate than using an application specific benchmark .
4-101:Also, during application execution, we can learn some application requirements and pass them to the scheduler .
4-102:One example is minimal bandwidth required by the application .
4-103:The lower bound on minimal required bandwidth is tightened each time a cluster with high inter cluster overhead is removed .
4-104:The bandwidth between each pair of clusters is estimated during the computation by measuring data transfer times, and the bandwidth to the removed cluster is set as a minimum .
4-105:Alternatively, information from a grid monitoring system can be used .
4-106:Such bounds can be passed to the scheduler to avoid adding inappropriate resources .
4-107:It is especially important when migrating from resources that cause performance problems: we have to be careful not to add the resources we have just removed .
4-108:Currently we use blacklisting we simply do not allow adding resources we removed before .
4-109:This means, however, that we cannot use these resources even if the cause of the performance problem disappears, e.g .
4-110:the bandwidth of a link might improve if the background traffic diminishes .
4-111:We are currently not able to perform opportunistic migration migrating to better resources when they are discovered .
4-112:If an application runs with efficiency between Emin and Emax, the adaptation component will not undertake any action, even if better resources become available .
4-113:Enabling opportunistic migration requires, again, the ability to specify to the scheduler what better resources are (faster, with a certain minimal bandwidth) and receiving notifications when such resources become available .
4-114:Existing grid schedulers such as GRAM from the Globus Toolkit (11) do not support such functionality .
4-115:The developers of the KOALA metascheduler (15) have recently started a project whose goal is to provide support for adaptive applications .
4-116:We are currently discussing with them the possibility of providing the functionalities required by us, aiming to extend our adaptivity strat124 egy to support opportunistic migration and to improve the initial resource selection. .
5 Implementation :
5-1:We incorporated our adaptation mechanism into Satin a Java framework for creating grid enabled divide and conquer applications .
5-2:With Satin, the programmer annotates the sequential code with divide and conquer primitives and compiles the annotated code with a special Satin compiler that generates the necessary communication and load balancing code .
5-3:Satin uses a very efficient, grid aware load balancing algorithm Cluster aware Random Work Stealing (19), which hides wide area latencies by overlapping local and remote stealing .
5-4:Satin also provides transparent fault tolerance and malleability (23) .
5-5:With Satin, removing and adding processors from to an ongoing computation incurs little overhead .
5-6:We instrumented the Satin runtime system to collect runtime statistics and send them to the adaptation coordinator .
5-7:The coordinator is implemented as a separate process .
5-8:Both coordinator and Satin are implemented entirely in Java on top of the Ibis communication library (21) .
5-9:The core of Ibis is also implemented in Java .
5-10:The resulting system therefore is highly portable (due to Java"s write once, run anywhere property) allowing the software to run unmodified on a heterogeneous grid .
5-11:Ibis also provides the Ibis Registry .
5-12:The Registry provides, among others, a membership service to the processors taking part in the computation .
5-13:The adaptation coordinator uses the Registry to discover the application processes, and the application processes use this service to discover each other .
5-14:The Registry also offers fault detection (additional to the fault detection provided by the communication channels) .
5-15:Finally, the Registry provides the possibility to send signals to application processes .
5-16:The coordinator uses this functionality to notify the processors that they need to leave the computation .
5-17:Currently the Registry is implemented as a centralized server .
5-18:For requesting new nodes, the Zorilla (9) system is used a peer to peer supercomputing middleware which allows straightforward allocation of processors in multiple clusters and or supercomputers .
5-19:Zorilla provides locality aware scheduling, which tries to allocate processors that are located close to each other in terms of communication latency .
5-20:In the future, Zorilla will also support bandwidth aware scheduling, which tries to maximize the total bandwidth in the system .
5-21:Zorilla can be easily replaced with another grid scheduler .
5-22:In the future, we are planning to integrate our adaptation component with GAT (3) which is becoming a standard in the grid community and KOALA (15) a scheduler that provides co allocation on top of standard grid middleware, such as the Globus Toolkit (11). .
6 Performance evaluation :
6-1:In this section, we will evaluate our approach .
6-2:We will demonstrate the performance of our mechanism in a few scenarios .
6-3:The first scenario is an ideal situation: the application runs on a reasonable set of nodes (i.e., such that the efficiency is around 50%) and no problems such as overloaded networks and processors, crashing processors etc .
6-4:occur .
6-5:This scenario allows us to measure the overhead of the adaptation support .
6-6:The remaining scenarios are typical for grid environments and demonstrate that with our adaptation support the application can avoid serious performance bottlenecks such as overloaded processors or network links .
6-7:For each scenario, we compare the performance of an application with adaptation support to a non adaptive version .
6-8:In the non adaptive version, the coordinator does not collect statistics and no benchmarking (for measuring processor speeds) is performed .
6-9:In the ideal scenario, 0 5 10 15 iteration number 0 200 400 600 iterationduration(secs.) starting on 8 nodes starting on 16 nodes starting on 24 nodes starting on 8 nodes starting on 16 nodes starting on 24 nodes }no adaptation }with adaptation Figure 3 .
6-10:Barnes Hut iteration durations with without adaptation, too few processors 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation CPU load introduced overloaded nodes removed started adding nodes 36 nodes reached Figure 4 .
6-11:Barnes Hut iteration durations with without adaptation, overloaded CPUs we additionally measure the performance of an application with collecting statistics and benchmarking turned on but without doing adaptation, that is, without allowing it to change the number of nodes .
6-12:This allows us to measure the overhead of benchmarking and collecting statistics .
6-13:In all experiments we used a monitoring period of 3 minutes for the adaptive versions of the applications .
6-14:All the experiments were carried out on the DAS 2 wide area system (8), which consists of five clusters located at five Dutch uni125 versities .
6-15:One of the clusters consists of 72 nodes, the others of 32 nodes .
6-16:Each node contains two 1 GHz Pentium processors .
6-17:Within a cluster, the nodes are connected by Fast Ethernet .
6-18:The clusters are connected by the Dutch university Internet backbone .
6-19:In our experiments, we used the Barnes Hut N body simulation .
6-20:BarnesHut simulates the evolution of a large set of bodies under influence of (gravitational or electrostatic) forces .
6-21:The evolution of N bodies is simulated in iterations of discrete time steps .
6-22:5.1 Scenario 0: adaptivity overhead In this scenario, the application is started on 36 nodes .
6-23:The nodes are equally divided over 3 clusters (12 nodes in each cluster) .
6-24:On this number of nodes, the application runs with 50% efficiency, so we consider it a reasonable number of nodes .
6-25:As mentioned above, in this scenario we measured three runtimes: the runtime of the application without adaptation support (runtime 1), the runtime with adaptation support (runtime 2) and the runtime with monitoring (i.e., collection of statistics and benchmarking) turned on but without allowing it to change the number of nodes (runtime 3) .
6-26:Those runtimes are shown in Figure 2, first group of bars .
6-27:The comparison between runtime 3 and 1 shows the overhead of adaptation support .
6-28:In this experiment it is around 15% .
6-29:Almost all overhead comes from benchmarking .
6-30:The benchmark is run 1 2 times per monitoring period .
6-31:This overhead can be made smaller by increasing the length of the monitoring period and decreasing the benchmarking frequency .
6-32:The monitoring period we used (3 minutes) is relatively short, because the runtime of the application was also relatively short (30 60 minutes) .
6-33:Using longer running applications would not allow us to finish the experimentation in a reasonable time .
6-34:However, real world grid applications typically need hours, days or even weeks to complete .
6-35:For such applications, a much longer monitoring period can be used and the adaptation overhead can be kept much lower .
6-36:For example, with the Barnes Hut application, if the monitoring period is extended to 10 minutes, the overhead drops to 6% .
6-37:Note that combining benchmarking with monitoring processor load (as described in Section 3.2) would reduce the benchmarking overhead to almost zero: since the processor load is not changing, the benchmarks would only need to be run at the beginning of the computation .
6-38:5.2 Scenario 1: expanding to more nodes In this scenario, the application is started on fewer nodes than the application can efficiently use .
6-39:This may happen because the user does not know the right number of nodes or because insufficient nodes were available at the moment the application was started .
6-40:We tried 3 initial numbers of nodes: 8 (Scenario 1a), 16 (Scenario 1b) and 24 (Scenario 1c) .
6-41:The nodes were located in 1 or 2 clusters .
6-42:In each of the three sub scenarios, the application gradually expanded to 36 40 nodes located in 4 clusters .
6-43:This allowed to reduce the application runtimes by 50% (Scenario 1a), 35% (Scenario 1b) and 12% (Scenario 1c) with respect to the non adaptive version .
6-44:Those runtimes are shown in Figure 2 .
6-45:Since Barnes Hut is an iterative application, we also measured the time of each iteration, as shown in Figure 3 .
6-46:Adaptation reduces the iteration time by a factor of 3 (Scenario 1a), 1.7 (Scenario 1b) and 1.2 (Scenario 1c) which allows us to conclude that the gains in the total runtime would be even bigger if the application were run longer than for 15 iterations .
6-47:5.3 Scenario 2: overloaded processors In this scenario, we started the application on 36 nodes in 3 clusters .
6-48:After 200 seconds, we introduced a heavy, artificial load on the processors in one of the clusters .
6-49:Such a situation may happen when an application with a higher priority is started on some of the resources .
6-50:Figure 4 shows the iteration durations of both the adaptive and non adaptive versions .
6-51:After introducing the load, the iteration 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation one cluster is badly connected badly connected cluster removed started adding nodes 36 nodes reached Figure 5 .
6-52:Barnes Hut iteration durations with without adaptation, overloaded network link 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation one cluster is badly connected 12 nodes lightly overloaded removed badly connected cluster removed 2 lightly overloaded nodes Figure 6 .
6-53:Barnes Hut iteration durations with without adaptation, overloaded CPUs and an overloaded network link duration increased by a factor of 2 to 3 .
6-54:Also, the iteration times became very variable .
6-55:The adaptive version reacted by removing the overloaded nodes .
6-56:After removing these nodes, the weighted average efficiency rose to around 35% which triggered adding new nodes and the application expanded back to 38 nodes .
6-57:So, the overloaded nodes were replaced by better nodes, which brought the iteration duration back to the initial values .
6-58:This reduced the total runtime by 14% .
6-59:The runtimes are shown in Figure 2 .
6-60:126 5.4 Scenario 3: overloaded network link In this scenario, we ran the application on 36 nodes in 3 clusters .
6-61:We simulated that the uplink to one of the clusters was overloaded and the bandwidth on this uplink was reduced to approximately 100 KB s .
6-62:To simulate low bandwidth we use the traffic shaping techniques described in (6) .
6-63:The iteration durations in this experiment are shown in Figure 5 .
6-64:The iteration durations of the nonadaptive version exhibit enormous variation: from 170 to 890 seconds .
6-65:The adaptive version removed the badly connected cluster after the first monitoring period .
6-66:As a result, the weighted average efficiency rose to around 35% and new nodes were gradually added until their number reached 38 .
6-67:This brought the iteration times down to around 100 seconds .
6-68:The total runtime was reduced by 60% (Figure 2) .
6-69:5.5 Scenario 4: overloaded processors and an overloaded network link In this scenario, we ran the application on 36 nodes in 3 clusters .
6-70:Again, we simulated an overloaded uplink to one of the clusters .
6-71:Additionally, we simulated processors with heterogeneous speeds by inserting a relatively light artificial load on the processors in one of the remaining clusters .
6-72:The iteration durations are shown in Figure 6 .
6-73:Again, the non adaptive version exhibits a great variation in iteration durations: from 200 to 1150 seconds .
6-74:The adaptive version removes the badly connected cluster after the first monitoring period which brings the iteration duration down to 210 seconds on average .
6-75:After removing one of the clusters, since some of the processors are slower (approximately 5 times), the weighted average efficiency raises only to around 40% .
6-76:Since this value lies between Emin and Emax, no nodes are added or removed .
6-77:This example illustrates what the advantages of opportunistic migration would be .
6-78:There were faster nodes available in the system .
6-79:If these nodes were added to the application (which could trigger removing the slower nodes) the iteration duration could be reduced even further .
6-80:Still, the adaptation reduced the total runtime by 30% (Figure 2) .
6-81:5.6 Scenario 5: crashing nodes In the last scenario, we also run the application on 36 nodes in 3 clusters .
6-82:After 500 seconds, 2 out of 3 clusters crash .
6-83:The iteration durations are shown in Figure 7 .
6-84:After the crash, the iteration duration raised from 100 to 200 seconds .
6-85:The weighted efficiency rose to around 30% which triggered adding new nodes in the adaptive version .
6-86:The number of nodes gradually went back to 35 which brought the iteration duration back to around 100 seconds .
6-87:The total runtime was reduced by 13% (Figure 2). .
7 Related work :
7-1:A number of Grid projects address the question of resource selection and adaptation .
7-2:In GrADS (18) and ASSIST (1), resource selection and adaptation requires a performance model that allows predicting application runtimes .
7-3:In the resource selection phase, a number of possible resource sets is examined and the set of resources with the shortest predicted runtime is selected .
7-4:If performance degradation is detected during the computation, the resource selection phase is repeated .
7-5:GrADS uses the ratio of the predicted execution times (of certain application phases) to the real execution times as an indicator of application performance .
7-6:ASSIST uses the number of iterations per time unit (for iterative applications) or the number of tasks per time unit (for regular master worker applications) as a performance indicator .
7-7:The main difference between these approaches and our approach is the use of performance models .
7-8:The main advantage is that once the performance model is known, the system is able to take more accurate migration decisions than with our approach .
7-9:However, even if the performance 0 5 10 15 iteration number 0 200 400 600 800 1000 iterationduration(secs.) no adaptation with adaptation 2 out of 3 clusters crash started adding nodes 36 nodes reached Figure 7 .
7-10:Barnes Hut iteration durations with without adaptation, crashing CPUs model is known, the problem of finding an optimal resource set (i.e .
7-11:the resource set with the minimal execution time) is NP complete .
7-12:Currently, both GrADS and ASSIST examine only a subset of all possible resource sets and therefore there is no guarantee that the resulting resource set will be optimal .
7-13:As the number of available grid resources increases, the accuracy of this approach diminishes, as the subset of possible resource sets that can be examined in a reasonable time becomes smaller .
7-14:Another disadvantage of these systems is that the performance degradation detection is suitable only for iterative or regular applications .
7-15:Cactus (2) and GridWay (14) do not use performance models .
7-16:However, these frameworks are only suitable for sequential (GridWay) or single site applications (Cactus) .
7-17:In that case, the resource selection problem boils down to selecting the fastest machine or cluster .
7-18:Processor clock speed, average load and a number of processors in a cluster (Cactus) are used to rank resources and the resource with the highest rank is selected .
7-19:The application is migrated if performance degradation is detected or better resources are discovered .
7-20:Both Cactus and GridWay use the number of iterations per time unit as the performance indicator .
7-21:The main limitation of this methodology is that it is suitable only for sequential or single site applications .
7-22:Moreover, resource selection based on clock speed is not always accurate .
7-23:Finally, performance degradation detection is suitable only for iterative applications and cannot be used for irregular computations such as search and optimization problems .
7-24:The resource selection problem was also studied by the AppLeS project (5) .
7-25:In the context of this project, a number of applications were studied and performance models for these applications were created .
7-26:Based on such a model a scheduling agent is built that uses the performance model to select the best resource set and the best application schedule on this set .
7-27:AppLeS scheduling agents are written on a case by case basis and cannot be reused for another application .
7-28:Two reusable templates were also developed for specific classes of applications, namely master worker (AMWAT template) and parameter sweep (APST template) applications .
7-29:Migration is not supported by the AppLeS software .
7-30:127 In (13), the problem of scheduling master worker applications is studied .
7-31:The authors assume homogeneous processors (i.e., with the same speed) and do not take communication costs into account .
7-32:Therefore, the problem is reduced to finding the right number of workers .
7-33:The approach here is similar to ours in that no performance model is used .
7-34:Instead, the system tries to deduce the application requirements at runtime and adjusts the number of workers to approach the ideal number. .
8-1:In this paper, we investigated the problem of resource selection and adaptation in grid environments
8-2:Existing approaches to these problems typically assume the existence of a performance model that allows predicting application runtimes on various sets of resources
8-3:However, creating performance models is inherently difficult and requires knowledge about the application
8-4:We propose an approach that does not require in depth knowledge about the application
8-5:We start the application on an arbitrary set of resources and monitor its performance
8-6:The performance monitoring allows us to learn certain application requirements such as the number of processors needed by the application or the application"s bandwidth requirements
8-7:We use this knowledge to gradually refine the resource set by removing inadequate nodes or adding new nodes if necessary
8-8:This approach does not result in the optimal resource set, but in a reasonable resource set, i.e
8-9:a set free from various performance bottlenecks such as slow network connections or overloaded processors
8-10:Our approach also allows the application to adapt to the changing grid conditions
8-11:The adaptation decisions are based on the weighted average efficiency  an extension of the concept of parallel efficiency defined for traditional, homogeneous parallel machines
8-12:If the weighted average efficiency drops below a certain level, the adaptation coordinator starts removing worst nodes
8-13:The badness of the nodes is defined by a heuristic formula
8-14:If the weighted average efficiency raises above a certain level, new nodes are added
8-15:Our simple adaptation strategy allows us to handle multiple scenarios typical for grid environments: expand to more nodes or shrink to fewer nodes if the application was started on an inappropriate number of processors, remove inadequate nodes and replace them with better ones, replace crashed processors, etc
8-16:The application adapts fully automatically to changing conditions
8-17:We implemented our approach in the Satin divide and conquer framework and evaluated it on the DAS 2 distributed supercomputer and demonstrate that our approach can yield significant performance improvements (up to 60% in our experiments)
8-18:Future work will involve extending our adaptation strategy to support opportunistic migration
8-19:This, however, requires grid schedulers with more sophisticated functionality than currently exists
8-20:Further research is also needed to decrease the benchmarking overhead
8-21:For example, the information about CPU load could be used to decrease the benchmarking frequency
8-22:Another line of research that we wish to investigate is using feedback control to refine the adaptation strategy during the application run
8-23:For example, the node badness formula could be refined at runtime based on the effectiveness of the previous adaptation decisions
8-24:Finally, the centralized implementation of the adaptation coordinator might become a bottleneck for applications which are running on very large numbers of nodes (hundreds or thousands)
8-25:This problem can be solved by implementing a hierarchy of coordinators: one subcoordinator per cluster which collects and processes statistics from its cluster and one main coordinator which collects the information from the sub coordinators
8-26:Acknowledgments This work was carried out in the context of Virtual Laboratory for e Science project (ww.vl e.nl)
8-27:This project is supported by a BSIK grant from the Dutch Ministry of Education, Culture and Science (OC&W) and is part of the ICT innovation program of the Ministry of Economic Affairs (EZ).
9-1:M
9-2:Aldinucci, F
9-3:Andre, J
9-4:Buisson, S
9-5:Campa, M
9-6:Coppola, M
9-7:Danelutto, and C
9-8:Zoccolo
9-9:Parallel program component adaptivity management
9-10:In ParCo 2005, Sept
9-11:2005
9-12:G
9-13:Allen, D
9-14:Angulo, I
9-15:Foster, G
9-16:Lanfermann, C
9-17:Liu, T
9-18:Radke, E
9-19:Seidel, and J
9-20:Shalf
9-21:The cactus worm: Experiments with resource discovery and allocation in a grid environment
9-22:Int"l Journal of High Performance Computing Applications, 15(4):345 358, 2001
9-23:G
9-24:Allen, K
9-25:Davis, K
9-26:N
9-27:Dolkas, N
9-28:D
9-29:Doulamis, T
9-30:Goodale, T
9-31:Kielmann, A
9-32:Merzky, J
9-33:Nabrzyski, J
9-34:Pukacki, T
9-35:Radke, M
9-36:Russell, E
9-37:Seidel, J
9-38:Shalf, and I
9-39:Taylor
9-40:Enabling applications on the grid  a gridlab overview
9-41:Int"l Journal of High Performance Computing Applications, 17(4):449 466, Aug
9-42:2003
9-43:J
9-44:E
9-45:Baldeschwieler, R
9-46:D
9-47:Blumofe, and E
9-48:A
9-49:Brewer
9-50:ATLAS: An Infrastructure for Global Computing
9-51:In 7th ACM SIGOPS European Workshop on System Support for Worldwide Applications, pages 165 172, Sept
9-52:1996
9-53:F
9-54:Berman, R
9-55:Wolski, H
9-56:Casanova, W
9-57:Cirne, H
9-58:Dail, M
9-59:Faerman, S
9-60:Figueira, J
9-61:Hayes, G
9-62:Obertelli, J
9-63:Schopf, G
9-64:Shao, S
9-65:Smallen, N
9-66:Spring, A
9-67:Su, and D
9-68:Zagorodnov
9-69:Adaptive Computing on the Grid Using AppLeS
9-70:IEEE Trans
9-71:on Parallel and Distributed Systems, 14(4):369 382, Apr
9-72:2003
9-73:D. M
9-74:Chiu, M
9-75:Kadansky, J
9-76:Provino, and J
9-77:Wesley
9-78:Experiences in programming a traffic shaper
9-79:In 5th IEEE Symp
9-80:on Computers and Communications, pages 470 476, 2000
9-81:W
9-82:Chrabakh and R
9-83:Wolski
9-84:GridSAT: A Chaff based Distributed SAT Solver for the Grid
9-85:In 2003 ACM IEEE conference on Supercomputing, page 37, 2003
9-86:The Distributed ASCI Supercomputer (DAS)
9-87:http:  www.cs.vu.nl das2
9-88:N
9-89:Drost, R
9-90:V
9-91:van Nieuwport, and H
9-92:E
9-93:Bal
9-94:Simple localityaware co allocation in peer to peer supercomputing
9-95:In 6th Int"l Workshop on Global Peer 2 Peer Computing, May 2005
9-96:D
9-97:L
9-98:Eager, J
9-99:Zahorjan, and E
9-100:D
9-101:Lazowska
9-102:Speedup versus efficiency in parallel systems
9-103:IEEE Transactions on Computers, 38(3):408 423, Mar
9-104:1989
9-105:I
9-106:Foster
9-107:Globus toolkit version 4: Software for serviceoriented systems
9-108:In IFIP International Conference on Network and Parallel Computing, pages 2 13
9-109:Springer Verlag LNCS 3779, 2005
9-110:J. P
9-111:Goux, S
9-112:Kulkarni, M
9-113:Yoder, and J
9-114:Linderoth
9-115:An Enabling Framework for Master Worker Applications on the Computational Grid
9-116:In 9th IEEE Int"l Symp
9-117:on High Performance Distributed Computing, pages 43 50, Aug
9-118:2000
9-119:E
9-120:Heymann, M
9-121:A
9-122:Senar, E
9-123:Luque, and M
9-124:Livny
9-125:Adaptive scheduling for master worker applications on the computational grid
9-126:In 1st IEEE ACM International Workshop on Grid Computing, pages 214 227
9-127:Springer Verlag LNCS 1971, 2000
9-128:128 E
9-129:Huedo, R
9-130:S
9-131:Montero, and I
9-132:M
9-133:Llorente
9-134:A framework for adaptive execution in grids
9-135:Software  Practice & Experience, 34(7):631 651, 2004
9-136:H
9-137:H
9-138:Mohamed and D
9-139:H
9-140:Epema
9-141:Experiences with the KOALA Co Allocating Scheduler in Multiclusters
9-142:In 5th IEEE ACM Int"l Symp
9-143:on Cluster Computing and the GRID, pages 640 650, May 2005
9-144:A
9-145:Plaat, H
9-146:E
9-147:Bal, and R
9-148:F
9-149:H
9-150:Hofman
9-151:Sensitivity of parallel applications to large differences in bandwidth and latency in two layer interconnects
9-152:In 5th Int"l Symp
9-153:On High Performance Computer Architecture, pages 244 253, Jan
9-154:1999
9-155:J
9-156:W
9-157:Romein, H
9-158:E
9-159:Bal, J
9-160:Schaeffer, and A
9-161:Plaat
9-162:A performance analysis of transposition table driven work scheduling in distributed search
9-163:IEEE Trans
9-164:on Parallel and Distributed Systems, 13(5):447 459, May 2002
9-165:S
9-166:S
9-167:Vadhiyar and J
9-168:J
9-169:Dongarra
9-170:Self adaptivity in Grid computing
9-171:Concurrency and Computation: Practice and Experience, 17(2 4):235 257, 2005
9-172:R
9-173:V
9-174:van Nieuwpoort, T
9-175:Kielmann, and H
9-176:E
9-177:Bal
9-178:Efficient load balancing for wide area divide and conquer applications
9-179:In 8th ACM SIGPLAN Symp
9-180:on Principles and Practices of Parallel Programming, pages 34 43, 2001
9-181:R
9-182:V
9-183:van Nieuwpoort, J
9-184:Maassen, T
9-185:Kielmann, and H
9-186:E
9-187:Bal
9-188:Satin: Simple and Efficient Java based Grid Programming
9-189:Scalable Computing: Practice and Experience, 6(3):19 32, Sept
9-190:2004
9-191:R
9-192:V
9-193:van Nieuwpoort, J
9-194:Maassen, G
9-195:Wrzesinska, R
9-196:Hofman, C
9-197:Jacobs, T
9-198:Kielmann, and H
9-199:E
9-200:Bal
9-201:Ibis: a Flexible and Efficient Java based Grid Programming Environment
9-202:Concurrency & Computation: Practice & Experience, 17(78):1079 1107, 2005
9-203:R
9-204:Wolski, N
9-205:Spring, and J
9-206:Hayes
9-207:The network weather service: A distributed resource performance forecasting service for metacomputing
9-208:Journal of Future Generation Computing Systems, 15(5 6):757 768, Oct
9-209:1999
9-210:G
9-211:Wrzesinska, R
9-212:V
9-213:van Nieuwport, J
9-214:Maassen, and H
9-215:E
9-216:Bal
9-217:Fault tolerance, Malleability and Migration for Divideand Conquer Applications on the Grid
9-218:In Int"l Parallel and Distributed Processing Symposium, Apr
9-219:2005
9-220:129
picture:
