Regularized Clustering for Documents ∗ 
content:
1 ABSTRACT :
1-1:In recent years, document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering .
1-2:In this paper, we propose a novel method for clustering documents using regularization .
1-3:Unlike traditional globally regularized clustering methods, our method first construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer .
1-4:So we call our algorithm Clustering with Local and Global Regularization .
1-5:We will show that the cluster memberships of the documents can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be efficiently solved by iterative methods .
1-6:Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods .
1-7:H.3.3 [Information Storage and Retrieval]: Information .
2 INTRODUCTION :
2-1:Document clustering has been receiving more and more attentions as an important and fundamental technique for unsupervised document organization, automatic topic extraction, and fast information retrieval or filtering .
2-2:A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies .
2-3:As pointed out by [8], the information retrieval needs can be expressed by a spectrum ranged from narrow keyword matching based search to broad information browsing such as what are the major international events in recent months .
2-4:Traditional document retrieval engines tend to fit well with the search end of the spectrum, i.e .
2-5:they usually provide specified search for documents matching the user"s query, however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed .
2-6:In such cases, efficient browsing through a good cluster hierarchy will be definitely helpful .
2-7:Generally, document clustering methods can be mainly categorized into two classes: hierarchical methods and partitioning methods .
2-8:The hierarchical methods group the data points into a hierarchical tree structure using bottom up or top down approaches .
2-9:For example, hierarchical agglomerative clustering [13] is a typical bottom up hierarchical clustering method .
2-10:It takes each data point as a single cluster to start off with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one final cluster .
2-11:On the other hand, partitioning methods decompose the dataset into a number of disjoint clusters which are usually optimal in terms of some predefined criterion functions .
2-12:For instance, K means [13] is a typical partitioning method which aims to minimize the sum of the squared distance between the data points and their corresponding cluster centers .
2-13:In this paper, we will focus on the partitioning methods .
2-14:As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model [16]): (1) the predefined criterion is usually non convex which causes many local optimal solutions; (2) the iterative procedure (e.g .
2-15:the Expectation Maximization algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations .
2-16:In the last decades, many methods have been proposed to overcome the above problems of the partitioning methods [19][28] .
2-17:Recently, another type of partitioning methods based on clustering on data graphs have aroused considerable interests in the machine learning and data mining community .
2-18:The basic idea behind these methods is to first model the whole dataset as a weighted graph, in which the graph nodes represent the data points, and the weights on the edges correspond to the similarities between pairwise points .
2-19:Then the cluster assignments of the dataset can be achieved by optimizing some criterions defined on the graph .
2-20:For example Spectral Clustering is one kind of the most representative graph based clustering approaches, it generally aims to optimize some cut value (e.g .
2-21:Normalized Cut [22], Ratio Cut [7], Min Max Cut [11]) defined on an undirected graph .
2-22:After some relaxations, these criterions can usually be optimized via eigen decompositions, which is guaranteed to be global optimal .
2-23:In this way, spectral clustering efficiently avoids the problems of the traditional partitioning methods as we introduced in last paragraph .
2-24:In this paper, we propose a novel document clustering algorithm that inherits the superiority of spectral clustering, i.e .
2-25:the final cluster results can also be obtained by exploit the eigen structure of a symmetric matrix .
2-26:However, unlike spectral clustering, which just enforces a smoothness constraint on the data labels over the whole data manifold [2], our method first construct a regularized linear label predictor for each data point from its neighborhood as in [25], and then combine the results of all these local label predictors with a global label smoothness regularizer .
2-27:So we call our method Clustering with Local and Global Regularization .
2-28:The idea of incorporating both local and global information into label prediction is inspired by the recent works on semi supervised learning [31], and our experimental evaluations on several real document datasets show that CLGR performs better than many state of the art clustering methods .
2-29:The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail .
2-30:The experimental results on several datasets are presented in section 3, followed by the conclusions and discussions in section 4. .
3 THE PROPOSED ALGORITHM :
3-1:In this section, we will introduce our Clustering with Local and Global Regularization algorithm in detail .
3-2:First let"s see the how the documents are represented throughout this paper .
3-3:2.1 Document Representation In our work, all the documents are represented by the weighted term frequency vectors .
3-4:Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations) .
3-5:The term frequency vector xi of document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk .
3-6:In this way, xi is also called the TFIDF representation of document di .
3-7:Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF IDF vector .
3-8:2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization .
3-9:In this subsection we will introduce the local regularization part in detail .
3-10:2.2.1 Motivation As we know that clustering is one type of learning techniques, it aims to organize the dataset in a reasonable way .
3-11:Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24] .
3-12:For example, in the two class classification scenario1 (in which we exactly know the label of each document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi .
3-13:By taking ∂J ∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector .
3-14:Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function .
3-15:A natural problem in Eq.(3) is that the matrix XXT may be singular and thus not invertable (e.g .
3-16:when m n) .
3-17:To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter .
3-18:Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix .
3-19:It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29] .
3-20:However, despite its empirical success, the regularized linear classifier is on earth a global classifier, i.e .
3-21:w∗ is estimated using the whole training set .
3-22:According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space .
3-23:In order to get better predictions, [6] proposed to train classifiers locally and use them to classify the testing points .
3-24:For example, a testing point will be classified by the local classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the documents coming from only two classes .
3-25:The generalizations of our method to multi class cases will be discussed in section 2.5 .
3-26:of it .
3-27:Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6] .
3-28:2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering .
3-29:The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k nearest neighborhood Ni, and then use it to predict the label of xi .
3-30:Finally we will combine all those local predictors by minimizing the sum of their prediction errors .
3-31:In this subsection we will introduce how to construct those local predictors .
3-32:Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the cluster membership of xj .
3-33:Then using Eq.(6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k th nearest neighbor of xi .
3-34:qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik .
3-35:The problem here is that XiXT i is an m × m matrix with m ni, i.e .
3-36:we should compute the inverse of an m × m matrix for every document vector, which is computationally prohibited .
3-37:Fortunately, we have the following theorem: Theorem 1 .
3-38:w∗ i in Eq.(8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix .
3-39:Proof .
3-40:Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .
3-41:Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi .
3-42:Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor .
3-43:Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi .
3-44:This is an attractive expression since we can determine the cluster assignment of u by using the inner products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function .
3-45:2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors .
3-46:Combining Eq.(10) with Eq.(6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way .
3-47:Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j) th entry of P, and αi j represents the j th entry of αi .
3-48:Till now we can write the criterion of clustering by combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques .
3-49:However, the results may not be good enough since we only exploit the local informations of the dataset .
3-50:In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local global way .
3-51:2.3 Global Regularization In data clustering, we usually require that the cluster assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster assignments; (2) the points on the same structure (e.g .
3-52:submanifold or cluster) tend to have the same cluster assignments [31] .
3-53:Without the loss of generality, we assume that the data points reside (roughly) on a low dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e .
3-54:2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x .
3-55:The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard measure on M .
3-56:If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field .
3-57:Generally, the graph can be viewed as the discretized form of manifold .
3-58:We can model the dataset as an weighted undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points .
3-59:Then it can be shown that minimizing Eq.(13) corresponds to minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j) th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj .
3-60:If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter .
3-61:It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18] .
3-62:In summary, using Eq.(15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold .
3-63:Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments .
3-64:2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t .
3-65:qi ∈ {−1, +1}, (18) where P is defined as in Eq.(12), and λ is a regularization parameter to trade off Jl and Jg .
3-66:However, the discrete fill in the whole high dimensional sample space .
3-67:And it has been shown that the manifold based methods can achieve good results on text classification tasks [31] .
3-68:3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi) .
3-69:constraint of pi makes the problem an NP hard integer programming problem .
3-70:A natural way for making the problem solvable is to remove the constraint and relax qi to be continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q .
3-71:Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t .
3-72:qT q = 1 (20) Using the Lagrangian method, we can derive that the optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster assignment of xi can be determined by the sign of qi, i.e .
3-73:xi will be classified as class one if qi > 0, otherwise it will be classified as class 2 .
3-74:2.5 Multi Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization for the two class clustering problem, and we will extending it to multi class clustering in this subsection .
3-75:First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C} .
3-76:qc is the classification function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c .
3-77:Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later .
3-78:Therefore, in this multi class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k th neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik) .
3-79:Then (wc∗ i )T xi returns the predicted confidence of xi belonging to class c .
3-80:Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 .
3-81:(23) As in Eq.(11), we can define an n×n matrix P (see Eq.(12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 .
3-82:(24) Similarly we can define the global smoothness regularizer in multi class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc .
3-83:(25) Then the criterion to be minimized for CLGR in multi class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix .
3-84:The same as in Eq.(20), we also add the constraint that QT Q = I to restrict the scale of Q .
3-85:Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t .
3-86:QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix .
3-87:Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points .
3-88:There are mainly two approaches to achieve this goal: embedding of xi in a C dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters. .
4 Since the optimal Q∗ :
4-1:is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .
4-2:The detailed algorithm can be referred to [26] .
4-3:The detailed algorithm procedure for CLGR is summarized in table 1. .
5 EXPERIMENTS :
5-1:In this section, experiments are conducted to empirically compare the clustering results of CLGR with other 8 representitive document clustering algorithms on 5 datasets .
5-2:First we will introduce the basic informations of those datasets .
5-3:3.1 Datasets We use a variety of datasets, most of which are frequently used in the information retrieval research .
5-4:Table 2 summarizes the characteristics of the datasets .
5-5:4 Here an indication matrix T is a n×c matrix with its (i, j)th entry Tij ∈ {0, 1} such that for each row of Q∗ there is only one 1 .
5-6:Then the xi can be assigned to the j th cluster such that j = argjQ∗ ij = 1 .
5-7:Table 1: Clustering with Local and Global Regularization Input: .
6 Dataset X = {xi}n :
6-1:i=1; .
7 Number of clusters C; :
7-1:.
8 Size of the neighborhood K; :
8-1:i=1; .
9 Global regularization parameter λ; :
9-1:Output: The cluster membership of each data point .
9-2:Procedure: data point; .
10 Construct the matrix M = (P − I)T :
10-1:(P − I) + λL; the matrix Q∗ according to Eq.(28); by properly discretize Q∗ .
10-2:Table 2: Descriptions of the document datasets Datasets Number of documents Number of classes CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 WebACE 2340 20 Newsgroup4 3970 4 CSTR .
10-3:This is the dataset of the abstracts of technical reports published in the Department of Computer Science at a university .
10-4:The dataset contained 476 abstracts, which were divided into four research areas: Natural Language Processing(NLP), Robotics Vision, Systems, and Theory .
10-5:WebKB .
10-6:The WebKB dataset contains webpages gathered from university computer science departments .
10-7:There are about 8280 documents and they are divided into 7 categories: student, faculty, staff, course, project, department and other .
10-8:The raw text is about 27MB .
10-9:Among these 7 categories, student, faculty, course and project are four most populous entity representing categories .
10-10:The associated subset is typically called WebKB4 .
10-11:Reuters .
10-12:The Reuters 21578 Text Categorization Test collection contains documents collected from the Reuters newswire in 1987 .
10-13:It is a standard text categorization benchmark and contains 135 categories .
10-14:In our experiments, we use a subset of the data collection which includes the 10 most frequent categories among the 135 topics and we call it Reuters top 10 .
10-15:WebACE .
10-16:The WebACE dataset was from WebACE project and has been used for document clustering [17][5] .
10-17:The WebACE dataset contains 2340 documents consisting news articles from Reuters new service via the Web in October 1997 .
10-18:These documents are divided into 20 classes .
10-19:News4 .
10-20:The News4 dataset used in our experiments are selected from the famous 20 newsgroups dataset5 .
10-21:The topic rec containing autos, motorcycles, baseball and hockey was selected from the version 20news 18828 .
10-22:The News4 dataset contains 3970 document vectors .
10-23:5 http: people.csail.mit.edu jrennie 20Newsgroups To pre process the datasets, we remove the stop words using a standard stop list, all HTML tags are skipped and all header fields except subject and organization of the posted articles are ignored .
10-24:In all our experiments, we first select the top 1000 words by mutual information with class labels .
10-25:3.2 Evaluation Metrics In the experiments, we set the number of clusters equal to the true number of classes C for all the clustering algorithms .
10-26:To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures .
10-27:Clustering Accuracy (Acc) .
10-28:The first performance measure is the Clustering Accuracy, which discovers the one toone relationship between clusters and classes and measures the extent to which each cluster contained data points from the corresponding class .
10-29:It sums up the whole matching degree between all pair class clusters .
10-30:Clustering accuracy can be computed as: Acc = 1 N max   Ck,Lm T(Ck, Lm)   , (29) where Ck denotes the k th cluster in the final results, and Lm is the true m th class .
10-31:T(Ck, Lm) is the number of entities which belong to class m are assigned to cluster k .
10-32:Accuracy computes the maximum sum of T(Ck, Lm) for all pairs of clusters and classes, and these pairs have no overlaps .
10-33:The greater clustering accuracy means the better clustering performance .
10-34:Normalized Mutual Information .
10-35:Another evaluation metric we adopt here is the Normalized Mutual Information NMI [23], which is widely used for determining the quality of clusters .
10-36:For two random variable X and Y, the NMI is defined as: NMI(X, Y) = I(X, Y) H(X)H(Y) , (30) where I(X, Y) is the mutual information between X and Y, while H(X) and H(Y) are the entropies of X and Y respectively .
10-37:One can see that NMI(X, X) = 1, which is the maximal possible value of NMI .
10-38:Given a clustering result, the NMI in Eq.(30) is estimated as NMI = C k=1 C m=1 nk,mlog n·nk,m nk ˆnm C k=1 nklog nk n C m=1 ˆnmlog ˆnm n , (31) where nk denotes the number of data contained in the cluster Ck (1 k C), ˆnm is the number of data belonging to the m th class (1 m C), and nk,m denotes the number of data that are in the intersection between the cluster Ck and the m th class .
10-39:The value calculated in Eq.(31) is used as a performance measure for the given clustering result .
10-40:The larger this value, the better the clustering performance .
10-41:3.3 Comparisons We have conducted comprehensive performance evaluations by testing our method and comparing it with 8 other representative data clustering methods using the same data corpora .
10-42:The algorithms that we evaluated are listed below. .
11 Traditional k means (KM). :
11-1:on [9] .
11-2:is based on [16] .
11-3:implementation is based on [26], and the variance of the Gaussian similarity is determined by Local Scaling [30] .
11-4:Note that the criterion that Ncut aims to minimize is just the global regularizer in our CLGR algorithm except that Ncut used the normalized Laplacian .
11-5:In this method we just minimize Jl (defined in Eq.(24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods .
11-6:implementation is based on [14] .
11-7:implementation is based on [27] .
11-8:[12] .
11-9:The implementation is based on [15] .
11-10:For computational efficiency, in the implementation of CPLR and our CLGR algorithm, we have set all the local regularization parameters {λi}n i=1 to be identical, which is set by grid search from {0.1, 1, 10} .
11-11:The size of the k nearest neighborhoods is set by grid search from {20, 40, 80} .
11-12:For the CLGR method, its global regularization parameter is set by grid search from {0.1, 1, 10} .
11-13:When constructing the global regularizer, we have adopted the local scaling method [30] to construct the Laplacian matrix .
11-14:The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20] .
11-15:3.4 Experimental Results The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4 .
11-16:From the two tables we mainly observe that: clustering methods in most of the datasets; usually outperforms the traditional k means clustering method, and the GMM method can achieve competitive results compared to the Spherical k means method; algorithms are usually worse than the results achieved from Spectral Clustering .
11-17:Since Spectral Clustering can be viewed as a weighted version of kernel k means, it can obtain good results the data clusters are arbitrarily shaped .
11-18:This corroborates that the documents vectors are not regularly distributed (spherical or elliptical) .
11-19:equivalence between NMF and Spectral Clustering, which Table 3: Clustering accuracies of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various methods CSTR WebKB4 Reuters WebACE News4 KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 has been proved theoretically in [10] .
11-20:It can be observed from the tables that NMF and Spectral Clustering usually lead to similar clustering results .
11-21:can usually achieve better results than traditional purely document vector based methods .
11-22:Since these methods perform an implicit feature selection at each iteration, provide an adaptive metric for measuring the neighborhood, and thus tend to yield better clustering results .
11-23:than the results achieved from Spectral Clustering, which supports Vapnik"s theory [24] that sometimes local learning algorithms can obtain better results than global learning algorithms .
11-24:Besides the above comparison experiments, we also test the parameter sensibility of our method .
11-25:There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λi"s to be identical to λ∗ in our experiments), and the size of the neighborhoods .
11-26:Therefore we have also done two sets of experiments: clustering performance with varying λ∗ and λ .
11-27:In this set of experiments, we find that our CLGR algorithm can achieve good results when the two regularization parameters are neither too large nor too small .
11-28:Typically our method can achieve good results when λ∗ and λ are around 0.1 .
11-29:Figure 1 shows us such a testing example on the WebACE dataset .
11-30:and testing the clustering performance with different −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regularization para (log 2 value) global regularization para (log 2 value) clusteringaccuracy Figure 1: Parameter sensibility testing results on the WebACE dataset with the neighborhood size fixed to 20, and the x axis and y axis represents the log2 value of λ∗ and λ .
11-31:sizes of neighborhoods .
11-32:In this set of experiments, we find that the neighborhood with a too large or too small size will all deteriorate the final clustering results .
11-33:This can be easily understood since when the neighborhood size is very small, then the data points used for training the local classifiers may not be sufficient; when the neighborhood size is very large, the trained classifiers will tend to be global and cannot capture the typical local characteristics .
11-34:Figure 2 shows us a testing example on the WebACE dataset .
11-35:Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitive to the choice of parameters, which makes it practical in real world applications. .
12 ABSTRACT :
12-1:In this paper, we derived a new clustering algorithm called clustering with local and global regularization
12-2:Our method preserves the merit of local learning algorithms and spectral clustering
12-3:Our experiments show that the proposed algorithm outperforms most of the state of the art algorithms on many benchmark datasets
12-4:In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm.
13 ABSTRACT :
13-1:L
13-2:Baker and A
13-3:McCallum
13-4:Distributional Clustering of Words for Text Classification
13-5:In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1998
13-6:M
13-7:Belkin and P
13-8:Niyogi
13-9:Laplacian Eigenmaps for Dimensionality Reduction and Data Representation
13-10:Neural Computation, 15 (6):1373 1396
13-11:June 2003
13-12:M
13-13:Belkin and P
13-14:Niyogi
13-15:Towards a Theoretical Foundation for Laplacian Based Manifold Methods
13-16:In Proceedings of the 18th Conference on Learning Theory (COLT)
13-17:2005
13-18:10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuracy Figure 2: Parameter sensibility testing results on the WebACE dataset with the regularization parameters being fixed to 0.1, and the neighborhood size varing from 10 to 100
13-19:M
13-20:Belkin, P
13-21:Niyogi and V
13-22:Sindhwani
13-23:Manifold Regularization: a Geometric Framework for Learning from Examples
13-24:Journal of Machine Learning Research 7, 1 48, 2006
13-25:D
13-26:Boley
13-27:Principal Direction Divisive Partitioning
13-28:Data mining and knowledge discovery, 2:325 344, 1998
13-29:L
13-30:Bottou and V
13-31:Vapnik
13-32:Local learning algorithms
13-33:Neural Computation, 4:888 900, 1992
13-34:P
13-35:K
13-36:Chan, D
13-37:F
13-38:Schlag and J
13-39:Y
13-40:Zien
13-41:Spectral K way Ratio Cut Partitioning and Clustering
13-42:IEEE Trans
13-43:Computer Aided Design, 13:1088 1096, Sep
13-44:1994
13-45:D
13-46:R
13-47:Cutting, D
13-48:R
13-49:Karger, J
13-50:O
13-51:Pederson and J
13-52:W
13-53:Tukey
13-54:Scatter Gather: A Cluster Based Approach to Browsing Large Document Collections
13-55:In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 1992
13-56:I
13-57:S
13-58:Dhillon and D
13-59:S
13-60:Modha
13-61:Concept Decompositions for Large Sparse Text Data using Clustering
13-62:Machine Learning, vol
13-63:42(1), pages 143 175, January 2001
13-64:C
13-65:Ding, X
13-66:He, and H
13-67:Simon
13-68:On the equivalence of nonnegative matrix factorization and spectral clustering
13-69:In Proceedings of the SIAM Data Mining Conference, 2005
13-70:C
13-71:Ding, X
13-72:He, H
13-73:Zha, M
13-74:Gu, and H
13-75:D
13-76:Simon
13-77:A min max cut algorithm for graph partitioning and data clustering
13-78:In Proc
13-79:of the 1st International Conference on Data Mining (ICDM), pages 107 114, 2001
13-80:C
13-81:Ding, T
13-82:Li, W
13-83:Peng, and H
13-84:Park
13-85:Orthogonal Nonnegative Matrix Tri Factorizations for Clustering
13-86:In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006
13-87:R
13-88:O
13-89:Duda, P
13-90:E
13-91:Hart, and D
13-92:G
13-93:Stork
13-94:Pattern Classification
13-95:John Wiley & Sons, Inc., 2001
13-96:T
13-97:Li, S
13-98:Ma, and M
13-99:Ogihara
13-100:Document Clustering via Adaptive Subspace Iteration
13-101:In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004
13-102:T
13-103:Li and C
13-104:Ding
13-105:The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering
13-106:In Proceedings of the 6th International Conference on Data Mining (ICDM)
13-107:2006
13-108:X
13-109:Liu and Y
13-110:Gong
13-111:Document Clustering with Cluster Refinement and Model Selection Capabilities
13-112:In Proc
13-113:of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002
13-114:E
13-115:Han, D
13-116:Boley, M
13-117:Gini, R
13-118:Gross, K
13-119:Hastings, G
13-120:Karypis, V
13-121:Kumar, B
13-122:Mobasher, and J
13-123:Moore
13-124:WebACE: A Web Agent for Document Categorization and Exploration
13-125:In Proceedings of the 2nd International Conference on Autonomous Agents (Agents98)
13-126:ACM Press, 1998
13-127:M
13-128:Hein, J
13-129:Y
13-130:Audibert, and U
13-131:von Luxburg
13-132:From Graphs to Manifolds  Weak and Strong Pointwise Consistency of Graph Laplacians
13-133:In Proceedings of the 18th Conference on Learning Theory (COLT), 470 485
13-134:2005
13-135:J
13-136:He, M
13-137:Lan, C. L
13-138:Tan, S. Y
13-139:Sung, and H. B
13-140:Low
13-141:Initialization of Cluster Refinement Algorithms: A Review and Comparative Study
13-142:In Proc
13-143:of Inter
13-144:Joint Conference on Neural Networks, 2004
13-145:A
13-146:Y
13-147:Ng, M
13-148:I
13-149:Jordan, Y
13-150:Weiss
13-151:On Spectral Clustering: Analysis and an algorithm
13-152:In Advances in Neural Information Processing Systems 14
13-153:2002
13-154:B
13-155:Sch¨olkopf and A
13-156:Smola
13-157:Learning with Kernels
13-158:The MIT Press
13-159:Cambridge, Massachusetts
13-160:2002
13-161:J
13-162:Shi and J
13-163:Malik
13-164:Normalized Cuts and Image Segmentation
13-165:IEEE Trans
13-166:on Pattern Analysis and Machine Intelligence, 22(8):888 905, 2000
13-167:A
13-168:Strehl and J
13-169:Ghosh
13-170:Cluster Ensembles  A Knowledge Reuse Framework for Combining Multiple Partitions
13-171:Journal of Machine Learning Research, 3:583 617, 2002
13-172:V
13-173:N
13-174:Vapnik
13-175:The Nature of Statistical Learning Theory
13-176:Berlin: Springer Verlag, 1995
13-177:Wu, M
13-178:and Sch¨olkopf, B
13-179:A Local Learning Approach for Clustering
13-180:In Advances in Neural Information Processing Systems 18
13-181:2006
13-182:S
13-183:X
13-184:Yu, J
13-185:Shi
13-186:Multiclass Spectral Clustering
13-187:In Proceedings of the International Conference on Computer Vision, 2003
13-188:W
13-189:Xu, X
13-190:Liu and Y
13-191:Gong
13-192:Document Clustering Based On Non Negative Matrix Factorization
13-193:In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2003
13-194:H
13-195:Zha, X
13-196:He, C
13-197:Ding, M
13-198:Gu and H
13-199:Simon
13-200:Spectral Relaxation for K means Clustering
13-201:In NIPS 14
13-202:2001
13-203:T
13-204:Zhang and F
13-205:J
13-206:Oles
13-207:Text Categorization Based on Regularized Linear Classification Methods
13-208:Journal of Information Retrieval, 4:5 31, 2001
13-209:L
13-210:Zelnik Manor and P
13-211:Perona
13-212:Self Tuning Spectral Clustering
13-213:In NIPS 17
13-214:2005
13-215:D
13-216:Zhou, O
13-217:Bousquet, T
13-218:N
13-219:Lal, J
13-220:Weston and B
13-221:Sch¨olkopf
13-222:Learning with Local and Global Consistency
13-223:NIPS 17, 2005
picture:
