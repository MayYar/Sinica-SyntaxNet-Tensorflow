has a MB that being members gives them high utility value
We assume that agents that have more accurate profiles of their adversaries will be more successful in such environments .
Such agents will be able to predict when a member is about to breach the alliance"s contract (item 2 in the above model), and take counter measures (when item 3 will falsify) .
The robustness of the alliance is in part a function of its members" trustfulness measure, objective position estimation, and other profile properties .
We should note that an agent can simultaneously be part of more than one alliance .
Such a temporary alliance, where the group members do not have a joint goal but act collaboratively for the interest of their own individual goals, is classified as a Treatment Group by modern psychologists [12] (in contrast to a Task Group, where its members have a joint goal) .
The Shared Activity model as presented in [5] modeled Treatment Group behavior using the same SharedPlans formalization .
When comparing both definitions of an alliance and a Treatment Group we found an unsurprising resemblance between both models
The main distinction between both models is the integration of a Helpful behavior act axiom, in the Shared Activity which cannot be part of ours .
This axiom states that an agent will consider taking action that will lower its Eval value (to a certain lower bound), if it believes that a group partner will gain a significant benefit .
Such behavior cannot occur in a pure adversarial environment (as a zero sum game is), where the alliance members are constantly on watch to manipulate their alliance to their own advantage .
A6 .
Evaluation Maximization Axiom .
In a case when all other axioms are inapplicable, we will proceed with the action that maximizes the heuristic value as computed in the Eval function .
(?Aag, Ao ? A, £\ ? Cag, Tn, w ? W) Bel(Aag, (?£^ ? Cag)Eval(Aag, £\, w) ? Eval(Aag, £^, w), Tn) ? Pot.Int.To(Aag, £\, Tn, T£\, w) T1 .
Optimality on Eval = Utility The above axiomatic model handles situations where the Utility is unknown and the agents are bounded rational agents .
The following theorem shows that in bilateral interactions, where the agents have the real Utility function (i.e., Eval = Utility) and are rational agents, the axioms provide the same optimal result as classic adversarial search (e.g., Min Max) .
Theorem 1 .
Let Ae ag be an unbounded rational AE agent using the Eval heuristic evaluation function, Au ag be the same agent using the true Utility function, and Ao be a sole unbounded utility based rational adversary .
Given that Eval = Utility
The proof will show that Ae ag, using the AE axioms, will select the same or equal utility £\ (when there is more than one action with the same max utility) when Eval = Utility .
(A1) Goal achieving axiom suppose there is an £\ such that its completion will achieve Au ag"s goal .
It will obtain the highest utility by Min Max for Au ag .
The Ae ag agent will select £\ or another action with the same utility value via A1 .
If such £\ does not exist, Ae ag cannot apply this axiom, and proceeds to A2 .
(A2) Preventive act axiom (1) Looking at the basic case (see Prop1), if there is a £] which leads Ao to achieve its goal, then a preventive action £\ will yield the highest utility for Au ag .
Au ag will choose it through the utility, while Ae ag will choose it through A2 .
(2) In the general case, £] is a highly beneficial action for Ao, thus yields low utility for Au ag, which will guide it to select an £\ that will prevent £], while Ae ag will choose it through A2.1 If such £] does not exist for Ao, then A2 is not applicable, and Ae ag can proceed to A3 .
(A3) Suboptimal tactical move axiom When using a heuristic Eval function, Ae ag has a partial belief in the profile of its adversary (item 4 in AE model), which may lead it to believe in SetActions (Prop1) .
In our case, Ae ag is holding a full profile on its optimal adversary and knows that Ao will behave optimally according to the real utility values on the complete search tree, therefore, any belief about suboptimal SetAction cannot exist, yielding this axiom inapplicable .
Ae ag will proceed to A4 .
(A4) Profile detection axiom Given that Ae ag has the full profile of Ao, none of Ae ag"s actions can increase its knowledge .
That axiom will not be applied, and the agent will proceed with A6 (A5 will be disregarded because the interaction is bilateral) .
(A6) Evaluation maximization axiom This axiom will select the max Eval for Ae ag .
Given that Eval = Utility, the same £\ that was selected by Au ag will be selected. .
