The main strength of our framework is that it allows us to exploit the three main elements of reciprocity
? Mutuality of expectations
? Recovery mechanisms
by means of redemption) .
In open systems in which we cannot enforce certain behaviours, these are effectively the only available means for indirect sanctions and rewards .
9 This is actually only a lower bound on the complexity for commitment processing which could become even worse if dominated by the complexity of verifying entailment |=; however, this would also hold for a static ACL semantics .
10 For example, this could be useful if we want to discard commitments whose status was last modified more than k time steps ago (this is problematic, as it might force us to discard certain unset pending commitments before they become pending active) .
The Sixth Intl .
Joint Conf .
on Autonomous Agents and Multi Agent Systems (AAMAS 07) 105 There are two further dimensions that affect DS based sanctioning and reward mechanisms and are orthogonal to the above
whether it is a reward or punishment), the other the degree of adaptation (reputation based mechanisms, for example, need not realistically reflect the behaviour of the culprit, but may instead utilise immediate (exaggerated) stigmatisation of agents as a deterrent) .
Albeit simple, our example DS described above makes use of all these aspects, and apart from consistency and completeness, it also satisfies some other useful properties
Our framework raises interesting questions regarding further potential properties of DS such as
While in some cases some agents should be able to enforce commitments upon others, this should generally be avoided to ensure agent autonomy .
either disallow commitment to contradictory actions or beliefs, or at least provide operators for rectifying such contradictory claims .
Under contradictory commitments, no possible behaviour can be compliantit is up to the designer to decide to which extent this should be permitted .
prediction must not deviate from compliant behaviour prediction if deviant behaviour has not been observed so far (in particular this must hold for the initial semantic state) .
This might not always be desirable as initial distrust is necessary in some systems, but it increases the chances that agents will agree to participate in communication .
dialogue operators will remain stable after a finite number of transitions, regardless of any further agent behaviour11 .
If this property holds, this would imply that agents can stop tracking semantic state transitions after some amount of initial interaction .
The advantage of this is reduced complexity, which of course comes at the price of giving up adaptiveness .
behaviour of an agent should lead to a semantic state that predicts compliant behaviour for that agent again .
Here, we have to trade off cautiousness against the provision of incentives to resume cooperative behaviour .
Trusting an agent makes others vulnerable to exploitation blacklisting an agent forever, though, might lead that agent to keep up its unpredictable and potentially malicious behaviour .
constraints, the same dynamics of semantics should apply to all parties involved .
Our simple example semantics satisfies all these properties apart from convergence .
Many of the above properties are debatable, as we have to trade off cautiousness against the provision of incentives for cooperative behaviour .
While we cannot make any general statements here regarding optimal DS ACL design, our framework provides the tools to test and evaluate the performance of different such communication inherent sanctioning and rewarding mechanisms (i.e .
social rules that do not presuppose ability to direct punishment or reward through physical actions) in real world applications. .
