In this section, we describe our experiment results .
We first describe our experiment setup and present an overview of various methods" performance .
Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms .
Next we analyze user term feedback behavior and its relation to retrieval performance .
Finally we compare term feedback to relevance feedback and show that it has its particular advantage .
6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms .
The tracks used the AQUAINT collection, a 3GB corpus of English newswire text .
The topics included 50 ones previously known to be hard, i.e .
with low retrieval performance .
It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough .
Participants of the track were able to submit custom designed clarification forms to solicit feedback from human assessors provided by Table 2
The last row is the percentage of MAP improvement over the baseline .
The parameter settings £g = 4, £f = 0.1, £\ = 0.3 are near optimal .
Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3
# terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST .
We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster .
They are
The total number of presented terms is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering .
For each topic, an assessor would complete the forms ordered by 6 ¡Ñ 8, 1 ¡Ñ 48 and 3 ¡Ñ 16, spending up to three minutes on each form .
The sample clarification form shown in Figure 1 is of type 3 ¡Ñ 16 .
It is a simple and compact interface in which the user can check relevant terms .
The form is self explanatory; there is no need for extra user training on how to use it .
Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length .
As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo feedback documents .
We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments) .
For all other parameters we use Lemur"s default settings .
The baseline turns out to perform above average among the track participants .
After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms .
After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval .
We evaluate the different retrieval methods" performance on their rankings of the top 1000 documents .
The evaluation metrics we adopt include mean average (non interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved .
Table 2 shows the performance of various methods and configurations of K ¡Ñ L .
The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters .
For example, TCFB3C means the TCFB method on the 3 ¡Ñ 16 clarification forms .
From Table 2 we can make the following observations
In other words, term feedback is truly helpful for improving retrieval accuracy .
3 ¡Ñ 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 ¡Ñ 8 ones .
counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial .
CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback .
by interpolation, it is able to outperform both .
This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster) .
Except for TCFB6C v.s .
CFB6C, the performance advantage of TCFB over TFB CFB is significant at p < 0.05 using the Wilcoxon signed rank test .
This is not true in the case of TFB v.s .
CFB, each of which is better than the other in nearly half of the topics .
6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts .
It is interesting to know whether our algorithms" performance deteriorates when the user is presented with fewer terms .
Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .
Therefore, we can easily simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M 
Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48 .
We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB .
For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively .
We conjecture the reason to be that while TFB"s performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work .
Also, the 3 ¡Ñ 16 clarification forms seem to be more robust than the 6 ¡Ñ 8 ones
Similarly, for CFB it is 95.0% against 98.0% .
This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful .
Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small .
For example, at only 12 terms CFB3C (the clarification form is of size 3 ¡Ñ 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms .
6.3 User Feedback Analysis In this part we study several aspects of user"s term feedback behavior, and whether they are connected to retrieval performance .
Figure 2
We see that the user is usually able to finish term feedback within a reasonably short amount of time
This suggests that term feedback is suitable for interactive ad hoc retrieval, where a user usually does not want to spend too much time on providing feedback .
We find that a user often makes mistakes when judging term relevance .
Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user .
Other times a dubious term may be included but turns out to be irrelevant .
Take the topic in Figure 1 for example .
There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment .
Table 4
terms 15.0 12.6 11.2 # rel .
checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his her ignorance of the event .
Indeed, without proper context it would be hard to make perfect judgment .
What is then, the extent to which the user is good at term feedback? Does it have serious impact on retrieval performance? To answer these questions, we need a measure of individual terms" true relevance .
We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance measure
If £mKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones .
We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold £m0 .
We can then define precision and recall of user term judgment accordingly
Table 4 shows the number of checked terms, relevant terms and relevant checked terms when £m0 is set to 1.0, as well as the precision recall of user term judgment .
Note that when the clarification forms contain more clusters, fewer terms are checked
Similar pattern holds for relevant terms and relevant checked terms .
There seems to be a trade off between increasing topic diversity by clustering and losing extra relevant terms
Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C .
The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18] .
In the case of 3 ¡Ñ 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose £mKLD value exceed 1.0 is 12.6 .
The user is able to recognize only 6.9 of these terms on average .
Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect .
On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5 .
We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback .
Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate .
Table 5
original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance .
The feedback process is simulated using document relevance judgment from NIST .
We use the mixture model based feedback method proposed in [25], with mixture noise set to 0.95 and feedback coefficient set to 0.9 .
Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run .
However, this does not hold for term feedback .
Thus, to make it fair w.r.t .
user"s information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out .
Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front .
Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C .
Table 6
N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents .
Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list .
We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback .
This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic .
We can then compare the terms in the truncated model with the checked terms .
Figure 3 shows the distribution of the terms" £mKLD scores .
We find that term feedback tends to produce expansion terms of higher quality(those with £mKLD > 1) compared to relevance feedback (with 10 feedback documents) .
This does not contradict the fact that the latter yields higher retrieval performance .
Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304 .
The truth Figure 3
We are interested to know under what circumstances term feedback has advantage over relevance feedback .
One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless .
This is not infrequent, as one might have thought
When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost .
Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 ¡Ñ 16 clarification form (we consider term t to be relevant if £mKLD(t) > 1.0) .
Furthermore, in 10 out of them TCFB3C outperforms the pseudo feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics) .
We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work
This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10 60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form .
Second, for some topics, a document needs to meet some special condition in order to be relevant .
The top N documents may be related to the topic, but nonetheless irrelevant .
In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones .
For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant .
But nevertheless, the feedback terms such as retail, commerce are good for query expansion. .
