TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback .
The algorithms take as input the original query q, the clusters {£ci} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model £cq that makes best use of the feedback evidence to capture the user"s information need .
First we describe our notations
? £cq 
? £ci (i = 1, 2, .
.
.
K)
? T = {ti,j} (i = 1 .
.
.
K, j = 1 .
.
.
L)
ti,j is the j th term chosen from cluster Ci .
? R = {£_w|w ? T}
5.1 TFB (Direct Term Feedback) This is a straight forward form of term feedback that does not involve any secondary structure .
We give a weight of 1 to terms judged relevant by the user, a weight of £g to query terms, zero weight to other terms, and then apply normalization
We call this method TFB (direct Term FeedBack) .
If we let £g = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does .
If we set £g > 1, we are putting more emphasis on the query terms than the checked ones .
Note that the result model will be more biased toward £cq if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case .
Figure 1
traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms .
The clusters represent different aspects of the query topic, each of which may or may not be relevant .
If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones) .
We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context .
Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms .
Because each cluster has an equal number of terms presented to the user, the simplest measure of a cluster"s relevance is the number of terms that are judged relevant in it .
Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification .
If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack)
We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|£cq ) = £fp(w|£cq) + (1 ? £f)p(w|£c1) which is merely pseudo feedback of the form proposed in [25] .
5.3 TCFB (Term cluster Feedback) TFB and CFB both have their drawbacks .
TFB assigns non zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the user"s ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones .
For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster .
CFB remedies TFB"s problem by treating the terms in a cluster collectively, so that unchecked unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged .
Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user .
Therefore, we try to combine the two methods, hoping to get the best out of both .
We do this by interpolating the TFB model with the CFB model, and call it TCFB
