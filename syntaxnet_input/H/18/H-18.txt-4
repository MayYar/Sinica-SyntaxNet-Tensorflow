Our goal is to segment documents and align the segments across documents (Figure 1) .
Let T be the set of terms {t1, t2, ..., tl}, which appear in the unlabelled set of documents D = {d1, d2, ..., dm} .
Let Sd be the set of sentences for document d ? D, i.e.{s1, s2, ..., snd } .
We have a 3D matrix of term frequency, in which the three dimensions are random variables of D, Sd, and T .
Sd actually is a random vector including a random variable for each d ? D .
The term frequency can be used to estimate the joint probability distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s) ND, where T(t, d, s) is the number of t in d"s sentence s and ND is the total number of terms in D .
?S represents the set of segments {?s1, ?s2, ..., ?sp} after segmentation and alignment among multiple documents, where the number of segments | ?S| = p .
A segment ?si of document d is a sequence of adjacent sentences in d .
Since for different documents si may discuss different sub topics, our goal is to cluster adjacent sentences in each document into segments, and align similar segments among documents, so that for different documents ?si is about the same sub topic .
The goal is to find the optimal topic segmentation and alignment mapping Segd(si) 
for d, {si, si+1, ..., sj} ¡÷ {?sq}, where q ? {1, ..., p}, where p is the segment number, and if i > j, then for d, ?sq is missing .
After segmentation and alignment, random vector Sd becomes an aligned random variable ?S .
Thus, P(D, Sd, T) becomes P(D, ?S, T) .
Term co clustering is a technique that has been employed [10] to improve the accuracy of document clustering .
We evaluate the effect of it for topic segmentation .
A term t is mapped to exactly one term cluster .
Term co clustering involves simultaneously finding the optimal term clustering mapping Clu(t) 
