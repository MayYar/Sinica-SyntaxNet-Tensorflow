Input: Figure 2 , Figure 3 , and Figure 4 show that on all data sets , the Bayesian hierarchical modeling approach has a statistical significant improvement over the regularized linear regression model , which is equivalent to the Bayesian hierarchical models learned at the first iteration .
Parse:
Figure NN ROOT
 +-- 2 CD num
 +-- , , punct
 +-- Figure NN conj
 |   +-- 3 CD num
 +-- and CC cc
 +-- show NN conj
 |   +-- Figure NN nn
 |   |   +-- 4 CD num
 |   +-- has VBZ ccomp
 |       +-- that IN mark
 |       +-- on IN prep
 |       |   +-- sets NNS pobj
 |       |       +-- all DT det
 |       |       +-- data NNS nn
 |       +-- , , punct
 |       +-- approach NN nsubj
 |       |   +-- the DT det
 |       |   +-- Bayesian JJ amod
 |       |   +-- hierarchical JJ amod
 |       |   +-- modeling NN nn
 |       +-- improvement NN dobj
 |           +-- a DT det
 |           +-- statistical JJ amod
 |           +-- significant JJ amod
 |           +-- over IN prep
 |               +-- model NN pobj
 |                   +-- the DT det
 |                   +-- regularized VBN amod
 |                   +-- linear JJ amod
 |                   +-- regression NN nn
 |                   +-- , , punct
 |                   +-- equivalent JJ rcmod
 |                       +-- which WDT nsubj
 |                       +-- is VBZ cop
 |                       +-- to IN prep
 |                           +-- models NNS pobj
 |                               +-- the DT det
 |                               +-- Bayesian JJ amod
 |                               +-- hierarchical JJ amod
 |                               +-- learned VBD partmod
 |                                   +-- at IN prep
 |                                       +-- iteration NN pobj
 |                                           +-- the DT det
 |                                           +-- first JJ amod
 +-- . . punct
Input: Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets .
Parse:
shows VBZ ROOT
 +-- analysis NN nsubj
 |   +-- Further JJ amod
 +-- correlation NN dobj
 |   +-- a DT det
 |   +-- negative JJ amod
 |   +-- between IN prep
 |       +-- number NN pobj
 |           +-- the DT det
 |           +-- of IN prep
 |           |   +-- data NNS pobj
 |           |       +-- training NN nn
 |           |       +-- for IN prep
 |           |           +-- user NN pobj
 |           |               +-- a DT det
 |           +-- and CC cc
 |           +-- improvement NN conj
 |               +-- the DT det
 |               +-- gets VBZ rcmod
 |                   +-- system NN nsubj
 |                       +-- the DT det
 +-- . . punct
Input: This suggests that the borrowing information from other users has more significant improvements for users with less training data , which is as expected .
Parse:
suggests VBZ ROOT
 +-- This DT nsubj
 +-- has VBZ ccomp
 |   +-- that IN mark
 |   +-- information NN nsubj
 |   |   +-- the DT det
 |   |   +-- borrowing VBG amod
 |   |   +-- from IN prep
 |   |       +-- users NNS pobj
 |   |           +-- other JJ amod
 |   +-- improvements NNS dobj
 |       +-- significant JJ amod
 |       |   +-- more RBR advmod
 |       +-- for IN prep
 |       |   +-- users NNS pobj
 |       +-- with IN prep
 |           +-- data NNS pobj
 |               +-- less JJR amod
 |               +-- training NN nn
 |               +-- , , punct
 |               +-- is VBZ rcmod
 |                   +-- which WDT nsubj
 |                   +-- expected VBN advcl
 |                       +-- as IN mark
 +-- . . punct
Input: However , the strength of the correlation differs over data sets , and the amount of training data is not the only characteristics that will influence the final performance .
Parse:
differs VBZ ROOT
 +-- However RB advmod
 +-- , , punct
 +-- strength NN nsubj
 |   +-- the DT det
 |   +-- of IN prep
 |       +-- correlation NN pobj
 |           +-- the DT det
 +-- over IN prep
 |   +-- sets NNS pobj
 |       +-- data NNS nn
 +-- and CC cc
 +-- characteristics NNS conj
 |   +-- amount NN nsubj
 |   |   +-- the DT det
 |   |   +-- of IN prep
 |   |       +-- data NNS pobj
 |   |           +-- training NN nn
 |   +-- is VBZ cop
 |   +-- not RB neg
 |   +-- the DT det
 |   +-- only JJ amod
 |   +-- influence VB rcmod
 |       +-- that WDT nsubj
 |       +-- will MD aux
 |       +-- performance NN dobj
 |           +-- the DT det
 |           +-- final JJ amod
 +-- . . punct
Input: Figure 2 and Figure 3 show that the proposed new algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets .
Parse:
show NN ROOT
 +-- Figure NN nsubj
 |   +-- 2 CD num
 |   +-- and CC cc
 |   +-- Figure NN conj
 |       +-- 3 CD num
 +-- works VBZ ccomp
 |   +-- that IN mark
 |   +-- algorithm NN nsubj
 |   |   +-- the DT det
 |   |   +-- proposed VBN amod
 |   |   +-- new JJ amod
 |   +-- better JJR advmod
 |       +-- than IN prep
 |           +-- algorithm NNP pobj
 |               +-- the DT det
 |               +-- standard JJ amod
 |               +-- EM NNP nn
 |               +-- on IN prep
 |                   +-- sets NNS pobj
 |                       +-- the DT det
 |                       +-- Netflix NNP nn
 |                       |   +-- and CC cc
 |                       |   +-- MovieLens NNPS conj
 |                       +-- data NN nn
 +-- . . punct
Input: This is not surprising since the number of related feature users pairs is much smaller than the number of unrelated feature user pairs on these two data sets , and thus the proposed new algorithm is expected to work better .
Parse:
surprising JJ ROOT
 +-- This DT nsubj
 +-- is VBZ cop
 +-- not RB neg
 +-- smaller JJR advcl
 |   +-- since IN mark
 |   +-- number NN nsubj
 |   |   +-- the DT det
 |   |   +-- of IN prep
 |   |       +-- pairs NNS pobj
 |   |           +-- related JJ amod
 |   |           +-- feature NN nn
 |   |           +-- users NNS nn
 |   +-- is VBZ cop
 |   +-- much RB advmod
 |   +-- than IN prep
 |   |   +-- number NN pobj
 |   |       +-- the DT det
 |   |       +-- of IN prep
 |   |           +-- pairs NNS pobj
 |   |               +-- unrelated JJ amod
 |   |               +-- user NN nn
 |   |               |   +-- feature NN nn
 |   |               +-- on IN prep
 |   |                   +-- sets NNS pobj
 |   |                       +-- these DT det
 |   |                       +-- two CD num
 |   |                       +-- data NNS nn
 |   +-- , , punct
 |   +-- and CC cc
 |   +-- expected VBN conj
 |       +-- thus RB advmod
 |       +-- algorithm NN nsubjpass
 |       |   +-- the DT det
 |       |   +-- proposed VBN amod
 |       |   +-- new JJ amod
 |       +-- is VBZ auxpass
 |       +-- work VB xcomp
 |           +-- to TO aux
 |           +-- better RBR advmod
 +-- . . punct
Input: Figure 4 shows that the two algorithms work similarly on the Reuters E data set .
Parse:
shows VBZ ROOT
 +-- Figure NN nsubj
 |   +-- 4 CD num
 +-- work VBP ccomp
 |   +-- that IN mark
 |   +-- algorithms NNS nsubj
 |   |   +-- the DT det
 |   |   +-- two CD num
 |   +-- similarly RB advmod
 |   +-- on IN prep
 |       +-- set VBN pobj
 |           +-- the DT det
 |           +-- Reuters NNP nn
 |           +-- E NNP nn
 |           +-- data NNS nn
 +-- . . punct
Input: The accuracy of the new algorithm is similar to that of the standard EM algorithm at each iteration .
Parse:
similar JJ ROOT
 +-- accuracy NN nsubj
 |   +-- The DT det
 |   +-- of IN prep
 |       +-- algorithm NN pobj
 |           +-- the DT det
 |           +-- new JJ amod
 +-- is VBZ cop
 +-- to IN prep
 |   +-- that DT pobj
 |       +-- of IN prep
 |           +-- algorithm NNP pobj
 |               +-- the DT det
 |               +-- standard NN amod
 |               +-- EM NNP nn
 |               +-- at IN prep
 |                   +-- iteration NN pobj
 |                       +-- each DT det
 +-- . . punct
Input: The general patterns are very similar on other Reuters '' subsets .
Parse:
similar JJ ROOT
 +-- patterns NNS nsubj
 |   +-- The DT det
 |   +-- general JJ amod
 +-- are VBP cop
 +-- very RB advmod
 +-- on IN prep
 |   +-- subsets NNS pobj
 |       +-- other JJ amod
 |       +-- Reuters NNP nn
 |       +-- '' '' punct
 +-- . . punct
Input: Further analysis shows that only 58 % of the user feature pairs are unrelated on this data set .
Parse:
shows VBZ ROOT
 +-- analysis NN nsubj
 |   +-- Further JJ amod
 +-- unrelated JJ ccomp
 |   +-- that IN mark
 |   +-- % NN nsubj
 |   |   +-- 58 CD num
 |   |   |   +-- only RB quantmod
 |   |   +-- of IN prep
 |   |       +-- pairs NNS pobj
 |   |           +-- the DT det
 |   |           +-- feature NN nn
 |   |               +-- user NN nn
 |   +-- are VBP cop
 |   +-- on IN prep
 |       +-- set NN pobj
 |           +-- this DT det
 |           +-- data NN nn
 +-- . . punct
Input: Since the number of unrelated user feature pairs is not extremely large , the sparseness is not a serious problem on the Reuters data set .
Parse:
problem NN ROOT
 +-- large JJ advcl
 |   +-- Since IN mark
 |   +-- number NN nsubj
 |   |   +-- the DT det
 |   |   +-- of IN prep
 |   |       +-- pairs NNS pobj
 |   |           +-- unrelated JJ amod
 |   |           +-- feature NN nn
 |   |               +-- user NN nn
 |   +-- is VBZ cop
 |   +-- not RB neg
 |   +-- extremely RB advmod
 +-- , , punct
 +-- sparseness NN nsubj
 |   +-- the DT det
 +-- is VBZ cop
 +-- not RB neg
 +-- a DT det
 +-- serious JJ amod
 +-- on IN prep
 |   +-- set NN pobj
 |       +-- the DT det
 |       +-- Reuters NNP nn
 |       +-- data NN nn
 +-- . . punct
Input: Thus the two learning algorithms perform similarly .
Parse:
perform VBP ROOT
 +-- Thus RB advmod
 +-- algorithms NNS nsubj
 |   +-- the DT det
 |   +-- two CD num
 |   +-- learning NN amod
 +-- similarly RB advmod
 +-- . . punct
Input: The results suggest that only on a corpus where the number of unrelated user feature pairs is much larger than the number of related pairs , such as on the Netflix data set , the proposed technique will get a significant improvement over standard EM .
Parse:
suggest VBP ROOT
 +-- results NNS nsubj
 |   +-- The DT det
 +-- get VB ccomp
 |   +-- that IN mark
 |   +-- on IN prep
 |   |   +-- only RB advmod
 |   |   +-- corpus NN pobj
 |   |       +-- a DT det
 |   |       +-- larger JJR rcmod
 |   |           +-- where WRB advmod
 |   |           +-- number NN nsubj
 |   |           |   +-- the DT det
 |   |           |   +-- of IN prep
 |   |           |       +-- pairs NNS pobj
 |   |           |           +-- unrelated JJ amod
 |   |           |           +-- feature NN nn
 |   |           |               +-- user NN nn
 |   |           +-- is VBZ cop
 |   |           +-- much RB advmod
 |   |           +-- than IN prep
 |   |               +-- number NN pobj
 |   |                   +-- the DT det
 |   |                   +-- of IN prep
 |   |                       +-- pairs NNS pobj
 |   |                           +-- related JJ amod
 |   |                           +-- , , punct
 |   |                           +-- as IN prep
 |   |                               +-- such JJ mwe
 |   |                               +-- on IN pcomp
 |   |                                   +-- data NNS pobj
 |   |                                       +-- the DT det
 |   |                                       +-- Netflix NNP nn
 |   |                                       +-- set VBN partmod
 |   +-- , , punct
 |   +-- technique NN nsubj
 |   |   +-- the DT det
 |   |   +-- proposed VBN amod
 |   +-- will MD aux
 |   +-- improvement NN dobj
 |       +-- a DT det
 |       +-- significant JJ amod
 |       +-- over IN prep
 |           +-- EM NNP pobj
 |               +-- standard JJ amod
 +-- . . punct
Input: However , the experiments also show that when the assumption does not hold , the new algorithm does not hurt performance .
Parse:
show VBP ROOT
 +-- However RB advmod
 +-- , , punct
 +-- experiments NNS nsubj
 |   +-- the DT det
 +-- also RB advmod
 +-- hurt VB ccomp
 |   +-- that IN mark
 |   +-- hold VB advcl
 |   |   +-- when WRB advmod
 |   |   +-- assumption NN nsubj
 |   |   |   +-- the DT det
 |   |   +-- does VBZ aux
 |   |   +-- not RB neg
 |   +-- , , punct
 |   +-- algorithm NN nsubj
 |   |   +-- the DT det
 |   |   +-- new JJ amod
 |   +-- does VBZ aux
 |   +-- not RB neg
 |   +-- performance NN dobj
 +-- . . punct
Input: Although the proposed technique is faster than standard Figure 2
Parse:
Figure NN ROOT
 +-- faster JJR advcl
 |   +-- Although IN mark
 |   +-- technique NN nsubj
 |   |   +-- the DT det
 |   |   +-- proposed VBN amod
 |   +-- is VBZ cop
 |   +-- than IN prep
 |       +-- standard JJ pobj
 +-- 2 CD num
Input: The new algorithm is statistical significantly better than EM algorithm at iterations 2 10 .
Parse:
better JJR ROOT
 +-- algorithm NN nsubj
 |   +-- The DT det
 |   +-- new JJ amod
 +-- is VBZ cop
 +-- statistical JJ advmod
 +-- significantly RB advmod
 +-- than IN prep
 |   +-- algorithm NN pobj
 |       +-- EM NNP nn
 |       +-- at IN prep
 |           +-- iterations NNS pobj
 |               +-- 10 CD num
 |                   +-- 2 CD number
 +-- . . punct
Input: Norm 2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration , and are statistical significantly worse than the Bayesian hierarchical models .
Parse:
equivalent JJ ROOT
 +-- models NNS nsubj
 |   +-- Norm NNP nn
 |   |   +-- 2 CD num
 |   +-- regularized VBN amod
 |   +-- linear JJ amod
 +-- are VBP cop
 +-- to IN prep
 |   +-- models NNS pobj
 |       +-- the DT det
 |       +-- Bayesian JJ amod
 |       +-- hierarchical JJ amod
 |       +-- learned VBD partmod
 |           +-- at IN prep
 |               +-- iteration NN pobj
 |                   +-- the DT det
 |                   +-- first JJ amod
 +-- , , punct
 +-- and CC cc
 +-- worse JJR conj
 |   +-- are VBP cop
 |   +-- statistical JJ nsubj
 |   +-- significantly RB advmod
 |   +-- than IN prep
 |       +-- models NNS pobj
 |           +-- the DT det
 |           +-- Bayesian JJ amod
 |           +-- hierarchical JJ amod
 +-- . . punct
Input: 0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3
Parse:
0 LS ROOT
 +-- 1.1 CD dep
     +-- 1.05 CD num
     |   +-- 2 CD number
     |   +-- 4 CD number
     |   +-- 6 CD number
     |   +-- 8 CD number
     |   +-- 10 CD number
     |   +-- 1 CD number
     +-- 1.4 CD dep
     |   +-- 1.35 CD num
     |       +-- 1.15 CD number
     |       +-- 1.2 CD number
     |       +-- 1.25 CD number
     |       +-- 1.3 CD number
     +-- EM NNP dep
     |   +-- Iterations NNPS nn
     |   +-- MeanSquareError NNP nn
     |   +-- New NNP nn
     |   +-- Algorithm NNP nn
     |   +-- Traditional NNP nn
     +-- Figure NN dep
         +-- 1 CD dep
         +-- 2 CD number
         +-- 3 CD number
         +-- 4 CD dep
         +-- 5 CD number
         +-- 6 CD number
         +-- 7 CD number
         +-- 8 CD number
         +-- 9 CD number
         +-- 10 CD number
         +-- 0.33 CD num
         +-- 0.34 CD num
         +-- 0.35 CD num
         +-- 0.36 CD num
         +-- 0.37 CD num
         +-- 0.38 CD num
         +-- 0.39 CD num
         +-- EM NNP nn
             +-- Iterations NNPS nn
             +-- ClassificationError NNP nn
             +-- New NNP nn
             +-- Algorithm NNP nn
             +-- Traditional NNP nn
Input: The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 -LRB- evaluated with mean square error -RRB- .
Parse:
better JJR ROOT
 +-- algorithm NN nsubj
 |   +-- The DT det
 |   +-- new JJ amod
 +-- is VBZ cop
 +-- statistical JJ advmod
 +-- significantly RB advmod
 +-- algorithm VB ccomp
 |   +-- than IN mark
 |   +-- EM NNP nsubj
 |   +-- at IN prep
 |   |   +-- iteration NN pobj
 |   |       +-- 2 CD num
 |   +-- to IN prep
 |   |   +-- 17 CD pobj
 |   +-- -LRB- -LRB- punct
 |   +-- evaluated VBN dep
 |   |   +-- with IN prep
 |   |       +-- error NN pobj
 |   |           +-- mean JJ amod
 |   |           +-- square JJ amod
 |   +-- -RRB- -RRB- punct
 +-- . . punct
Input: 1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4
Parse:
Iterations NNPS ROOT
 +-- 3.5 CD num
 |   +-- 2.5 CD number
 |   |   +-- 1 CD number
 |   |   +-- 6 CD number
 |   |   +-- 11 CD number
 |   |   +-- 16 CD number
 |   |   +-- 21 CD number
 |   |   +-- 0.5 CD number
 |   |   +-- 1.5 CD number
 |   |   +-- 2 CD number
 |   +-- 3 CD number
 +-- EM NNP dep
 |   +-- MeanSquareError NNP nn
 |   +-- New NNP nn
 |   +-- Algorithm NNP nn
 |   +-- Traditional NNP nn
 +-- 0.55 CD dep
 |   +-- 1 CD number
 |   +-- 6 CD number
 |   +-- 11 CD number
 |   +-- 16 CD number
 |   +-- 21 CD number
 |   +-- 0.35 CD number
 |   +-- 0.4 CD number
 |   +-- 0.45 CD number
 |   +-- 0.5 CD number
 +-- Iterations NNPS dep
     +-- 0.65 CD num
     |   +-- 0.6 CD number
     +-- EM NNP dep
     |   +-- ClassificationError NNP nn
     |   +-- New NNP nn
     |   +-- Algorithm NNP nn
     |   +-- Traditional NNP nn
     +-- Figure NN dep
         +-- 4 CD num
Input: Performances on Reuters C , Reuters M , Reuters G are similar .
Parse:
similar JJ ROOT
 +-- Performances NNS nsubj
 |   +-- on IN prep
 |       +-- C NNP pobj
 |           +-- Reuters NNP nn
 |           +-- , , punct
 |           +-- M NNP appos
 |           |   +-- Reuters NNP nn
 |           +-- G NNP appos
 |               +-- Reuters NNP nn
 +-- are VBP cop
 +-- . . punct
Input: 1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM , can it really learn millions of user models quickly ? Our results show that the modified EM algorithm converges quickly , and 2 3 modified EM iterations would result in a reliable estimation .
Parse:
5 CD ROOT
 +-- 1 CD number
 +-- 2 CD number
 +-- 3 CD number
 +-- 4 CD number
 +-- 0.011 CD dep
     +-- 0.0115 CD prep
         +-- 0.012 CD dep
             +-- 0.0125 CD prep
                 +-- 0.013 CD dep
                     +-- 0.0135 CD prep
                         +-- 0.014 CD dep
                             +-- EM NNP dep
                                 +-- Iterations NNPS dep
                                 +-- MeanSquareError NNP nn
                                 +-- New NNP nn
                                 +-- Algorithm NNP nn
                                 +-- Traditional NNP nn
                                 +-- 1 CD dep
                                     +-- 3 CD dep
                                         +-- 2 CD num
                                         +-- 4 CD prep
                                             +-- 0.0102 CD dep
                                                 +-- 5 CD num
                                                 +-- 0.0104 CD prep
                                                     +-- 0.0106 CD dep
                                                         +-- 0.0108 CD prep
                                                             +-- 0.011 CD dep
                                                                 +-- EM NNP prep
                                                                     +-- 0.0112 CD dep
                                                                     +-- 0.0114 CD num
                                                                     +-- Iterations NNPS nn
                                                                     +-- ClassificationError NNP nn
                                                                     +-- New NNP nn
                                                                     +-- Algorithm NNP nn
                                                                     +-- Traditional NNP amod
                                                                     +-- EM NNP nn
                                                                     +-- , , punct
                                                                     +-- learn VB dep
                                                                     |   +-- can MD aux
                                                                     |   +-- it PRP nsubj
                                                                     |   +-- really RB advmod
                                                                     |   +-- millions NNS dobj
                                                                     |   |   +-- of IN prep
                                                                     |   |       +-- models NNS pobj
                                                                     |   |           +-- user NN nn
                                                                     |   +-- quickly RB advmod
                                                                     +-- ? . punct
                                                                     +-- show VBP dep
                                                                     |   +-- results NNS nsubj
                                                                     |   |   +-- Our PRP$ poss
                                                                     |   +-- converges NNS ccomp
                                                                     |       +-- that IN mark
                                                                     |       +-- algorithm NNP nsubj
                                                                     |       |   +-- the DT det
                                                                     |       |   +-- modified JJ amod
                                                                     |       |   +-- EM NNP nn
                                                                     |       +-- quickly RB advmod
                                                                     |       +-- , , punct
                                                                     |       +-- and CC cc
                                                                     |       +-- result VB conj
                                                                     |           +-- iterations NNS nsubj
                                                                     |           |   +-- 3 CD num
                                                                     |           |   |   +-- 2 CD number
                                                                     |           |   +-- modified VBN amod
                                                                     |           |   +-- EM NNP nn
                                                                     |           +-- would MD aux
                                                                     |           +-- in IN prep
                                                                     |               +-- estimation NN pobj
                                                                     |                   +-- a DT det
                                                                     |                   +-- reliable JJ amod
                                                                     +-- . . punct
Input: We evaluated the algorithm on the whole Netflix data set -LRB- 480 , 189 users , 159 , 836 features , and 100 million ratings -RRB- running on a single CPU PC -LRB- 2GB memory , P4 3GHz -RRB- .
Parse:
evaluated VBD ROOT
 +-- We PRP nsubj
 +-- algorithm NN dobj
 |   +-- the DT det
 |   +-- on IN prep
 |       +-- data NNS pobj
 |           +-- the DT det
 |           +-- whole JJ amod
 |           +-- Netflix NNP nn
 |           +-- set VBN partmod
 |               +-- -LRB- -LRB- punct
 |               +-- 480 CD dobj
 |               |   +-- , , punct
 |               |   +-- users NNS conj
 |               |   |   +-- 189 CD num
 |               |   +-- 159 CD conj
 |               |   +-- features NNS conj
 |               |   |   +-- 836 CD num
 |               |   +-- and CC cc
 |               |   +-- ratings NNS conj
 |               |       +-- million CD num
 |               |           +-- 100 CD number
 |               +-- -RRB- -RRB- punct
 +-- running VBG xcomp
 |   +-- on IN prep
 |       +-- PC NN pobj
 |           +-- a DT det
 |           +-- single JJ amod
 |           +-- CPU NN nn
 |           +-- -LRB- -LRB- punct
 |           +-- memory NN dep
 |           |   +-- 2GB CD num
 |           |   +-- , , punct
 |           |   +-- 3GHz CD appos
 |           |       +-- P4 CD num
 |           +-- -RRB- -RRB- punct
 +-- . . punct
Input: The system finished one modified EM iteration in about 4 hours .
Parse:
finished VBD ROOT
 +-- system NN nsubj
 |   +-- The DT det
 +-- iteration NN dobj
 |   +-- one CD num
 |   +-- modified VBD amod
 |   +-- EM NNP nn
 +-- in IN prep
 |   +-- hours NNS pobj
 |       +-- 4 CD num
 |           +-- about RB quantmod
 +-- . . punct
Input: This demonstrates that the proposed technique can efficiently handle large scale system like Netflix. .
Parse:
demonstrates VBZ ROOT
 +-- This DT nsubj
 +-- handle VB ccomp
 |   +-- that IN mark
 |   +-- technique NN nsubj
 |   |   +-- the DT det
 |   |   +-- proposed VBN amod
 |   +-- can MD aux
 |   +-- efficiently RB advmod
 |   +-- system NN dobj
 |       +-- scale NN nn
 |       |   +-- large JJ amod
 |       +-- like IN prep
 |           +-- Netflix. NNP pobj
 +-- . . punct
