
1
Input: We have presented an SVM method that directly optimizes MAP .
Parse:
presented VBN ROOT
 +-- We PRP nsubj
 +-- have VBP aux
 +-- method NN dobj
 |   +-- an DT det
 |   +-- SVM NN nn
 |   +-- optimizes VBZ rcmod
 |       +-- that WDT nsubj
 |       +-- directly RB advmod
 |       +-- MAP NNP dobj
 +-- . . punct

2
Input: It provides a principled approach and avoids difficult to control heuristics .
Parse:
provides VBZ ROOT
 +-- It PRP nsubj
 +-- approach NN dobj
 |   +-- a DT det
 |   +-- principled VBN amod
 +-- and CC cc
 +-- avoids VBZ conj
 |   +-- difficult JJ advmod
 |   +-- control VB xcomp
 |       +-- to TO aux
 |       +-- heuristics NNS dobj
 +-- . . punct

3
Input: We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time .
Parse:
formulated VBD ROOT
 +-- We PRP nsubj
 +-- problem NN dobj
 |   +-- the DT det
 |   +-- optimization NN nn
 +-- and CC cc
 +-- presented VBD conj
 |   +-- algorithm NN dobj
 |       +-- an DT det
 |       +-- finds VBZ rcmod
 |           +-- which WDT nsubj
 |           +-- provably RB advmod
 |           +-- solution NN dobj
 |           |   +-- the DT det
 |           +-- in IN prep
 |               +-- time NN pobj
 |                   +-- polynomial JJ amod
 +-- . . punct

4
Input: We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods .
Parse:
shown VBN ROOT
 +-- We PRP nsubj
 +-- have VBP aux
 +-- empirically RB advmod
 +-- superior JJ ccomp
 |   +-- that IN mark
 |   +-- method NN nsubj
 |   |   +-- our PRP$ poss
 |   +-- is VBZ cop
 |   +-- generally RB advmod
 |   +-- to IN prep
 |   +-- or CC cc
 |   +-- competitive JJ conj
 |       +-- with IN prep
 |           +-- methods NNS pobj
 |               +-- conventional JJ amod
 |               +-- SVMs NNS nn
 +-- . . punct

5
Input: Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea .
Parse:
makes VBZ ROOT
 +-- method NN nsubj
 |   +-- Our PRP$ poss
 |   +-- new JJ amod
 +-- optimize VB xcomp
 |   +-- it PRP nsubj
 |   +-- easy JJ dep
 |   |   +-- conceptually RB advmod
 |   |   +-- just RB advmod
 |   |   +-- as RB advmod
 |   +-- to TO aux
 |   +-- SVMs NNS dobj
 |   |   +-- for IN prep
 |   |       +-- MAP NNP pobj
 |   +-- possible JJ advcl
 |       +-- as IN mark
 |       +-- was VBD cop
 |       +-- previously RB advmod
 |       +-- for IN prep
 |           +-- only RB advmod
 |           +-- Accuracy NNP pobj
 |               +-- and CC cc
 |               +-- ROCArea NNP conj
 +-- . . punct

6
Input: The computational cost for training is very reasonable in practice .
Parse:
reasonable JJ ROOT
 +-- cost NN nsubj
 |   +-- The DT det
 |   +-- computational JJ amod
 |   +-- for IN prep
 |       +-- training NN pobj
 +-- is VBZ cop
 +-- very RB advmod
 +-- in IN prep
 |   +-- practice NN pobj
 +-- . . punct

7
Input: Since other methods typically require tuning multiple heuristics , we also expect to train fewer models before finding one which achieves good performance .
Parse:
expect VBP ROOT
 +-- require VBP advcl
 |   +-- Since IN mark
 |   +-- methods NNS nsubj
 |   |   +-- other JJ amod
 |   +-- typically RB advmod
 |   +-- tuning VBG xcomp
 |       +-- heuristics NNS dobj
 |           +-- multiple JJ amod
 +-- , , punct
 +-- we PRP nsubj
 +-- also RB advmod
 +-- train VB xcomp
 |   +-- to TO aux
 |   +-- models NNS dobj
 |   |   +-- fewer JJR amod
 |   +-- before IN prep
 |       +-- finding VBG pcomp
 |           +-- one CD dobj
 |               +-- achieves VBZ rcmod
 |                   +-- which WDT nsubj
 |                   +-- performance NN dobj
 |                       +-- good JJ amod
 +-- . . punct

8
Input: The learning framework used by our method is fairly general .
Parse:
general JJ ROOT
 +-- framework NN nsubj
 |   +-- The DT det
 |   +-- learning NN amod
 |   +-- used VBN partmod
 |       +-- by IN prep
 |           +-- method NN pobj
 |               +-- our PRP$ poss
 +-- is VBZ cop
 +-- fairly RB advmod
 +-- . . punct

9
Input: A natural extension of this framework would be to develop methods to optimize for other important IR measures , such as Normalized Discounted Cumulative Gain -LRB- 2 , 3 , 4 , 12 -RRB- and Mean Reciprocal Rank. .
Parse:
be VB ROOT
 +-- extension NN nsubj
 |   +-- A DT det
 |   +-- natural JJ amod
 |   +-- of IN prep
 |       +-- framework NN pobj
 |           +-- this DT det
 +-- would MD aux
 +-- develop VB xcomp
 |   +-- to TO aux
 |   +-- methods NNS dobj
 |       +-- optimize VB infmod
 |           +-- to TO aux
 |           +-- for IN prep
 |               +-- measures NNS pobj
 |                   +-- other JJ amod
 |                   +-- important JJ amod
 |                   +-- IR NNP nn
 |                   +-- , , punct
 |                   +-- as IN prep
 |                       +-- such JJ mwe
 |                       +-- Gain NNP pobj
 |                           +-- Normalized NNP nn
 |                           +-- Discounted VBD nn
 |                           +-- Cumulative NNP nn
 |                           +-- -LRB- -LRB- punct
 |                           +-- 2 CD dep
 |                           |   +-- , , punct
 |                           |   +-- 3 CD num
 |                           |   +-- 4 CD num
 |                           |   +-- 12 CD num
 |                           +-- -RRB- -RRB- punct
 |                           +-- and CC cc
 |                           +-- Rank. NNP conj
 |                               +-- Mean NNP nn
 |                               +-- Reciprocal NNP nn
 +-- . . punct
