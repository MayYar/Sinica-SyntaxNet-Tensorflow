
1
Input: Since the covariance matrix Hsse used in traditional approaches is only dependent on the measured samples , i.e .
Parse:
i.e NN ROOT
 +-- dependent JJ advcl
 |   +-- Since IN mark
 |   +-- matrix NN nsubj
 |   |   +-- the DT det
 |   |   +-- covariance NN nn
 |   |   +-- used VBD rcmod
 |   |       +-- Hsse NNP nsubj
 |   |       +-- in IN prep
 |   |           +-- approaches NNS pobj
 |   |               +-- traditional JJ amod
 |   +-- is VBZ cop
 |   +-- only RB advmod
 |   +-- on IN prep
 |       +-- samples NNS pobj
 |           +-- the DT det
 |           +-- measured JJ amod
 +-- , , punct
 +-- . . punct

2
Input: zi '' s , these approaches fail to evaluate the expected errors on the unmeasured samples .
Parse:
fail VBP ROOT
 +-- zi UH discourse
 |   +-- '' '' punct
 |   +-- s POS dep
 +-- , , punct
 +-- approaches NNS nsubj
 |   +-- these DT det
 +-- evaluate VB xcomp
 |   +-- to TO aux
 |   +-- errors NNS dobj
 |       +-- the DT det
 |       +-- expected VBN amod
 |       +-- on IN prep
 |           +-- samples NNS pobj
 |               +-- the DT det
 |               +-- unmeasured JJ amod
 +-- . . punct

3
Input: In this Section , we introduce a novel active learning algorithm called Laplacian Optimal Design which makes efficient use of both measured -LRB- labeled -RRB- and unmeasured -LRB- unlabeled -RRB- samples .
Parse:
introduce VBP ROOT
 +-- In IN prep
 |   +-- Section NN pobj
 |       +-- this DT det
 +-- , , punct
 +-- we PRP nsubj
 +-- learning NN dobj
 |   +-- a DT det
 |   +-- novel JJ amod
 |   +-- active JJ amod
 |   +-- called VBN partmod
 |   |   +-- algorithm RB advmod
 |   |   +-- Design NNP dep
 |   |       +-- Laplacian NNP nn
 |   |       +-- Optimal NNP nn
 |   +-- makes VBZ rcmod
 |       +-- which WDT nsubj
 |       +-- use NN dobj
 |           +-- efficient JJ amod
 |           +-- of IN prep
 |               +-- samples NNS pobj
 |                   +-- both DT det
 |                   +-- measured VBN amod
 |                   |   +-- -LRB- -LRB- punct
 |                   |   +-- labeled VBN dep
 |                   |   +-- -RRB- -RRB- punct
 |                   |   +-- and CC cc
 |                   |   +-- unmeasured JJ conj
 |                   +-- -LRB- -LRB- punct
 |                   +-- unlabeled JJ amod
 |                   +-- -RRB- -RRB- punct
 +-- . . punct

4
Input: 3.1 The Objective Function In many machine learning problems , it is natural to assume that if two points xi , xj are sufficiently close to each other , then their measurements -LRB- f -LRB- xi -RRB- , f -LRB- xj -RRB- -RRB- are close as well .
Parse:
natural JJ ROOT
 +-- 3.1 CD dep
 |   +-- Function NN dep
 |       +-- The DT det
 |       +-- Objective NNP nn
 +-- In IN prep
 |   +-- problems NNS pobj
 |       +-- many JJ amod
 |       +-- learning NN nn
 |           +-- machine NN nn
 +-- , , punct
 +-- it PRP nsubj
 +-- is VBZ cop
 +-- assume VB xcomp
 |   +-- to TO aux
 |   +-- close JJ ccomp
 |       +-- close JJ nsubj
 |       |   +-- that IN mark
 |       |   +-- xi NN advcl
 |       |   |   +-- if IN mark
 |       |   |   +-- points NNS nsubj
 |       |   |       +-- two CD num
 |       |   +-- , , punct
 |       |   +-- xj NFP punct
 |       |   +-- are VBP cop
 |       |   +-- sufficiently RB advmod
 |       |   +-- to IN prep
 |       |   |   +-- other JJ pobj
 |       |   |       +-- each DT det
 |       |   +-- then RB advmod
 |       |   +-- measurements NNS dep
 |       |   |   +-- their PRP$ poss
 |       |   +-- -LRB- -LRB- punct
 |       |   +-- f UH advmod
 |       |   |   +-- -LRB- -LRB- punct
 |       |   |   +-- xi NNP dep
 |       |   |   +-- -RRB- -RRB- punct
 |       |   +-- f UH prep
 |       |   |   +-- xj SYM pobj
 |       |   |       +-- -LRB- -LRB- punct
 |       |   |       +-- -RRB- -RRB- punct
 |       |   +-- -RRB- -RRB- punct
 |       +-- are VBP cop
 |       +-- well RB advmod
 |           +-- as RB advmod
 +-- . . punct

5
Input: Let S be a similarity matrix .
Parse:
Let VB ROOT
 +-- matrix NN ccomp
 |   +-- S PRP nsubj
 |   +-- be VB cop
 |   +-- a DT det
 |   +-- similarity NN amod
 +-- . . punct

6
Input: Thus , a new loss function which respects the geometrical structure of the data space can be defined as follows
Parse:
defined VBN ROOT
 +-- Thus RB advmod
 +-- , , punct
 +-- function NN nsubjpass
 |   +-- a DT det
 |   +-- new JJ amod
 |   +-- loss NN nn
 |   +-- respects VBZ rcmod
 |       +-- which WDT nsubj
 |       +-- structure NN dobj
 |           +-- the DT det
 |           +-- geometrical JJ amod
 |           +-- of IN prep
 |               +-- space NN pobj
 |                   +-- the DT det
 |                   +-- data NNS nn
 +-- can MD aux
 +-- be VB auxpass
 +-- follows VBZ advcl
     +-- as IN mark

7
Input: Note that , the loss function -LRB- 3 -RRB- is essentially the same as the one used in Laplacian Regularized Regression -LRB- LRR , -LRB- 2 -RRB- -RRB- .
Parse:
Note VB ROOT
 +-- same JJ ccomp
 |   +-- that DT mark
 |   +-- , , punct
 |   +-- function NN nsubj
 |   |   +-- the DT det
 |   |   +-- loss NN nn
 |   |   +-- -LRB- -LRB- punct
 |   |   +-- 3 CD dep
 |   |   +-- -RRB- -RRB- punct
 |   +-- is VBZ cop
 |   +-- essentially RB advmod
 |   +-- the DT det
 |   +-- as IN prep
 |       +-- one NN pobj
 |           +-- the DT det
 |           +-- used VBN partmod
 |               +-- in IN prep
 |                   +-- Regression NNP pobj
 |                       +-- Laplacian NNP nn
 |                       +-- Regularized NNP nn
 |                       +-- -LRB- -LRB- punct
 |                       +-- LRR NNS dep
 |                       |   +-- , , punct
 |                       |   +-- 2 CD appos
 |                       |       +-- -LRB- -LRB- punct
 |                       |       +-- -RRB- -RRB- punct
 |                       +-- -RRB- -RRB- punct
 +-- . . punct

8
Input: However , LRR is a passive learning algorithm where the training data is given .
Parse:
learning NN ROOT
 +-- However RB advmod
 +-- , , punct
 +-- LRR EX nsubj
 +-- is VBZ cop
 +-- a DT det
 +-- passive JJ amod
 +-- algorithm NNP dobj
 |   +-- given VBN rcmod
 |       +-- where WRB advmod
 |       +-- data NN nsubjpass
 |       |   +-- the DT det
 |       |   +-- training NN nn
 |       +-- is VBZ auxpass
 +-- . . punct

9
Input: In this paper , we are focused on how to select the most informative data for training .
Parse:
focused VBN ROOT
 +-- In IN prep
 |   +-- paper NN pobj
 |       +-- this DT det
 +-- , , punct
 +-- we PRP nsubjpass
 +-- are VBP auxpass
 +-- on IN prep
 |   +-- select VB pcomp
 |       +-- how WRB advmod
 |       +-- to TO aux
 |       +-- data NNS dobj
 |           +-- the DT det
 |           +-- informative JJ amod
 |           |   +-- most RBS advmod
 |           +-- for IN prep
 |               +-- training NN pobj
 +-- . . punct

10
Input: The loss function with our choice of symmetric weights Sij -LRB- Sij = Sji -RRB- incurs a heavy penalty if neighboring points xi and xj are mapped far apart .
Parse:
incurs VBZ ROOT
 +-- function NN nsubj
 |   +-- The DT det
 |   +-- loss NN nn
 |   +-- with IN prep
 |       +-- choice NN pobj
 |           +-- our PRP$ poss
 |           +-- of IN prep
 |               +-- Sij NNP pobj
 |                   +-- weights NNS nn
 |                   |   +-- symmetric JJ amod
 |                   +-- -LRB- -LRB- punct
 |                   +-- Sji NNP dep
 |                   |   +-- Sij NNP nn
 |                   |   +-- = SYM nn
 |                   +-- -RRB- -RRB- punct
 +-- penalty NN dobj
 |   +-- a DT det
 |   +-- heavy JJ amod
 +-- points NNS advcl
 |   +-- if IN mark
 |   +-- neighboring VBG nsubj
 |   +-- xi NN dobj
 |   +-- and CC cc
 |   +-- mapped VBN conj
 |       +-- xj , nsubjpass
 |       +-- are VBP auxpass
 |       +-- apart RB advmod
 |           +-- far RB advmod
 +-- . . punct

11
Input: Therefore , minimizing J0 -LRB- w -RRB- is an attempt to ensure that if xi and xj are close then f -LRB- xi -RRB- and f -LRB- xj -RRB- are close as well .
Parse:
attempt NN ROOT
 +-- Therefore RB advmod
 +-- , , punct
 +-- minimizing VBG csubj
 |   +-- J0 CD dobj
 |       +-- w CC dep
 |           +-- -LRB- -LRB- punct
 |           +-- -RRB- -RRB- punct
 +-- is VBZ cop
 +-- an DT det
 +-- ensure VB xcomp
 |   +-- to TO aux
 |   +-- close JJ ccomp
 |       +-- that IN mark
 |       +-- close JJ nsubj
 |       |   +-- if IN mark
 |       |   +-- xi NN nsubj
 |       |   |   +-- and CC cc
 |       |   |   +-- xj `` conj
 |       |   +-- are VBP cop
 |       |   +-- then RB advmod
 |       |   +-- f JJ dep
 |       |   |   +-- xi NNP dep
 |       |   |       +-- -LRB- -LRB- punct
 |       |   |       +-- -RRB- -RRB- punct
 |       |   +-- and CC cc
 |       |   +-- xj LS conj
 |       |       +-- f NN dep
 |       |       +-- -LRB- -LRB- punct
 |       |       +-- -RRB- -RRB- punct
 |       +-- are VBP cop
 |       +-- well RB advmod
 |           +-- as RB advmod
 +-- . . punct

12
Input: There are many choices of the similarity matrix Canonical experimental design approaches -LRB- e.g .
Parse:
are VBP ROOT
 +-- There EX expl
 +-- choices NNS nsubj
 |   +-- many JJ amod
 |   +-- of IN prep
 |       +-- approaches NNS pobj
 |           +-- the DT det
 |           +-- similarity NN amod
 |           +-- matrix NN amod
 |           +-- Canonical JJ amod
 |           +-- design NN nn
 |           |   +-- experimental JJ amod
 |           +-- -LRB- -LRB- punct
 |           +-- e.g UH dep
 +-- . . punct

13
Input: A Optimal Design , D Optimal Design , and E Optimal -RRB- only consider linear functions .
Parse:
consider VBP ROOT
 +-- Design NNP nsubj
 |   +-- A DT det
 |   +-- Optimal NNP nn
 |   +-- , , punct
 |   +-- Design NNP conj
 |   |   +-- D NNP nn
 |   |   +-- Optimal NNP nn
 |   +-- and CC cc
 |   +-- Optimal NNP conj
 |       +-- E NNP nn
 |       +-- -RRB- -RRB- punct
 +-- only RB advmod
 +-- functions NNS dobj
 |   +-- linear JJ amod
 +-- . . punct

14
Input: They fail to discover the intrinsic geometry in the data when the data space is highly nonlinear .
Parse:
fail VBP ROOT
 +-- They PRP nsubj
 +-- discover VB xcomp
 |   +-- to TO aux
 |   +-- geometry NN dobj
 |   |   +-- the DT det
 |   |   +-- intrinsic JJ amod
 |   |   +-- in IN prep
 |   |       +-- data NNS pobj
 |   |           +-- the DT det
 |   +-- nonlinear JJ advcl
 |       +-- when WRB advmod
 |       +-- space NN nsubj
 |       |   +-- the DT det
 |       |   +-- data NN nn
 |       +-- is VBZ cop
 |       +-- highly RB advmod
 +-- . . punct

15
Input: In this section , we describe how to perform Laplacian Experimental Design in Reproducing Kernel Hilbert Space which gives rise to Kernel Laplacian Experimental Design .
Parse:
describe VBP ROOT
 +-- In IN prep
 |   +-- section NN pobj
 |       +-- this DT det
 +-- , , punct
 +-- we PRP nsubj
 +-- perform VB ccomp
 |   +-- how WRB advmod
 |   +-- to TO aux
 |   +-- Design NNP dobj
 |   |   +-- Laplacian NNP nn
 |   |   +-- Experimental NNP nn
 |   +-- in IN prep
 |       +-- Reproducing VBG pcomp
 |           +-- Space NNP dobj
 |               +-- Kernel NNP nn
 |               +-- Hilbert NNP nn
 |               +-- gives VBZ rcmod
 |                   +-- which WDT nsubj
 |                   +-- rise NN dobj
 |                       +-- to IN prep
 |                           +-- Design NNP pobj
 |                               +-- Kernel NNP nn
 |                               +-- Laplacian NNP nn
 |                               +-- Experimental NNP nn
 +-- . . punct
